<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head><title>MATH103: Probability</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='LNforHTML.css' rel='stylesheet' type='text/css' /> 
<meta content='LNforHTML.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
<div class='maketitle'>
                                                                                      
                                                                                      

<h2 class='titleHead'>MATH103: Probability</h2>
      <div class='author'><span class='cmss-12'>Lecturer: Dr John Haslegrave</span>
<br /><span class='cmss-12'>Email: </span><a class='url' href='j.haslegrave@lancaster.ac.uk'><span class='cmtt-12'>j.haslegrave@lancaster.ac.uk</span></a><br /></div>
<br />
<div class='date'><span class='cmss-12'>2024–25</span><br />
<span class='cmss-12'>Lecture notes created by John Haslegrave, Robin Hillier, Natasha Blitvić, David Leslie,
Amanda Turner, and other past lecturers of the Year 1 Probability module.</span></div>
                                                                                      
                                                                                      
</div>
<!-- l. 55 --><p class='noindent'>
                                                                                      
                                                                                      
                                                                                      
                                                                                      
</p>
<h2 class='likechapterHead'><a id='x1-1000'></a>Contents</h2>
<div class='tableofcontents'>
<span class='chapterToc'>1 <a href='#x1-30001' id='QQ2-1-3'>Random events</a></span>
<br /> <span class='sectionToc'>1.1 <a href='#x1-40001.1' id='QQ2-1-4'>Motivating examples</a></span>
<br /> <span class='sectionToc'>1.2 <a href='#x1-50001.2' id='QQ2-1-5'>Events and the sample space</a></span>
<br /> <span class='sectionToc'>1.3 <a href='#x1-100001.3' id='QQ2-1-10'>The Discrete Uniform Law</a></span>
<br /> <span class='sectionToc'>1.4 <a href='#x1-130001.4' id='QQ2-1-13'>Empirical probability (not examinable)</a></span>
<br /><span class='chapterToc'>2 <a href='#x1-140002' id='QQ2-1-14'>The axiomatic approach</a></span>
<br /> <span class='sectionToc'>2.1 <a href='#x1-150002.1' id='QQ2-1-15'>The axioms of probability</a></span>
<br /> <span class='sectionToc'>2.2 <a href='#x1-160002.2' id='QQ2-1-16'>Consequences of the axioms</a></span>
<br /> <span class='sectionToc'>2.3 <a href='#x1-170002.3' id='QQ2-1-17'>Conditional probability</a></span>
<br /> <span class='sectionToc'>2.4 <a href='#x1-180002.4' id='QQ2-1-18'>Bayes’ theorem</a></span>
<br /> <span class='sectionToc'>2.5 <a href='#x1-190002.5' id='QQ2-1-19'>Independent events</a></span>
<br /> <span class='sectionToc'>2.6 <a href='#x1-210002.6' id='QQ2-1-21'>Summary</a></span>
<br /><span class='chapterToc'>3 <a href='#x1-220003' id='QQ2-1-22'>Discrete random variables</a></span>
<br /> <span class='sectionToc'>3.1 <a href='#x1-230003.1' id='QQ2-1-23'>Definition</a></span>
<br /> <span class='sectionToc'>3.2 <a href='#x1-250003.2' id='QQ2-1-25'>Probability mass functions</a></span>
<br /> <span class='sectionToc'>3.3 <a href='#x1-260003.3' id='QQ2-1-26'>The probability of an event</a></span>
<br /> <span class='sectionToc'>3.4 <a href='#x1-270003.4' id='QQ2-1-27'>Expectation</a></span>
<br /> <span class='sectionToc'>3.5 <a href='#x1-280003.5' id='QQ2-1-28'>Variance</a></span>
<br /> <span class='sectionToc'>3.6 <a href='#x1-290003.6' id='QQ2-1-29'>Chebyshev’s inequality (not examinable)</a></span>
<br /><span class='chapterToc'>4 <a href='#x1-300004' id='QQ2-1-30'> Models for discrete random variables</a></span>
<br /> <span class='sectionToc'>4.1 <a href='#x1-310004.1' id='QQ2-1-31'>Useful mathematical identities proved elsewhere</a></span>
<br /> <span class='sectionToc'>4.2 <a href='#x1-400004.2' id='QQ2-1-40'>Discrete uniform random variables</a></span>
<br /> <span class='sectionToc'>4.3 <a href='#x1-410004.3' id='QQ2-1-41'>Bernoulli random variables</a></span>
<br /> <span class='sectionToc'>4.4 <a href='#x1-420004.4' id='QQ2-1-42'>Binomial random variables</a></span>
                                                                                      
                                                                                      
<br /> <span class='sectionToc'>4.5 <a href='#x1-440004.5' id='QQ2-1-44'>Geometric random variables</a></span>
<br /> <span class='sectionToc'>4.6 <a href='#x1-450004.6' id='QQ2-1-45'>Poisson random variables</a></span>
<br /> <span class='sectionToc'>4.7 <a href='#x1-470004.7' id='QQ2-1-47'>Negative binomial random variables (not examinable)</a></span>
<br /> <span class='sectionToc'>4.8 <a href='#x1-480004.8' id='QQ2-1-48'>Summary</a></span>
<br /><span class='chapterToc'>5 <a href='#x1-490005' id='QQ2-1-49'>Continuous random variables</a></span>
<br /> <span class='sectionToc'>5.1 <a href='#x1-500005.1' id='QQ2-1-50'>Introduction to continuous variables</a></span>
<br /> <span class='sectionToc'>5.2 <a href='#x1-510005.2' id='QQ2-1-51'>The cumulative distribution function</a></span>
<br /> <span class='sectionToc'>5.3 <a href='#x1-540005.3' id='QQ2-1-54'>The probability density function</a></span>
<br /> <span class='sectionToc'>5.4 <a href='#x1-590005.4' id='QQ2-1-59'>Expectation and variance</a></span>
<br /> <span class='sectionToc'>5.5 <a href='#x1-600005.5' id='QQ2-1-60'>Quantiles</a></span>
<br /> <span class='sectionToc'>5.6 <a href='#x1-610005.6' id='QQ2-1-61'>Transformations of random variables</a></span>
<br /><span class='chapterToc'>6 <a href='#x1-620006' id='QQ2-1-62'>Models for continuous random variables</a></span>
<br /> <span class='sectionToc'>6.1 <a href='#x1-630006.1' id='QQ2-1-63'>The uniform distribution</a></span>
<br /> <span class='sectionToc'>6.2 <a href='#x1-640006.2' id='QQ2-1-64'>The exponential distribution</a></span>
<br /> <span class='sectionToc'>6.3 <a href='#x1-680006.3' id='QQ2-1-68'>The gamma distribution</a></span>
<br /> <span class='sectionToc'>6.4 <a href='#x1-690006.4' id='QQ2-1-69'>The normal distribution</a></span>
<br /><span class='chapterToc'>7 <a href='#x1-700007' id='QQ2-1-70'>More than one random variable</a></span>
<br /> <span class='sectionToc'>7.1 <a href='#x1-710007.1' id='QQ2-1-71'>Joint probability mass functions</a></span>
<br /> <span class='sectionToc'>7.2 <a href='#x1-730007.2' id='QQ2-1-73'>Independence</a></span>
<br /> <span class='sectionToc'>7.3 <a href='#x1-740007.3' id='QQ2-1-74'>The weak law of large numbers</a></span>
</div>
<!-- l. 64 --><p class='noindent'><span class='paragraphHead'><a id='x1-2000'></a><span class='cmssbx-10x-x-109'>Additional Reading:</span></span>
Two standard references at this level are: </p>
     <ul class='itemize1'>
     <li class='itemize'>S. Ross, <span class='cmssbx-10x-x-109'>A First Course in Probability</span>, 5th Edition (2003).  Macmillan: New York.
     </li>
     <li class='itemize'>G. Grimmett and D. Welsh, <span class='cmssbx-10x-x-109'>Probability: An Introduction </span>(1986). Oxford University Press.</li></ul>
<!-- l. 70 --><p class='noindent'>However, any basic book on probability should be helpful. Do not hesitate to browse the library and look for
a book whose exposition style appeals to you.<br class='newline' />
                                                                                      
                                                                                      
</p><!-- l. 72 --><p class='noindent'>There are many helpful on-line resources, such as the Khan Academy:<br class='newline' /><a class='url' href='https://www.khanacademy.org/math/statistics-probability/probability-library'><span class='cmtt-10x-x-109'>https://www.khanacademy.org/math/statistics-probability/probability-library</span></a>.
</p><!-- l. 75 --><p class='noindent'>You may also find Wikipedia <a class='url' href='http://www.wikipedia.org'><span class='cmtt-10x-x-109'>http://www.wikipedia.org</span></a> helpful, although please be aware that
Wikipedia content is user-contributed and so is not always reliable.
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 1</span><br /><a id='x1-30001'></a>Random events</h2>
<!-- l. 86 --><p class='noindent'>Much of what we do is based on the belief that the future is largely unpredictable. Very few people would
play games such as roulette or the lottery, or buy and sell insurance policies if the outcomes were known in
advance. Probability is the study of chance, and attempts to express ideas of uncertainty quantitatively and
qualitatively. The theory of probability involves mathematics, logic, and considerations of the underlying
physical mechanisms that cause variability. In many applications the concepts are easy and the results are
consistent with intuition. However, in some cases it is more difficult and following our intuition can lead to
contradictions. It is for this reason that we need a formal consistent mathematical theory of
probability.
</p><!-- l. 90 --><p class='noindent'>The mathematical study of probability began with a correspondence between Blaise Pascal and Pierre de
Fermat in 1654. At the time, gamblers in France often had their games of chance interrupted by the
authorities. Fermat wrote most of his mathematics in letters to other mathematicians, and corresponded
with Pascal about the following question.
     </p><blockquote class='quote'>
     <!-- l. 94 --><p class='noindent'><span class='cmssbx-10x-x-109'>Suppose a game between two equally skilled players is interrupted. Given the
     scores of the players at the time of interruption, and the number of points needed
     to win the game, what is a fair way to divide the stakes?</span></p></blockquote>
<h3 class='sectionHead'><span class='titlemark'>1.1   </span> <a id='x1-40001.1'></a>Motivating examples</h3>
<div class='newtheorem'>
<!-- l. 99 --><p class='noindent'><span class='head'>
<a id='x1-4001r1'></a>
<span class='cmssbx-10x-x-109'>Example 1.1.</span>  </span>The probability that: </p>
     <ul class='itemize1'>
     <li class='itemize'>a fair coin gives a head when tossed is \(1/2\)
     </li>
     <li class='itemize'>a fair die shows a 6 is \(1/6\)
     </li>
     <li class='itemize'>an ace is drawn from a deck of cards is<br class='newline' />\(4/52=1/13=0.0769\)</li></ul>
</div>
<!-- l. 107 --><p class='noindent'>
</p><!-- l. 109 --><p class='noindent'>In each case these values coincide with intuition.
</p><!-- l. 111 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<div class='newtheorem'>
<!-- l. 112 --><p class='noindent'><span class='head'>
<a id='x1-4002r2'></a>
<span class='cmssbx-10x-x-109'>Example 1.2.</span>  </span>I play a guessing game with Jack and Jill. Jack selects a fair coin, tosses it, and
conceals the result.
</p><!-- l. 116 --><p class='noindent'>He asks me what is the probability that the coin is showing a head. I answer \(1/2\), by symmetry.
</p><!-- l. 120 --><p class='noindent'>Jack shows me the coin. It shows a <span class='cmssbx-10x-x-109'>head</span>. He asks me what is the probability that the coin is showing
a head.
</p><!-- l. 122 --><p class='noindent'>I answer that the probability is    \(1\).
</p><!-- l. 125 --><p class='noindent'>Jack shows the result of a new toss to Jill while concealing it from me. He asks me what is the
probability that the coin is showing a head.
</p><!-- l. 129 --><p class='noindent'>How should I now reply this time?
</p>
</div>
<!-- l. 130 --><p class='noindent'>
</p><!-- l. 132 --><p class='noindent'>This example poses questions concerning the nature of probability. Is probability a physical property of
the coin or is it somehow also determined by the experiment or by the experimenters and the
observers?
</p><!-- l. 136 --><p class='noindent'>The following is a more complicated situation that we shall return to later in the course. </p>
<div class='center'>
<!-- l. 149 --><p class='noindent'>
</p>
<div class='fbox'>Three indistinguishable purses each contain two coins. One purse contains two
gold coins, another contains two silver coins and the third contains one gold coin
and one silver coin.
               <span class='fbox'>GG                      GS                      SS</span>
A purse is selected at random and at random a coin is selected from it. It turns
out to be gold. What is the probability that the other coin in that purse is also
gold?                                                                </div>
</div>
<!-- l. 150 --><p class='noindent'>Here intuition might lead different people to different answers, at most one of which can be
correct.
</p><!-- l. 153 --><p class='noindent'>Thus we need to build up some theory in order to be sure of finding the correct value (which we shall do in
Chapter <a href='#x1-140002'>2<!-- tex4ht:ref: axioms  --></a>).
                                                                                      
                                                                                      
</p>
<h3 class='sectionHead'><span class='titlemark'>1.2   </span> <a id='x1-50001.2'></a>Events and the sample space</h3>
<!-- l. 158 --><p class='noindent'>Sets occur throughout mathematics, and provide a unifying language for the subject. Although they are
used extensively in probability, they should be covered elsewhere. The material required for this module is
summarised in the supplementary notes available on Moodle.
</p><!-- l. 161 --><p class='noindent'>Probability is a measure of the chance that an outcome or an event may occur, in the same way that length
is a measure of the magnitude of an object. In particular, probability is defined on the framework of events
that consists of: </p>
     <ul class='itemize1'>
     <li class='itemize'>the outcomes of an experiment (the elements of a set) ;
     </li>
     <li class='itemize'>the sample space which is the list of all these outcomes (the universal set) ;
     </li>
     <li class='itemize'>the events, collections of outcomes (subsets of the sample space).</li></ul>
<!-- l. 176 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-60001.2'></a>The sample space \( \Omega \)</h4>
<!-- l. 180 --><p class='noindent'>When a coin is tossed twice there are four distinct outcomes and these can be listed as  \[\{\text {HH, HT, TH, TT}\}.\]
</p><!-- l. 183 --><p class='noindent'>More generally an experiment will have \( N\) distinct outcomes which can be denoted by \[\{ \omega _1,\omega _2,\dots ,\omega _N\}.\]
</p>
<div class='center'>
<!-- l. 196 --><p class='noindent'>
</p>
<div class='fbox'>The set of all possible outcomes is \(\Omega \) and is known as the <span class='cmssbx-10x-x-109'>sample space</span>.
A particular outcome \( \omega \in \Omega \) is a <span class='cmssbx-10x-x-109'>sample point</span>.
\(\Omega \) and \(\omega \) are the (capital and lower-case) Greek letter “Omega”.
An experiment taking place corresponds to one of the outcomes \(\omega \in \Omega \) occurring
(i.e. happening).                                                      </div>
</div>
                                                                                      
                                                                                      
<div class='newtheorem'>
<!-- l. 198 --><p class='noindent'><span class='head'>
<a id='x1-6001r3'></a>
<span class='cmssbx-10x-x-109'>Example 1.3.</span>  </span>
   </p><dl class='enumerate'><dt class='enumerate'>
a. </dt><dd class='enumerate'>
   <!-- l. 200 --><p class='noindent'>A die is thrown so \[\Omega =\{{\quad 1,2,3,4,5,6}\quad \}.\]
   </p></dd><dt class='enumerate'>
b. </dt><dd class='enumerate'>
   <!-- l. 202 --><p class='noindent'>Three coins are thrown so \[\Omega =\{{\quad \qquad \text {HHH, HHT, HTH, THH, TTH, THT, HTT, TTT}}\quad \qquad \}.\]
   </p></dd><dt class='enumerate'>
c. </dt><dd class='enumerate'>
   <!-- l. 204 --><p class='noindent'>A die and a coin are thrown so \[\Omega =\{{\qquad \text {H}1,\text {H}2,\dots ,\text {H}6,\text {T}1,\dots ,\text {T}6 }\qquad \}.\]</p></dd></dl>
</div>
<!-- l. 207 --><p class='noindent'>
</p><!-- l. 210 --><p class='noindent'>The set of outcomes in the sample space has two very important properties.
</p>
<div class='center'>
<!-- l. 218 --><p class='noindent'>
</p>
<div class='fbox'>The set of outcomes in the sample space are
     <dl class='description'><dt class='description'>
     <!-- l. 218 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>exhaustive:</span> </p></dt><dd class='description'>
     <!-- l. 218 --><p class='noindent'>all possible outcomes are listed, and
     </p></dd><dt class='description'>
     <!-- l. 218 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>exclusive:</span> </p></dt><dd class='description'>
     <!-- l. 218 --><p class='noindent'>no two outcomes can both occur.</p></dd></dl>                                   </div>
</div>
<!-- l. 220 --><p class='noindent'>The number of elements in the sample space is the number of distinct possible outcomes of the
experiment. Sometimes it is helpful to represent the sample space diagrammatically, as in the following
examples.
</p>
                                                                                      
                                                                                      
<div class='newtheorem'>
<!-- l. 225 --><p class='noindent'><span class='head'>
<a id='x1-6005r4'></a>
<span class='cmssbx-10x-x-109'>Example 1.4.</span>  </span>A diagram of the sample space for a die and a coin
</p><!-- l. 228 --><p class='noindent'>Each point represents a possible outcome in the sample space.
</p>
</div>
<!-- l. 229 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 231 --><p class='noindent'><span class='head'>
<a id='x1-6006r5'></a>
<span class='cmssbx-10x-x-109'>Example 1.5.</span>  </span>The sample space for drawing a single card from a pack of playing cards:
</p>
</div>
<!-- l. 233 --><p class='noindent'>
</p><!-- l. 236 --><p class='noindent'>Identifying the sample space correctly is extremely important because all subsequent probability calculations
make use of the structure of the sample space.
</p><!-- l. 240 --><p class='noindent'>The examples so far have had a sample space we can write down (relatively) easily. However some sample
spaces can be infinite:
</p>
<div class='newtheorem'>
<!-- l. 242 --><p class='noindent'><span class='head'>
<a id='x1-6007r6'></a>
<span class='cmssbx-10x-x-109'>Example 1.6.</span>  </span> A coin is thrown until a tail is observed so that \[ \Omega =\{{\qquad \text {T, HT, HHT, HHHT}, \ldots }\qquad \}.\]
</p>
</div>
<!-- l. 245 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 246 --><p class='noindent'><span class='head'>
<a id='x1-6008r7'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Example 1.7.</span>  </span>The lifetime of a computer component is measured so that \[ \Omega ={ [0,\infty )}\]
</p>
</div>
<!-- l. 249 --><p class='noindent'>
</p><!-- l. 251 --><p class='noindent'>Even when the space is finite, listing it may present challenges:
</p>
<div class='newtheorem'>
<!-- l. 252 --><p class='noindent'><span class='head'>
<a id='x1-6009r8'></a>
<span class='cmssbx-10x-x-109'>Example 1.8.</span>  </span>Two cards are selected (without replacement) from a pack of \( 52\) playing cards.
</p><!-- l. 256 --><p class='noindent'>There are    \( 52\)  ways of choosing the first and    \( 51\)  ways of choosing the second having chosen the
first. In all    \( 52\times 51=2652\) which gives a big diagram!
</p>
</div>
<!-- l. 259 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 260 --><p class='noindent'><span class='head'>
<a id='x1-6010r9'></a>
<span class='cmssbx-10x-x-109'>Example 1.9.</span>  </span>A bag contains three balls, red, white and blue. A ball is selected and its colour
observed, then a second ball is selected <span class='cmssbx-10x-x-109'>without </span>replacing the first.
</p><!-- l. 264 --><p class='noindent'>The sample space is \( \Omega =\{ {\qquad \text {BR, BW, WR, WB, RW, RB} }\qquad \}\).
</p>
</div>
<!-- l. 266 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 267 --><p class='noindent'><span class='head'>
<a id='x1-6011r10'></a>
<span class='cmssbx-10x-x-109'>Example 1.10.</span>  </span>Two people are selected from the following four \(\{\)Alf, Bert, Cynthia, Doris\(\}\) and order
is unimportant. List the possible outcomes.
                                                                                      
                                                                                      
</p><!-- l. 272 --><p class='noindent'>\( \Omega = \{ {\qquad \text {AB, AC, AD, BC, BD, CD} }\qquad \}\).
</p>
</div>
<!-- l. 273 --><p class='noindent'>
</p><!-- l. 276 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-70001.2'></a>Events</h4>
<div class='center'>
<!-- l. 282 --><p class='noindent'>
</p>
<div class='fbox'>An <span class='cmssbx-10x-x-109'>event</span> \( A\) is a subset of the possible outcomes contained in the sample space \( \Omega \).
We write \( A \subseteq \Omega \) to denote that every element of \( A\) is a member of \( \Omega \).
An event \( A \subseteq \Omega \) <span class='cmssbx-10x-x-109'>occurs</span> if, when the experiment is performed, the outcome \( \omega \in \Omega \) satisfies \( \omega \in A\).</div>
</div>
<div class='newtheorem'>
<!-- l. 284 --><p class='noindent'><span class='head'>
<a id='x1-7001r11'></a>
<span class='cmssbx-10x-x-109'>Example 1.11.</span>  </span>A die is rolled, so the sample space is \(\Omega =\{1,2,3,4,5,6\}\). The event that the die shows an even face
is the subset    \( \{2,4,6\}\) .
</p>
</div>
<!-- l. 287 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 288 --><p class='noindent'><span class='head'>
<a id='x1-7002r12'></a>
<span class='cmssbx-10x-x-109'>Example 1.12.</span>  </span>Two coins are tossed, so the sample space is \(\Omega =\{\text {HH},\text {HT},\text {TH},\text {TT}\}\). The event that a tail occurs is the
subset    \(\{\text {HT}, \text {TH}, \text {TT}\}\) .
</p>
</div>
                                                                                      
                                                                                      
<!-- l. 291 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 293 --><p class='noindent'><span class='head'>
<a id='x1-7003r13'></a>
<span class='cmssbx-10x-x-109'>Example 1.13.</span>  </span>Illustrate the sample space when two dice are thrown and indicate the event that
the sum is \(5\).
</p>
</div>
<!-- l. 296 --><p class='noindent'>
</p><!-- l. 298 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead'><a id='x1-80001.2'></a>Special events</h5>
<!-- l. 300 --><p class='noindent'>There are some special events which exist for any sample space: </p>
     <ul class='itemize1'>
     <li class='itemize'>the whole sample space \(\Omega \) is an event; when the experiment occurs, we observe an \(\omega \in \Omega \), so the event
     \(\Omega \) is certain to occur.
     </li>
     <li class='itemize'>the empty set \( \varnothing =\{\}\) is an event with no elementary outcomes; \(\omega \not \in \varnothing \) for any \(\omega \), so \(\varnothing \) is an event which will
     never occur.</li></ul>
<!-- l. 306 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-90001.2'></a>Operations on events</h4>
<!-- l. 307 --><p class='noindent'>Two related operations on events are intersection corresponding to “and”, and union corresponding to
“or”. </p>
<div class='center'>
<!-- l. 310 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<div class='fbox'>The <span class='cmssbx-10x-x-109'>union</span> \( A \cup B\) of the events \( A\) and \( B\) is the set of outcomes \(\omega \) that are in \( A\) <span class='cmssbx-10x-x-109'>or</span> in \( B\) or in
both.
The <span class='cmssbx-10x-x-109'>intersection</span> \( A \cap B\) is the set of outcomes \(\omega \) that are in \( A\) <span class='cmssbx-10x-x-109'>and</span> in \( B\).              </div>
</div>
<div class='center'>
<!-- l. 315 --><p class='noindent'>
</p>
<div class='fbox'>The events \( A\) and \( B\) are <span class='cmssbx-10x-x-109'>mutually exclusive</span> or <span class='cmssbx-10x-x-109'>disjoint</span> if they have no outcomes
in common; that is \( A\cap B\) is the impossible event or equivalently \( A\cap B=\varnothing \).                 </div>
</div>
<div class='newtheorem'>
<!-- l. 317 --><p class='noindent'><span class='head'>
<a id='x1-9001r14'></a>
<span class='cmssbx-10x-x-109'>Example 1.14.</span>  </span>Let \(A\) be the event that a person has normal diastolic blood pressure (DBP) reading
\(\{\text {DBP}&lt;90\}\), and let \(B\) be the event that person has borderline DBP readings \(\{90\leq \text {DBP}\leq 95\}\). What are the events \(A\cup B\) and \(A \cap B\)?
</p>
</div>
<!-- l. 321 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 322 --><p class='noindent'><span class='head'>
<a id='x1-9002r15'></a>
<span class='cmssbx-10x-x-109'>Exercise 1.15.</span>  </span>A coin is thrown twice: let \( A\) denote the event that the same face occurs twice, and
let \( B\) denote the event that the coin shows different faces. Verify that these are mutually exclusive.
What does it mean here to be mutually exclusive?
</p>
</div>
<!-- l. 326 --><p class='noindent'>
</p>
<div class='center'>
<!-- l. 331 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>complementary</span> event to \( A\) is the event \( A^c\) consisting of those outcomes that
are in \( \Omega \) but are not in \( A\).
Note that \(A \cup A^c = \Omega \) and \(A \cap A^c = \varnothing \).                                                       </div>
</div>
<div class='newtheorem'>
<!-- l. 333 --><p class='noindent'><span class='head'>
<a id='x1-9003r16'></a>
<span class='cmssbx-10x-x-109'>Example 1.16.</span>  </span>If \({ A}=\{2,4,6\}\) is the event that the die shows an even face then \( A^c = {\quad \{ 1, 3, 5 \} }\) and is identical to the event
that the die shows an odd face.
</p>
</div>
<!-- l. 334 --><p class='noindent'>
</p><!-- l. 336 --><p class='noindent'>A <span class='cmssbx-10x-x-109'>partition</span> of the sample space splits the sample space into disjoint subsets.
</p>
<div class='newtheorem'>
<!-- l. 338 --><p class='noindent'><span class='head'>
<a id='x1-9004r17'></a>
<span class='cmssbx-10x-x-109'>Example 1.17.</span>  </span>The score on a die can be partitioned according to whether it is even or odd: \( \{1,2,3,4,5,6\}=\) \( \{2,4,6\}\cup \{1,3,5\}\).
</p><!-- l. 342 --><p class='noindent'>Write \( A_1=\{2,4,6\}\) and \( A_2=\{1,3,5\}\). Then \( A_1\) and \( A_2\) are mutually exclusive and exhaustive.
</p><!-- l. 345 --><p class='noindent'>We may express the sample space as \( \Omega =A_1\cup A_2\) where \( A_1\cap A_2=\varnothing \).
</p>
</div>
<!-- l. 347 --><p class='noindent'>
</p>
<div class='center'>
<!-- l. 360 --><p class='noindent'>
</p>
<div class='fbox'>More generally, the \( k\) sets \( A_1,A_2,\dots ,A_k\) form a <span class='cmssbx-10x-x-109'>partition</span> of the set \( B\) if the sets \( A_1,A_2,\dots ,A_k\) are <span class='cmssbx-10x-x-109'>mutually
exclusive</span> and <span class='cmssbx-10x-x-109'>exhaustive</span>, so that \[ A_i\cap A_j=\varnothing , \] for all \(i\neq j\) and \[ B=A_1\cup A_2\cup \cdots \cup A_k. \]                           </div>
</div>
<!-- l. 362 --><p class='noindent'>We can also define the difference of two events. </p>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 365 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>difference</span> \( A \setminus B\) of the events \( A\) and \( B\) is the set of outcomes \(\omega \) that are in \( A\) but not
in \( B\).                                                                  </div>
</div>
<!-- l. 366 --><p class='noindent'>Note that there is no simple relationship between \(A\setminus B\) and \(B\setminus A\).
</p><!-- l. 368 --><p class='noindent'>We can rewrite this notation using the complement: \(A\setminus B=A\cap B^c\).
</p><!-- l. 370 --><p class='noindent'>The complementary event is a special case of a difference: \(A^c=\Omega \setminus A\).
</p><!-- l. 374 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>1.3   </span> <a id='x1-100001.3'></a>The Discrete Uniform Law</h3>
<!-- l. 377 --><p class='noindent'>The Discrete Uniform law represents our intuitive notion of probability. It applies to situations where the set
\( \Omega \) is finite, \( \Omega = \{ \omega _1, \dots , \omega _n \}\), and each of the \( n\) sample points is <span class='cmssbx-10x-x-109'>equally likely</span>. The probability of the event \( A\)
is defined to be \[ \mathbb {P}(A) = \frac {|A|}{|\Omega |}, \] where \(|A|\) is the number of sample points in \( A\) and \(|\Omega |\) is the number of sample
points in \(\Omega \). Note that \( \mathbb {P}(\{ \omega _i \}) = 1/|\Omega |\) for all \( i = 1, \dots , n\), so that all sample points have equal probability associated to
them.
</p>
<div class='newtheorem'>
<!-- l. 384 --><p class='noindent'><span class='head'>
<a id='x1-10001r18'></a>
<span class='cmssbx-10x-x-109'>Exercise 1.18.</span>  </span>Find the probability that
   </p><dl class='enumerate'><dt class='enumerate'>
a. </dt><dd class='enumerate'>
   <!-- l. 387 --><p class='noindent'>A fair coin gives a head when tossed.
   </p></dd><dt class='enumerate'>
b. </dt><dd class='enumerate'>
   <!-- l. 388 --><p class='noindent'>A fair die shows a prime when rolled.
   </p></dd><dt class='enumerate'>
c. </dt><dd class='enumerate'>
   <!-- l. 389 --><p class='noindent'>A heart is drawn from a pack of cards.</p></dd></dl>
<!-- l. 391 --><p class='noindent'>In each case, consider the size of the sample space and the size of the event in question.
</p>
</div>
                                                                                      
                                                                                      
<!-- l. 392 --><p class='noindent'>
</p><!-- l. 396 --><p class='noindent'>Classical probability thus devotes a lot of effort to counting how many sample points are in the full sample
space, and how many sample points are in events of interest. This is studied in its own right in a
mathematical topic called <span class='cmssbx-10x-x-109'>combinatorics</span>, which is studied both in the A-level syllabus and then again in
MATH112. It covers topics such as: </p>
     <ul class='itemize1'>
     <li class='itemize'>factorials;
     </li>
     <li class='itemize'>permutations and combinations;
     </li>
     <li class='itemize'>numbers of ways of selecting specific hands of cards.</li></ul>
<!-- l. 403 --><p class='noindent'>Combinatorics plays an important part in probability, statistics, physics, actuary sciences, operations
research, and other fields. So that both the mathematics majors and non-majors are equipped with basic
skills in this arena, we will introduce three basic counting principles, and illustrate these with some
examples.
</p>
<div class='newtheorem'>
<!-- l. 405 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Counting Principle </span>(Principle 1)<span class='cmssbx-10x-x-109'>.</span>  </span>If an experiment involves two stages, such that the first stage
can have \(N_1\) possible outcomes, the second stage \(N_2\) possible outcomes, and the outcome of the second
stage is not restricted by the outcome of the first stage, then the total number of outcomes of the
experiment is \(N_1 N_2\). More generally, if there are \(r\) stages, with \(N_1,\ldots ,N_r\) possible outcomes, and each stage is not
affected by previous ones, there are \(N_1N_2\cdots N_r\) possible outcomes of the experiment.
</p>
</div>
<!-- l. 407 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 409 --><p class='noindent'><span class='head'>
<a id='x1-10005r19'></a>
<span class='cmssbx-10x-x-109'>Example 1.19.</span>  </span>We toss a coin twice: there are \(2\times 2=4\) possible outcomes.
</p><!-- l. 412 --><p class='noindent'>We toss a coin three times: there are \(2\times 2\times 2=8\quad \) possible outcomes.
                                                                                      
                                                                                      
</p><!-- l. 414 --><p class='noindent'>We toss a coin \(n\) times (where \(n\in \mathbb N\)), there are \(2^n\quad \) outcomes.
</p><!-- l. 417 --><p class='noindent'>We roll a six-sided die twice: there are \(6\times 6=36\quad \) possible outcomes.
</p><!-- l. 419 --><p class='noindent'>We roll a six-sided die \(n\) times: there are \(6^n\quad \) possible outcomes.
</p>
</div>
<!-- l. 420 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 422 --><p class='noindent'><span class='head'>
<a id='x1-10006r20'></a>
<span class='cmssbx-10x-x-109'>Example 1.20.</span>  </span>Though Principle 1 seems simple when applied to coin tosses and dice rolls, it is
remarkably powerful. Consider for instance the problem of counting all subsets of a finite set \(S\). Let’s
consider a few examples.
</p><!-- l. 425 --><p class='noindent'>If \(S=\varnothing \), then \(S\) has only one subset (namely, \(\varnothing \)).
</p><!-- l. 427 --><p class='noindent'>If \(S=\{s_1\}\), then \(S\) has subsets: \(\varnothing \) and \(\{s_1\}\)
</p><!-- l. 429 --><p class='noindent'>If \(S=\{s_1, s_2\}\), then \(S\) has subsets:  \(\varnothing , \{s_1\}, \{s_2\}, \{s_1,s_2\}\)
</p><!-- l. 431 --><p class='noindent'>We’re observing that a set with \(0\) elements has exactly \(2^0=1\) subsets, a set with one element has exactly \(2^1=2\)
subsets, a set with two elements has exactly \(2^2=4\) subsets. Is it generally true that if \(|S|=n\), then \(S\) has \(2^n\) subsets?
</p><!-- l. 433 --><p class='noindent'>Yes it is. We can construct a subset of \(S=\{s_1,\ldots ,s_n\}\) by first deciding whether or not \(s_1\) will be in the subset, then,
regardless of what you decided for \(s_1\), deciding whether or not \(s_2\) will be in your subset, and so on. That
is, for each \(i\in \{2,3,\ldots , n\}\), decide whether \(s_i\) will be in your subset, independently of your decision regarding \(s_1,\ldots , s_{i-1}\). By
Principle 1, there are \(2^n\) outcomes of this decision-making process. Each outcome is a different subset
of \(S\), and every subset of \(S\) is a possible outcome. So, there are \(2^n\) subsets of \(S\).
</p>
</div>
<!-- l. 434 --><p class='noindent'>
</p><!-- l. 436 --><p class='noindent'>Our next principle covers a common situation where the number of choices decreases at each step.
</p>
<div class='center'>
<!-- l. 440 --><p class='noindent'>
</p>
<div class='fbox'>For \(n\in \mathbb N\), we define \(n!\) (“\(n\) factorial”) to be the product \[n(n-1)(n-2)\cdots 3\times 2\times 1.\] We define \(0!\) to be \(1\).          </div>
</div>
                                                                                      
                                                                                      
<div class='newtheorem'>
<!-- l. 442 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Counting Principle </span>(Principle 2)<span class='cmssbx-10x-x-109'>.</span>  </span>The number of ways of arranging \(n\) objects in a line (or, in other
words, of <span class='cmssbx-10x-x-109'>permuting</span> \(n\) objects) is \(n!\).
</p>
</div>
<!-- l. 442 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 444 --><p class='noindent'><span class='head'>
<a id='x1-10007r21'></a>
<span class='cmssbx-10x-x-109'>Example 1.21.</span>  </span>Alex, Bob, and Charlotte arrive at the counter to order coffee. How many ways do
they have of forming a line?
</p><!-- l. 447 --><p class='noindent'>We can enumerate all possible configurations:  ABC, ACB, BAC, BCA, CAB, CBA.
</p><!-- l. 450 --><p class='noindent'>Or, we can count the configurations without enumerating them: there are three ways to pick who
goes in the first position, two ways to pick (among the remaining two people) who will go in the
second position, and one remaining way of picking the last remaining person for the last position.
In other words, there are \(3\times 2\times 1\) possible configurations, that is, \(3!=6\).
</p>
</div>
<!-- l. 451 --><p class='noindent'>
</p><!-- l. 456 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-110001.3'></a>Binomial coefficients</h4>
<!-- l. 457 --><p class='noindent'>The expression \( \frac {n!}{r!(n-r)!}\) occurs in the binomial theorem, and is also: </p>
     <ul class='itemize1'>
     <li class='itemize'>the number of permutations of \( n\) objects composed of two types with \( r\) of one type and \( n-r\) of the
     other; and
     </li>
     <li class='itemize'>the number of ways of selecting \( r\) objects from \( n\) distinguishable objects.</li></ul>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 469 --><p class='noindent'>
</p>
<div class='fbox'><span class='cmssbx-10x-x-109'>Binomial coefficients</span> are written \[ \binom {n}{r} = \frac {n!}{r!(n-r)!} \] for \(r = 0,1,\dots ,n\) and \(n\in \mathbb {N}_0\).
We read \(\binom {n}{r}\) as “\(n\) choose \(r\)”.                                                </div>
</div>
<!-- l. 471 --><p class='noindent'>We can phrase the above as our third principle.
</p>
<div class='newtheorem'>
<!-- l. 473 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Counting Principle </span>(Principle 3)<span class='cmssbx-10x-x-109'>.</span>  </span>The number of ways of forming a committee of \(k\) people out of
a population of \(n\) people is \(\binom {n}{k}\). (Since we’re forming a committee, we assume that the ordering of the
chosen individuals doesn’t matter, and, since there is only one of each person, repetitions are not
allowed.)
</p>
</div>
<!-- l. 474 --><p class='noindent'>
</p><!-- l. 476 --><p class='noindent'>A useful observation is the following: we can equivalently choose the people who are <span class='cmssbx-10x-x-109'>not </span>going to be
on the committee, and there must therefore be the same number of ways to choose either
group.
</p><!-- l. 478 --><p class='noindent'>We can also see this directly from the formula.
</p>
<div class='newtheorem'>
<!-- l. 480 --><p class='noindent'><span class='head'>
<a id='x1-11001r22'></a>
<span class='cmssbx-10x-x-109'>Example 1.22.</span>  </span>How many ways can \(4\) people be selected from \(6\)?
</p>
</div>
<!-- l. 482 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 484 --><p class='noindent'><span class='head'>
<a id='x1-11002r23'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Exercise 1.23.</span>  </span>A coin is thrown five times. Heads are shown on exactly three throws. How many
different sequences of heads and tails are there that have three heads? List them.
</p>
</div>
<!-- l. 486 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 490 --><p class='noindent'><span class='head'>
<a id='x1-11003r24'></a>
<span class='cmssbx-10x-x-109'>Exercise 1.24.</span>  </span>What is the probability of throwing exactly three heads when you toss a coin five
times?
</p>
</div>
<!-- l. 492 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 496 --><p class='noindent'><span class='head'>
<a id='x1-11004r25'></a>
<span class='cmssbx-10x-x-109'>Example 1.25.</span>  </span>Find the number of ways of choosing two digits (from \(0\) to \(9\)) and four letters from
the English alphabet (26 letters).
</p>
</div>
<!-- l. 498 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 500 --><p class='noindent'><span class='head'>
<a id='x1-11005r26'></a>
<span class='cmssbx-10x-x-109'>Example 1.26.</span>  </span>Assume that a valid licence plate is formed by six symbols, two of which are digits
and four of which are letters from the English alphabet, all chosen <span class='cmssbx-10x-x-109'>without </span>repetition. How many
valid license plates are there?
</p><!-- l. 503 --><p class='noindent'><span class='cmssbx-10x-x-109'>Note</span>: implicit in this problem is the fact that the <span class='cmssbx-10x-x-109'>order matters</span>. For example, JA93TZ is one
such licence plate, 9ZJ3AT is another – even though they use the same symbols, these are different
licence plates. Frequently, the main difficulty in combinatorics is not applying the three principles,
                                                                                      
                                                                                      
but rather working out which assumptions to make about whether order matters or repetitions are
allowed.
</p>
</div>
<!-- l. 504 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 506 --><p class='noindent'><span class='head'>
<a id='x1-11006r27'></a>
<span class='cmssbx-10x-x-109'>Example 1.27.</span>  </span>Assume that a valid post code is of the format “letter letter digit digit letter letter”,
where letters are drawn from the English alphabet, all without repetitions. How many valid post
codes are there?
</p>
</div>
<!-- l. 508 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 510 --><p class='noindent'><span class='head'>
<a id='x1-11007r28'></a>
<span class='cmssbx-10x-x-109'>Example 1.28.</span>  </span>Assume that a valid post code is of the format “letter letter digit digit letter letter”,
but now repetitions are allowed. How many valid post codes are there?
</p>
</div>
<!-- l. 512 --><p class='noindent'>
</p><!-- l. 514 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-120001.3'></a>The birthday problem</h4>
<div class='newtheorem'>
<!-- l. 515 --><p class='noindent'><span class='head'>
<a id='x1-12001r29'></a>
<span class='cmssbx-10x-x-109'>Exercise 1.29.</span>  </span>Would you bet that at least two people in your workshop had exactly the same
                                                                                      
                                                                                      
birthday? How large must a workshop be to make the probability of finding two people with the
same birthday at least \( 0.50\)?
</p>
</div>
<!-- l. 520 --><p class='noindent'>
</p><!-- l. 523 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>1.4   </span> <a id='x1-130001.4'></a>Empirical probability (not examinable)</h3>
<!-- l. 524 --><p class='noindent'>A serious limitation of the discrete uniform law is that it only applies in situations where the sample space
is finite and all outcomes are equiprobable. While this might be useful for drawing cards, rolling dice, or
pulling balls from urns, it offers no method for dealing with outcomes with unequal probabilities, or where
the sample space may be infinite.
</p><!-- l. 526 --><p class='noindent'>The <span class='cmssbx-10x-x-109'>frequentist</span> or <span class='cmssbx-10x-x-109'>empirical</span> approach to probability is based on the idea that the underlying probability
of an event can be measured by repeated trials. Supposing that \( A\) is an event for some experiment, then if
you repeat the experiment a number of times, \( n\), we might hope that the proportion of trials in which \( A\)
occurs tends to stabilise as \( n\rightarrow \infty \). We would like to call this \(\operatorname {Prob}(A)\). More precisely \[ \operatorname {Prob}(A) = \lim _{n \rightarrow \infty } {n_A}/{n}, \] where \( n_A\) is the number of times
event \( A\) occurs in the first \( n\) trials.
</p>
<div class='newtheorem'>
<!-- l. 532 --><p class='noindent'><span class='head'>
<a id='x1-13001r30'></a>
<span class='cmssbx-10x-x-109'>Example 1.30.</span>  </span></p>
     <ul class='itemize1'>
     <li class='itemize'>If you toss a coin \(1000\) times and get heads \(200\) times, that suggests the coin is biased and \( {\rm Prob}(\{H\}) \approx {1/5}\).
     </li>
     <li class='itemize'>Suppose a survey asks \(500\) people how they will vote in the next election and \(150\) say they support
     party X. If \( A\) is the event that a given person supports party X, then \(\operatorname {Prob}(A) \approx {3/10}\).</li></ul>
<!-- l. 537 --><p class='noindent'>In both cases, increasing the number of trials will improve the approximation.
</p>
</div>
<!-- l. 538 --><p class='noindent'>
</p><!-- l. 541 --><p class='noindent'>What can we say about the possible values of: </p>
                                                                                      
                                                                                      
     <ul class='itemize1'>
     <li class='itemize'>\(\operatorname {Prob}(A)\) in general (if it exists)?
     </li>
     <li class='itemize'>\(\operatorname {Prob}(\Omega )\)?
     </li>
     <li class='itemize'>\(\operatorname {Prob}(\varnothing )\)?</li></ul>
<!-- l. 548 --><p class='noindent'>Furthermore, if \( A\) and \( B\) are exclusive events, and \( C=A\cup B\), set </p>
     <ul class='itemize1'>
     <li class='itemize'>\( n_A\) to be the number of times \( A\) occurs in the first \( n\) trials
     </li>
     <li class='itemize'>\( n_B\) to be the number of times \( B\) occurs in the first \( n\) trials
     </li>
     <li class='itemize'>\( n_C\) to be the number of times \( C\) occurs in the first \( n\) trials</li></ul>
<!-- l. 557 --><p class='noindent'>Then \( n_C = n_A+n_B\) since \( A\) and \( B\) are exclusive. Therefore \[ \frac {c_n}n = \frac {a_n}n + \frac {b_n}n.\] Taking the limit as \( n{\rightarrow }\infty \) we see that \[ \operatorname {Prob}(C) = \operatorname {Prob}(A) + \operatorname {Prob}(B). \] But since \( C=A\cup B\), we have \[ \operatorname {Prob}(A\cup B) = \operatorname {Prob}(A) + \operatorname {Prob}(B) \] for the
exclusive events \( A\) and \( B\).
</p><!-- l. 564 --><p class='noindent'>We have deduced above some plausible rules about how probabilities should work.
</p><!-- l. 566 --><p class='noindent'>However, how can we know if this thinking is valid? It seems intuitively reasonable, but we can’t be sure.
In particular, it is impossible to conduct an infinite number of trials, and it is unclear how
large \( n\) must be to give a good approximation. More seriously, \(n_A/n\) may not converge at all, or
even if it does, if we repeat the experiment again, we may not necessarily obtain the same
limit.
</p><!-- l. 569 --><p class='noindent'>The modern theory of probability works the other way round: we assume that for each event \( A\) there exists a
number \(\mathbb {P}(A)\) called the probability of \( A\), and place axioms on the function \(\mathbb {P}\), making sure that it satisfies
our intuitive expectations. We will see that these axioms imply the convergence we hoped for
above.
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 2</span><br /><a id='x1-140002'></a>The axiomatic approach</h2>
<!-- l. 574 --><p class='noindent'>The limitations of classical and empirical probability show that it is necessary to build a rigorous framework
for a mathematical theory of probability.
</p>
<h3 class='sectionHead'><span class='titlemark'>2.1   </span> <a id='x1-150002.1'></a>The axioms of probability</h3>
<!-- l. 578 --><p class='noindent'>Let \( \Omega \) be a sample space. The <span class='cmssbx-10x-x-109'>probability</span> \(\mathbb {P}\) is a real-valued function defined on subsets of \( \Omega \) that satisfies the
following three properties.
</p>
<div class='center'>
<!-- l. 589 --><p class='noindent'>
</p>
<div class='fbox'>     <dl class='description'><dt class='description'>
     <!-- l. 589 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Axiom 1 (</span>positivity<span class='cmssbx-10x-x-109'>)</span> </p></dt><dd class='description'>
     <!-- l. 589 --><p class='noindent'>\( \mathbb {P}(A) \geq 0\) for all \( A \subseteq \Omega \).
     </p></dd><dt class='description'>
     <!-- l. 589 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Axiom 2 (</span>normalisation<span class='cmssbx-10x-x-109'>)</span> </p></dt><dd class='description'>
     <!-- l. 589 --><p class='noindent'>\( \mathbb {P}(\Omega )=1\).
     </p></dd><dt class='description'>
     <!-- l. 589 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Axiom 3 (</span>countable additivity<span class='cmssbx-10x-x-109'>)</span> </p></dt><dd class='description'>
     <!-- l. 589 --><p class='noindent'>If \(A_1\) and \(A_2\) are disjoint events, then \[\mathbb {P}(A_1\cup A_2)=\mathbb {P}(A_1)+\mathbb {P}(A_2).\]
     </p><!-- l. 589 --><p class='noindent'>More generally, if \(A_1,A_2,A_3,\ldots \subset \Omega \) are events that are pairwise disjoint (that is, \(A_i\cap A_j=\varnothing \) whenever
     \(i\neq j\)), then \[ \mathbb {P}\left (\bigcup _{i=1}^\infty A_i\right )=\sum _{i=1}^\infty \mathbb {P}(A_i).\]</p></dd></dl>                                                         </div>
</div>
<!-- l. 591 --><p class='noindent'>By the first part of Axiom \(3\), arguing by induction we deduce that whenever \(A_1,\ldots , A_n\) are pairwise disjoint, we have \[\mathbb {P}\left (\bigcup _{i=1}^n A_i\right )=\sum _{i=1}^n \mathbb {P}(A_i) \quad \text { (for any }n\in \mathbb N).\]
This part of the Axiom 3 is called <span class='cmssbx-10x-x-109'>finite additivity</span> of \(\mathbb {P}\).
</p><!-- l. 594 --><p class='noindent'>However, suppose we now have a countable collection of pairwise disjoint sets \(A_1,A_2,\ldots .\) (For example, suppose we’re
tossing a coin and let \(A_k\) be the event that we observe the first T on the \(i\)th toss, as in Example <a href='#x1-6007r6'>1.6<!-- tex4ht:ref: example-first-tail  --></a>.)
Unfortunately, the inductive argument cannot give us that \(\mathbb {P}\left (\bigcup _{i=1}^\infty A_i\right )=\sum _{i=1}^\infty \mathbb {P}(A_i)\). It is therefore necessary to include this
statement as part of our third axiom. These two statements combined constitute the <span class='cmssbx-10x-x-109'>countable additivity</span>
of \(\mathbb {P}\).
</p><!-- l. 596 --><p class='noindent'>The number \( \mathbb {P}(A)\) is called the probability of the event \( A\) and can be thought of as a measure of the likelihood
                                                                                      
                                                                                      
that \( A\) occurs.
</p><!-- l. 599 --><p class='noindent'>The whole theory of probability relies on these axioms. Subject only to these axioms the probability \(\mathbb {P}\) is
otherwise unspecified. We will soon see many examples of particular probability laws \(\mathbb {P}\) that play an
important role in practice.
</p>
<div class='newtheorem'>
<!-- l. 602 --><p class='noindent'><span class='head'>
<a id='x1-15001r1'></a>
<span class='cmssbx-10x-x-109'>Example 2.1.</span>  </span>Suppose the sample space \( \Omega \) contains four outcomes, \( \Omega =\{1,2,3,4\}\). Assuming \(\mathbb {P}\) satisfies Axiom 3, which of
the following are valid probability distributions?
    </p><dl class='enumerate'><dt class='enumerate'>
 i. </dt><dd class='enumerate'>
    <!-- l. 606 --><p class='noindent'>\( \mathbb {P}(\{1\})=1/2\), \( \mathbb {P}(\{2\})=\mathbb {P}(\{3\})=\mathbb {P}(\{4\})=1/6\).
    </p></dd><dt class='enumerate'>
 ii. </dt><dd class='enumerate'>
    <!-- l. 608 --><p class='noindent'>\( \mathbb {P}(\{1\})=\mathbb {P}(\{2\})=\mathbb {P}(\{3\})=\mathbb {P}(\{4\})=1/2\).
    </p></dd><dt class='enumerate'>
 iii. </dt><dd class='enumerate'>
    <!-- l. 610 --><p class='noindent'>\( \mathbb {P}(\{1\})=-0.2,\) and \( \mathbb {P}(\{2\})=\mathbb {P}(\{3\})=\mathbb {P}(\{4\})=0.4\).</p></dd></dl>
</div>
<!-- l. 612 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 614 --><p class='noindent'><span class='head'>
<a id='x1-15005r2'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.2.</span>  </span>Show that the Discrete Uniform law, defined in Section <a href='#x1-100001.3'>1.3<!-- tex4ht:ref: classical  --></a> on page <a href='#x1-100001.3'>13<!-- tex4ht:ref: classical  --></a>, is a probability.
</p>
</div>
<!-- l. 616 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 620 --><p class='noindent'><span class='head'>
<a id='x1-15006r3'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Example 2.3.</span>  </span>A fair coin is tossed twice so \[ \Omega = \{\text {HH, HT, TH, TT} \}.\] Since the coin is fair, we may assume all sample points are
equally likely: \[ \mathbb {P}(\{\text {HH}\}) = \mathbb {P} (\{\text {HT}\}) = \mathbb {P} (\{\text {TH}\}) = \mathbb {P} (\{\text {TT}\}). \] Now \begin {align*}  \mathbb {P} (\{\text {HH}\}) + \mathbb {P} (\{\text {HT}\} ) + \mathbb {P} (\{\text {TH}\}) + \mathbb {P} (\{\text {TT}\} ) &amp;= .  \end {align*}
</p><!-- l. 631 --><p class='noindent'>Therefore the probability of each outcome is \(1/4\).<br class='newline' />
</p><!-- l. 633 --><p class='noindent'>We can deduce other probabilities from this; for example \begin {align*}  \mathbb {P} (\text {exactly one T})&amp;=  \end {align*}
</p>
</div>
<!-- l. 636 --><p class='noindent'>
</p><!-- l. 640 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2.2   </span> <a id='x1-160002.2'></a>Consequences of the axioms</h3>
<!-- l. 641 --><p class='noindent'>The statements contained in this section can be derived by mathematical deduction from the axioms. They
are consequences of the axioms and while the axioms themselves are accepted on faith, any statement that
follows from these needs to be justified.
</p>
<div class='newtheorem'>
<!-- l. 646 --><p class='noindent'><span class='head'>
<a id='x1-16001r4'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.4 </span>(<span class='cmssbx-10x-x-109'>Monotonicity</span>)<span class='cmssbx-10x-x-109'>.</span>  </span>If \(A\subset B\), then \(\mathbb {P}(A)\leq \mathbb {P}(B)\).
</p>
</div>
<!-- l. 647 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 649 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span></p>
     <ul class='itemize1'>
     <li class='itemize'>Observe that \((B\cap A^c )\) and \(A\) are disjoint and, furthermore, that \(B=(B\cap A^c )\cup A\). (Note: it helps to draw the Venn
     diagram.)
     </li>
     <li class='itemize'>Therefore, by Axiom 3, \(\mathbb {P}(B)=\mathbb {P}(B\cap A^c )+\mathbb {P}(A).\)
     </li>
     <li class='itemize'>But, by Axiom 1, \(\mathbb {P}(B\cap A^c )\geq 0\).
     </li>
     <li class='itemize'>Therefore, \(\mathbb {P}(B)\geq \mathbb {P}(A).\)</li></ul>
                                                                                     □
</div>
<!-- l. 657 --><p class='noindent'><span class='cmssbx-10x-x-109'>Consequence of monotonicity: </span>Since every event \(A\) is a subset of \(\Omega \), we have \(\mathbb {P}(A)\leq \mathbb {P}(\Omega )\). So, in conjunction with
Axiom 1, we have that \[ 0\leq \mathbb {P}(A)\leq 1.\]
</p>
<div class='newtheorem'>
<!-- l. 661 --><p class='noindent'><span class='head'>
<a id='x1-16002r5'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.5 </span>(The law of <span class='cmssbx-10x-x-109'>complementary events</span>)<span class='cmssbx-10x-x-109'>.</span>  </span>\[{ \mathbb {P}(A^c )=1 - \mathbb {P}(A)}.\]
</p>
</div>
<!-- l. 662 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 664 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<div class='newtheorem'>
<!-- l. 667 --><p class='noindent'><span class='head'>
<a id='x1-16003r6'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Exercise 2.6.</span>  </span>A fair coin is thrown three times. What is the probability that at least one head
occurs?
</p>
</div>
<!-- l. 670 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 672 --><p class='noindent'><span class='head'>
<a id='x1-16004r7'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.7.</span>  </span>Show that \( \mathbb {P}(\varnothing ) = 0\).
</p>
</div>
<!-- l. 674 --><p class='noindent'>
</p><!-- l. 677 --><p class='noindent'>The law of complementary events is a useful way to consider single properties. However it provides no
mechanism for dealing with two events simultaneously, such as an individual being numerate
and literate. The first useful law for combining knowledge about more than one event is the
following.
</p>
<div class='newtheorem'>
<!-- l. 680 --><p class='noindent'><span class='head'>
<a id='x1-16005r8'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.8 </span>(The <span class='cmssbx-10x-x-109'>partition law</span>)<span class='cmssbx-10x-x-109'>.</span>  </span>\[ \mathbb {P}(A)=\mathbb {P}(A\cap B) + \mathbb {P}(A\cap B^c ).\]
</p>
</div>
<!-- l. 682 --><p class='noindent'>
</p><!-- l. 684 --><p class='noindent'><img alt='PIC' height='170' src='venn.png' width='170' />
</p>
<div class='proof'>
<!-- l. 687 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 689 --><p class='noindent'>The ‘partition law’ can be formulated in a more general way, which we call the Total Probability Theorem.
This more general version will be useful in several numerical examples later on.
</p>
<div class='newtheorem'>
<!-- l. 691 --><p class='noindent'><span class='head'>
<a id='x1-16006r9'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.9 </span>(The Total Probability Theorem)<span class='cmssbx-10x-x-109'>.</span>  </span>Consider events \(B_1,B_2,\ldots ,B_n\) that are pairwise disjoint (that
is, \(B_i\cap B_j=\varnothing \) whenever \(i\neq j\)) and are such that \(\Omega =\cup _{i=1}^n B_i\). Then, for any event \(A\subset \Omega \), we have \[{ \mathbb {P}(A)=\sum _{i=1}^n \mathbb {P}(A\cap B_i)}.\] More generally, if \(\Omega =\cup _{i=1}^\infty B_i\) with \(B_i\cap B_j=\varnothing \) whenever
\(i\neq j\), we have \[{ \mathbb {P}(A)=\sum _{i=1}^\infty \mathbb {P}(A\cap B_i)}.\]
</p>
</div>
<!-- l. 695 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 699 --><p class='noindent'><span class='head'>
<a id='x1-16007r10'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.10.</span>  </span>If a randomly selected token is </p>
     <ul class='itemize1'>
     <li class='itemize'>round and red with probability \( 0.1\),
     </li>
     <li class='itemize'>round and blue with probability \( 0.2\),
     </li>
     <li class='itemize'>square and red with probability \( 0.3\), and
     </li>
     <li class='itemize'>square and blue with probability \( 0.4\),</li></ul>
<!-- l. 710 --><p class='noindent'>and there are no other possibilities, find the probability that the selected token is square, and the
probability that it is blue.
</p>
</div>
                                                                                      
                                                                                      
<!-- l. 712 --><p class='noindent'>
</p><!-- l. 716 --><p class='noindent'>If two events \(A\) and \(B\) are disjoint, we know from the additivity axiom that \(\mathbb {P}(A\cup B)=\mathbb {P}(A)+\mathbb {P}(B)\). The addition law gives a general
rule for any pair of events.
</p>
<div class='newtheorem'>
<!-- l. 717 --><p class='noindent'><span class='head'>
<a id='x1-16008r11'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.11 </span>(The <span class='cmssbx-10x-x-109'>addition law</span>, also known as the inclusion-exclusion formula)<span class='cmssbx-10x-x-109'>.</span>  </span>\[\mathbb {P}(A\cup B)=\mathbb {P}(A) + \mathbb {P}(B) - \mathbb {P}(A\cap B).\]
</p>
</div>
<!-- l. 720 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 724 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<div class='newtheorem'>
<!-- l. 726 --><p class='noindent'><span class='head'>
<a id='x1-16009r12'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.12.</span>  </span>A fair die is thrown twice. Let the event \( A\) denote an even number on the first throw,
and let \( B\) denote an even number on the second. Find the probability of having at least one even
number.
</p>
</div>
<!-- l. 731 --><p class='noindent'>
A diagrammatic representation of the sample space makes this clearer: </p>
<div class='center'>
<!-- l. 733 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<div class='tabular'> <table class='tabular' id='TBL-2'><colgroup id='TBL-2-1g'><col id='TBL-2-1' /><col id='TBL-2-2' /></colgroup><colgroup id='TBL-2-3g'><col id='TBL-2-3' /><col id='TBL-2-4' /><col id='TBL-2-5' /><col id='TBL-2-6' /><col id='TBL-2-7' /><col id='TBL-2-8' /></colgroup><tr id='TBL-2-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-1-1' style='white-space:nowrap; text-align:right;'>second</td><td class='td11' id='TBL-2-1-2' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-2-1-3' style='white-space:nowrap; text-align:right;'> 2</td><td class='td11' id='TBL-2-1-4' style='white-space:nowrap; text-align:right;'> 4</td><td class='td11' id='TBL-2-1-5' style='white-space:nowrap; text-align:right;'> 6</td><td class='td11' id='TBL-2-1-6' style='white-space:nowrap; text-align:right;'>1</td><td class='td11' id='TBL-2-1-7' style='white-space:nowrap; text-align:right;'>3</td><td class='td11' id='TBL-2-1-8' style='white-space:nowrap; text-align:right;'>5</td></tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-2-' style='vertical-align:baseline;'><td class='td
11' id='TBL-2-2-1' style='white-space:nowrap; text-align:right;'>      </td><td class='td11' id='TBL-2-2-2' style='white-space:nowrap; text-align:right;'>5</td><td class='td11' id='TBL-2-2-3' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-2-4' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-2-5' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-2-6' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-2-7' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-2-8' style='white-space:nowrap; text-align:right;'>*</td></tr><tr id='TBL-2-3-' style='vertical-align:baseline;'><td class='td
11' id='TBL-2-3-1' style='white-space:nowrap; text-align:right;'>      </td><td class='td11' id='TBL-2-3-2' style='white-space:nowrap; text-align:right;'>3</td><td class='td11' id='TBL-2-3-3' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-3-4' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-3-5' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-3-6' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-3-7' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-3-8' style='white-space:nowrap; text-align:right;'>*</td></tr><tr id='TBL-2-4-' style='vertical-align:baseline;'><td class='td
11' id='TBL-2-4-1' style='white-space:nowrap; text-align:right;'>   first</td><td class='td11' id='TBL-2-4-2' style='white-space:nowrap; text-align:right;'>1</td><td class='td11' id='TBL-2-4-3' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-4-4' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-4-5' style='white-space:nowrap; text-align:right;'> b</td><td class='td11' id='TBL-2-4-6' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-4-7' style='white-space:nowrap; text-align:right;'>*</td><td class='td11' id='TBL-2-4-8' style='white-space:nowrap; text-align:right;'>*</td>
</tr><tr id='TBL-2-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-5-1' style='white-space:nowrap; text-align:right;'>      </td><td class='td11' id='TBL-2-5-2' style='white-space:nowrap; text-align:right;'>6</td><td class='td11' id='TBL-2-5-3' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-5-4' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-5-5' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-5-6' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-5-7' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-5-8' style='white-space:nowrap; text-align:right;'>a</td>
</tr><tr id='TBL-2-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-6-1' style='white-space:nowrap; text-align:right;'>      </td><td class='td11' id='TBL-2-6-2' style='white-space:nowrap; text-align:right;'>4</td><td class='td11' id='TBL-2-6-3' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-6-4' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-6-5' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-6-6' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-6-7' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-6-8' style='white-space:nowrap; text-align:right;'>a</td>
</tr><tr id='TBL-2-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-7-1' style='white-space:nowrap; text-align:right;'>      </td><td class='td11' id='TBL-2-7-2' style='white-space:nowrap; text-align:right;'>2</td><td class='td11' id='TBL-2-7-3' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-7-4' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-7-5' style='white-space:nowrap; text-align:right;'>ab</td><td class='td11' id='TBL-2-7-6' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-7-7' style='white-space:nowrap; text-align:right;'>a</td><td class='td11' id='TBL-2-7-8' style='white-space:nowrap; text-align:right;'>a</td>
</tr></table></div></div>
<!-- l. 742 --><p class='noindent'>Each point is equally probable.
</p><!-- l. 745 --><p class='noindent'>The laws of probability may be illustrated by a two-way table.
</p>
<div class='newtheorem'>
<!-- l. 749 --><p class='noindent'><span class='head'>
<a id='x1-16010r13'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.13.</span>  </span>Show that \( \mathbb {P}(A^c \cap B^c )=1-\mathbb {P}(A)-\mathbb {P}(B)+\mathbb {P}(A\cap B) \).
</p><!-- l. 752 --><p class='noindent'>[<span class='cmssbx-10x-x-109'>Hint</span>: \(A^c \cap B^c = (A\cup B)^c \).]
</p>
</div>
<!-- l. 753 --><p class='noindent'>
</p><!-- l. 755 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2.3   </span> <a id='x1-170002.3'></a>Conditional probability</h3>
<!-- l. 757 --><p class='noindent'>As we saw in a previous example the probability of an event depends not just on the experiment itself but
on other information. Conditional probability forms a framework in which this additional information can be
incorporated.
</p><!-- l. 761 --><p class='noindent'>Suppose we have two events, \( A\) and \( B\), and we know that \( B\) has occurred. The question is, what does this tell us
about whether \( A\) occurred?
</p><!-- l. 762 --><p class='noindent'>We resort to extracting intuition from the ideas of empirical probability: suppose we carry out the
experiment \( n\) times. \( B\) occurs on \( n_B\) trials. \( A\) and \( B\) occur together on \( n_{ A\cap B}\) of the trials. So \( A\) also occurs on a proportion \( n_{ A\cap B}/n_B\)
of the trials in which \( B\) occurs.
</p><!-- l. 769 --><p class='noindent'>This motivates the following definition. </p>
<div class='center'>
<!-- l. 774 --><p class='noindent'>
</p>
<div class='fbox'>If \( A\) and \( B\) are two events with \( \mathbb {P}(B)&gt;0\), then the <span class='cmssbx-10x-x-109'>conditional probability</span> of \( A\) given \( B\) is
written as \( \mathbb {P}(A\mid B)\) and defined to be \[\mathbb {P}(A\mid B)= \frac {\mathbb {P}(A\cap B)}{\mathbb {P}(B)}.\]                                           </div>
</div>
                                                                                      
                                                                                      
<!-- l. 776 --><p class='noindent'>Note the following immediate consequence of this definition: \[\mathbb {P}(A\cap B)=\mathbb {P}(B\mid A)\mathbb {P}(A)=\mathbb {P}(A\mid B)\mathbb {P}(B).\]
</p>
<div class='newtheorem'>
<!-- l. 780 --><p class='noindent'><span class='head'>
<a id='x1-17001r14'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.14.</span>  </span>If a fair die is thrown and the face shows a number \(X\). </p>
     <ul class='itemize1'>
     <li class='itemize'>Suppose that \(X\neq 2\). Find the probability that \(X\) is prime.
     </li>
     <li class='itemize'>Find the probability that \(X\neq 2\) given it is prime.</li></ul>
</div>
<!-- l. 786 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 789 --><p class='noindent'><span class='head'>
<a id='x1-17002r15'></a>
<span class='cmssbx-10x-x-109'>Practice question 2.15.</span>  </span>A bag contains \( 3\) blue, \( 5\) white and \( 2\) red marbles. A marble is selected at
random; it turns out to be blue. Find the probability that the next marble selected (without replacing
the first) is also blue.
</p>
</div>
<!-- l. 793 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 797 --><p class='noindent'><span class='head'>
<a id='x1-17003r16'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.16.</span>  </span>Three indistinguishable purses each contain two coins. One purse contains two gold
coins, another contains two silver coins and the third contains one gold coin and a silver coin.<br class='newline' />                                        <span class='fbox'>GG                                                          GS
SS</span>                                                                                    <br class='newline' />A purse is selected at random, then at random a coin is selected from it. The selected coin turns
out to be gold. Find the probability that the other coin in the purse is also gold.
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 807 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 809 --><p class='noindent'><span class='head'>
<a id='x1-17004r17'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.17.</span>  </span>Does \( \mathbb {P}(A\mid B)\) satisfy \( 0\le \mathbb {P}(A\mid B)\le 1\)?
</p>
</div>
<!-- l. 811 --><p class='noindent'>
</p><!-- l. 815 --><p class='noindent'>The partition law can be rephrased as the law of total probability, which is an extremely useful way to break
down considerations about real life events.
</p>
<div class='newtheorem'>
<!-- l. 816 --><p class='noindent'><span class='head'>
<a id='x1-17005r18'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.18 </span>(the <span class='cmssbx-10x-x-109'>law of total probability</span>)<span class='cmssbx-10x-x-109'>.</span>  </span>\[ \mathbb {P}(A) = \mathbb {P}(A\mid B)\mathbb {P}(B) + \mathbb {P}(A\mid B^c )\mathbb {P}(B^c ).\]
</p>
</div>
<!-- l. 818 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 822 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<div class='newtheorem'>
<!-- l. 826 --><p class='noindent'><span class='head'>
<a id='x1-17006r19'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Example 2.19.</span>  </span>  In  a  population  of  children,  \(60\%\)  are  vaccinated  against  whooping  cough.  The
probabilities of contracting whooping cough are \( 1/1000\) if the child is vaccinated and \( 1/100\) if not. Find the
probability that a child selected at random will contract whooping cough.
</p>
</div>
<!-- l. 833 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 836 --><p class='noindent'><span class='head'>
<a id='x1-17007r20'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.20.</span>  </span>Note that \( \{B,B^c \}\) is a partition of the sample space \(\Omega \). Conjecture and prove a generalisation
of the law of total probability to find \( \mathbb {P}(A)\) from \(\mathbb {P}(A\mid B_1)\), \(\mathbb {P}(A\mid B_2)\) and \(\mathbb {P}(A\mid B_3)\) when \( \{B_1,B_2,B_3\}\) are a partition of the sample space.
</p>
</div>
<!-- l. 839 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 842 --><p class='noindent'><span class='head'>
<a id='x1-17008r21'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.21.</span>  </span> A test for a disease gives positive results \(90\%\) of the time when a disease is present,
and \(10\%\) of the time when the disease is absent. It is known that \(1\%\) of the population have the disease.
Of those that receive a positive test result, \(80\%\) of patients receive treatment. For a randomly selected
member of the population, what is the probability of receiving treatment?
</p>
</div>
<!-- l. 844 --><p class='noindent'>
</p><!-- l. 848 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2.4   </span> <a id='x1-180002.4'></a>Bayes’ theorem</h3>
<!-- l. 850 --><p class='noindent'>Thomas Bayes (1701–1761), a clergyman and amateur statistician, was ignored by his contemporaries but
has had a profound effect on modern statistical thinking.
                                                                                      
                                                                                      
</p><!-- l. 854 --><p class='noindent'>Often we care about the probability of \( A\) given \( B\) but information is given about the probability of \( B\) given \( A\).
Bayes’ theorem provides the basis for transforming this information.
</p>
<div class='newtheorem'>
<!-- l. 860 --><p class='noindent'><span class='head'>
<a id='x1-18001r22'></a>
<span class='cmssbx-10x-x-109'>Theorem 2.22 </span>(Bayes’ theorem)<span class='cmssbx-10x-x-109'>.</span>  </span>If \( A\) and \( B\) are events in the sample space with \( \mathbb {P}(A), \mathbb {P}(B)&gt;0\) then \[ \mathbb {P}(B\mid A)=\frac {\mathbb {P}(A\mid B)\mathbb {P}(B)}{\mathbb {P}(A)}. \]
</p>
</div>
<!-- l. 865 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 869 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<!-- l. 871 --><p class='noindent'>Another way to express this theorem, using the law of total probability, is \[ \mathbb {P}(B\mid A)=\frac {\mathbb {P}(A\mid B)\mathbb {P}(B)}{\mathbb {P}(A\mid B)\mathbb {P}(B)+\mathbb {P}(A\mid B^c )\mathbb {P}(B^c )}. \] To evaluate the right hand side
we need to know the probabilities \( \mathbb {P}(A\mid B)\), \( \mathbb {P}(B)\), \( \mathbb {P}(A\mid B^c )\) and \( \mathbb {P}(B^c )\).
</p><!-- l. 878 --><p class='noindent'>When more than two possibilities are present, as when \( \{B_1,B_2,\dots ,B_k\}\) form a partition of the sample space \( \Omega \), Bayes’
formula extends to \begin {align*}  \mathbb {P}(B_i\mid A) &amp;=\frac { \mathbb {P}(A\mid B_i) \mathbb {P}(B_i) }{ \mathbb {P}(A)}\\ &amp;=\frac { \mathbb {P}(A\mid B_i)\mathbb {P}(B_i) }{ \sum _{j=1}^k\mathbb {P}(A\mid B_j)\mathbb {P}(B_j) }.  \end {align*}
</p>
<div class='newtheorem'>
<!-- l. 887 --><p class='noindent'><span class='head'>
<a id='x1-18002r23'></a>
<span class='cmssbx-10x-x-109'>Practice question 2.23.</span>  </span>For the whooping cough exercise above (Example <a href='#x1-17006r19'>2.19<!-- tex4ht:ref: ExampleWhooping  --></a> on p<a href='#x1-17006r19'>32<!-- tex4ht:ref: ExampleWhooping  --></a>), find the
probability that a child is vaccinated given the occurrence of whooping cough.
</p>
</div>
<!-- l. 891 --><p class='noindent'>
</p>
<div class='newtheorem'>
                                                                                      
                                                                                      
<!-- l. 893 --><p class='noindent'><span class='head'>
<a id='x1-18003r24'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.24.</span>  </span>Return to the disease example (Example <a href='#x1-17008r21'>2.21<!-- tex4ht:ref: ExampleDisease  --></a> on p<a href='#x1-17008r21'>32<!-- tex4ht:ref: ExampleDisease  --></a>). If you receive a positive test
result, what is the probability that you have the disease?
</p>
</div>
<!-- l. 895 --><p class='noindent'>
</p><!-- l. 899 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2.5   </span> <a id='x1-190002.5'></a>Independent events</h3>
<!-- l. 903 --><p class='noindent'>In the previous section we saw that \( \mathbb {P}(A\mid B)\), the conditional probability of \( A\) given \( B\), where \(\mathbb {P}(B)&gt;0\), was in general not equal
to \( \mathbb {P}(A)\), the unconditional probability of \( A\). In the special case when \[ \mathbb {P}(A\mid B)=\mathbb {P}(A), \] we say that \( A\) is <span class='cmssbx-10x-x-109'>independent</span> of \( B\).
Independence means that knowing that the event \( B\) has occurred does not change the chance that \( A\) will
occur.
</p><!-- l. 911 --><p class='noindent'>Note that, using the definitions of conditional probability, \( \mathbb {P}(A\cap B)=\mathbb {P}(A\mid B)\mathbb {P}(B)\) so if \( A\) is independent of \( B\), then \( \mathbb {P}(A \cap B)=\mathbb {P}(A)\mathbb {P}(B)\). We therefore get
the following definition of independence. </p>
<div class='center'>
<!-- l. 919 --><p class='noindent'>
</p>
<div class='fbox'>\( A\) and \( B\) are <span class='cmssbx-10x-x-109'>independent</span> events if and only if \[ \mathbb {P}(A\cap B)=\mathbb {P}(A)\mathbb {P}(B). \]                              </div>
</div>
<div class='newtheorem'>
<!-- l. 923 --><p class='noindent'><span class='head'>
<a id='x1-19001r25'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.25.</span>  </span>If two coins are thrown and the four possible outcomes are equally likely, show that
the events “head on first coin” and “head on second coin” are independent.
</p>
</div>
<!-- l. 925 --><p class='noindent'>
</p><!-- l. 928 --><p class='noindent'>Similar calculations show the independence of any pair of events with one referring to the first coin and the
other to the second coin.
                                                                                      
                                                                                      
</p>
<div class='newtheorem'>
<!-- l. 930 --><p class='noindent'><span class='head'>
<a id='x1-19002r26'></a>
<span class='cmssbx-10x-x-109'>Practice question 2.26.</span>  </span>Suppose that the coin is biased and that the probability of a head occurring
on any throw is \( \theta \), which can be any number between \( 0\) and \( 1\). Use independence to determine the
probabilities of the four outcomes when throwing the coin twice.
</p>
</div>
<!-- l. 936 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 938 --><p class='noindent'><span class='head'>
<a id='x1-19003r27'></a>
<span class='cmssbx-10x-x-109'>Example 2.27.</span>  </span>We roll a fair \(4\)-sided die twice. The sample space is \(\Omega =\{(i,j)\mid i=1,2,3,4, j=1,2,3,4\}\), with \(\mathbb {P}(\{(i,j)\})=1/16\) for each \((i,j)\in \Omega \).
</p><!-- l. 941 --><p class='noindent'>Let \(A_1\) be the event that the 1st throw lands a 3 and \(A_2\) the event that the 2nd throw lands a 1. Clearly,
</p>
<div class='math-display'>
<img alt='A1 = {(3,1),(3,2),(3,3),(3,4)},  A2 =  {(1,1),(2,1),(3,1 ),(4,1)},  A1 ∩ A2 = {(3,1)}.
' class='math-display' src='LNforHTML0x.svg' /></div>
<!-- l. 943 --><p class='noindent'>Hence \[\mathbb {P}(A_1\cap A_2)=\frac {1}{16}=\frac {1}{4}\times \frac {1}{4}=\mathbb {P}(A_1)\mathbb {P}(A_2).\] Therefore, \(A_1\) and \(A_2\) are independent.<br class='newline' />
</p><!-- l. 947 --><p class='noindent'>Now let \(B_1\) be the event that the first throw is 1 and \(B_2\) the event that the sum of the two throws is 2.
That is, \[B_1={\{(1,1),(1,2),(1,3),(1,4)\}},\quad B_2={ \{(1,1)\}},\quad B_1\cap B_2={\{(1,1)\}}.\] Hence, \[\mathbb {P}(B_1\cap B_2)=\frac {1}{16}\neq \mathbb {P}(B_1)\mathbb {P}(B_2)=\frac {1}{4}\times \frac {1}{16}.\] Therefore, \(B_1\) and \(B_2\) are  not independent.<br class='newline' />
</p><!-- l. 952 --><p class='noindent'>Finally, let \(B_1\) be again the event that the 1st throw lands a 1 and \(C_2\) the event that the sum of the two
throws is 5. That is,
                                                                                      
                                                                                      
</p>
<div class='math-display'>
<img alt='B1 = {(1,1),(1,2),(1,3),(1,4 )},  C2 =  {(1,4),(4,1),(2,3 ),(3,2)},  B1 ∩ C2 = {(1,4)}.
' class='math-display' src='LNforHTML1x.svg' /></div>
<!-- l. 954 --><p class='noindent'>Hence, \[ \mathbb {P}(B_1\cap C_2)=\frac {1}{16}=\frac {1}{4}\times \frac {1}{4}= \mathbb {P}(B_1)\mathbb {P}(C_2).\] Therefore, the two events are independent! (So, even though knowing the outcome of the
first throw gives us information about the sum, it does not change the probability of observing a
sum of 5.)
</p>
</div>
<!-- l. 957 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 959 --><p class='noindent'><span class='head'>
<a id='x1-19004r28'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.28.</span>  </span>Suppose that the probability of mothers being hypertensive (having high blood
pressure) is \( 0.1\) and that for fathers is \( 0.2\). Find the probability of a child’s parents both being hypertensive,
assuming both events are independent.
</p>
</div>
<!-- l. 965 --><p class='noindent'>
Note: we would expect these two events to be independent if the primary determinants of hypertensivity
were genetic, however if the primary determinants were environmental then we might expect the two events
not to be independent.
</p>
<div class='newtheorem'>
<!-- l. 972 --><p class='noindent'><span class='head'>
<a id='x1-19005r29'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.29.</span>  </span>If \( \mathbb {P}(A)=0.2\) and \( \mathbb {P}(B)=0.3\) find \( \mathbb {P}(A\cap B)\) if
     </p><dl class='enumerate'><dt class='enumerate'>
   i. </dt><dd class='enumerate'>
     <!-- l. 975 --><p class='noindent'>\( A\) and \( B\) are independent,
                                                                                      
                                                                                      
     </p></dd><dt class='enumerate'>
   ii. </dt><dd class='enumerate'>
     <!-- l. 976 --><p class='noindent'>\( A\) and \( B\) are exclusive.</p></dd></dl>
</div>
<!-- l. 978 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 980 --><p class='noindent'><span class='head'>
<a id='x1-19008r30'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.30.</span>  </span>Consider a mother and child’s blood pressures. Let \( A=\{\mbox {mother's DBP}\geq 95\}\) and \( B=\{\mbox {child's DBP}\geq 80\}\). Suppose we know that \( \mathbb {P}(A)=0.1\), \( \mathbb {P}(B)=0.2\)
and \( \mathbb {P}(A\cap B)=0.05\). Are \(A\) and \(B\) independent?
</p>
</div>
<!-- l. 987 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 989 --><p class='noindent'><span class='head'>
<a id='x1-19009r31'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.31.</span>  </span>Suppose that eye colours are only brown or green. Suppose also that there is a simple
genetic coding for this: if both your eye colour alleles are G then you have green eyes, otherwise you
have brown eyes. Suppose each allele is G with probability \(0.1\) (and you may assume independence).
What is the probability that a random population member has green eyes? What is the probability
that two randomly selected (unrelated) people both have green eyes?
</p>
</div>
<!-- l. 990 --><p class='noindent'>
</p><!-- l. 993 --><p class='noindent'>Genes are passed from parent to child – each parent passes one of their two alleles, selected
uniformly at random, to the child. If both parents have green eyes, what is the eye colour of the
child?
</p><!-- l. 996 --><p class='noindent'>If we don’t know the parents’ eye colour, but do know the eye colour of a sibling, this can give us useful
information. What can we say about the probability of a brother and sister both having green
eyes?
</p><!-- l. 999 --><p class='noindent'>Note that if the population prevalence of G is much smaller than \( 1/10\), then \( \mathbb {P}(\text {GG})\) decreases significantly, but the
bound on \( \mathbb {P}(\text {sister GG}\mid \text {brother GG})\) is unchanged.
                                                                                      
                                                                                      
</p><!-- l. 1001 --><p class='noindent'>Misunderstanding this kind of dependence can have tragic consequences. There have been
several cases in which mothers were wrongfully convicted of murdering their children based
on statistically illiterate testimony from doctors about the unlikelihood of multiple cases of
sudden infant death (SIDS) in the same family. These doctors made the error of treating two
siblings suffering SIDS as independent events. Essentially this is like confusing the independent
case and the sibling case above, except that the figures for SIDS roughly correspond to taking
\(\mathbb {P}(G)\approx \frac {1}{92}\).
</p><!-- l. 1004 --><p class='noindent'>Calculate the probability for two independent people, and approximate the probability for two siblings, to
have green eyes based on this value. The extremely low value we get in the independent case was used as
evidence to convict (which can itself lead to a different error known as the Prosecutor’s Fallacy), whereas
the real probability will be closer to the value we get for siblings.
</p>
<div class='newtheorem'>
<!-- l. 1006 --><p class='noindent'><span class='head'>
<a id='x1-19010r32'></a>
<span class='cmssbx-10x-x-109'>Example 2.32.</span>  </span>If \(A\subset B\), can \(A\) and \(B\) be independent?
</p>
</div>
<!-- l. 1008 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1010 --><p class='noindent'><span class='head'>
<a id='x1-19011r33'></a>
<span class='cmssbx-10x-x-109'>Example 2.33.</span>  </span>If \(A\cap B=\varnothing \), can \(A\) and \(B\) be independent?
</p>
</div>
<!-- l. 1012 --><p class='noindent'>
</p><!-- l. 1014 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-200002.5'></a>Independence for multiple events</h4>
<!-- l. 1015 --><p class='noindent'>We have seen several examples concerning the independence (or otherwise) of two events. Often we have
more than two events; what does it mean for multiple events to be independent?
</p>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 1022 --><p class='noindent'>
</p>
<div class='fbox'>Three events, \(A\), \(B\) and \(C\), are <span class='cmssbx-10x-x-109'>independent</span> if and only if all the following are satisfied: \begin {gather*}  \mathbb {P}(A\cap B)=\mathbb {P}(A)\mathbb {P}(B), \quad \mathbb {P}(B\cap C)=\mathbb {P}(B)\mathbb {P}(C), \quad \mathbb {P}(A\cap C)=\mathbb {P}(A)\mathbb {P}(C),\\ \mathbb {P}(A\cap B\cap C)=\mathbb {P}(A)\mathbb {P}(B)\mathbb {P}(C) \end {gather*}
                                                                     </div>
</div>
<!-- l. 1023 --><p class='noindent'>It is important to know that the last statement does not follow from the others. That is, even if \(A\) and \(B\) are
independent, \(B\) and \(C\) are independent, and \(A\) and \(C\) are independent, it is still possible that \(A\), \(B\) and \(C\) are <span class='cmssbx-10x-x-109'>not</span>
independent.
</p>
<div class='newtheorem'>
<!-- l. 1025 --><p class='noindent'><span class='head'>
<a id='x1-20001r34'></a>
<span class='cmssbx-10x-x-109'>Exercise 2.34.</span>  </span>I roll two standard fair dice, one red and one blue. Let \(A\) be the event that the red die shows
an odd number, \(B\) the event that the blue die shows an odd number, and \(C\) be the event that the total is
odd.
   </p><dl class='enumerate'><dt class='enumerate'>
a. </dt><dd class='enumerate'>
   <!-- l. 1027 --><p class='noindent'>Are \(A\) and \(C\) independent? What about \(B\) and \(C\)?
   </p></dd><dt class='enumerate'>
b. </dt><dd class='enumerate'>
   <!-- l. 1028 --><p class='noindent'>Are \(A\), \(B\) and \(C\) independent?</p></dd></dl>
</div>
<!-- l. 1030 --><p class='noindent'>
</p><!-- l. 1032 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2.6   </span> <a id='x1-210002.6'></a>Summary</h3>
<!-- l. 1033 --><p class='noindent'>We conclude with a summary of how the various notions introduced in this chapter are related.
</p>
                                                                                      
                                                                                      
<div class='tabular'> <table class='tabular' id='TBL-3'><colgroup id='TBL-3-1g'><col id='TBL-3-1' /></colgroup><colgroup id='TBL-3-2g'><col id='TBL-3-2' /></colgroup><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-1-1' style='white-space:nowrap; text-align:left;'>Sample space: \(\Omega \)          </td><td class='td11' id='TBL-3-1-2' style='white-space:nowrap; text-align:left;'>a set.                                                                 </td>
</tr><tr id='TBL-3-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-2-1' style='white-space:nowrap; text-align:left;'>Events (subsets of \(\Omega \)):    </td><td class='td11' id='TBL-3-2-2' style='white-space:nowrap; text-align:left;'>subsets of the sample space.                                    </td>
</tr><tr id='TBL-3-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-3-1' style='white-space:nowrap; text-align:left;'>Probability law \(\mathbb {P}(\cdot )\):          </td><td class='td11' id='TBL-3-3-2' style='white-space:nowrap; text-align:left;'>a measure of “chance” (likelihood) that an event occurs. </td>
</tr><tr id='TBL-3-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-4-1' style='white-space:nowrap; text-align:left;'>Conditional probability \(\mathbb {P}(\cdot \mid A)\):</td><td class='td11' id='TBL-3-4-2' style='white-space:nowrap; text-align:left;'>as above, but takes into account the fact that we already</td>
</tr><tr id='TBL-3-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-5-1' style='white-space:nowrap; text-align:left;'>                   </td><td class='td11' id='TBL-3-5-2' style='white-space:nowrap; text-align:left;'>know that \(A\) has occurred.                                         </td>
</tr><tr id='TBL-3-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-6-1' style='white-space:nowrap; text-align:left;'>Independence of \(A\) and \(B\): </td><td class='td11' id='TBL-3-6-2' style='white-space:nowrap; text-align:left;'>concerns events \(A\) and \(B\), but depends on the choice of \(\mathbb {P}\).     </td>
</tr><tr class='hline'><td></td><td></td></tr></table></div>
<!-- l. 1049 --><p class='noindent'>Since whether \(A\) and \(B\) are independent depends not only on the choice of \(A\) and \(B\) but also on \(\mathbb {P}\), independent
events may become dependent when conditioned on another event, or vice versa.
</p><!-- l. 1051 --><p class='noindent'>For example, let \(A\) be the event that person 1 has a rare genetic disease and \(B\) the event that person 2 has a
rare genetic disease. When these two people are selected at random from the general population, we expect
\(A\) and \(B\) to be independent. However, as we have discovered, things can change significantly if we discover
that the two randomly selected people are in fact siblings.
</p>
<div class='newtheorem'>
<!-- l. 1053 --><p class='noindent'><span class='head'>
<a id='x1-21001r35'></a>
<span class='cmssbx-10x-x-109'>Practice question 2.35.</span>  </span>In a standard deck of cards, the one-eyed cards are the King of Diamonds,
the Jack of Spades and the Jack of Hearts. A card is selected at random. Let \(E\) be the event that
it is a one-eyed card, \(R\) be the event that it is a red card, and \(J\) be the event that it is a jack. Work
out \(\mathbb {P}(E)\), \(\mathbb {P}(R)\) and \(\mathbb {P}(E\cap R)\). Are \(E\) and \(R\) independent? Now work out the conditional probabilities \(\mathbb {P}(E\mid J)\), \(\mathbb {P}(R\mid J)\) and \(\mathbb {P}(E\cap R\mid J)\). Are \(E\) and \(R\)
independent given \(J\)?
</p>
</div>
<!-- l. 1054 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 3</span><br /><a id='x1-220003'></a>Discrete random variables</h2>
<!-- l. 1058 --><p class='noindent'>We are not always interested in an experiment itself, but rather in some consequence of its random
outcome. For example, in a football match, we may be interested the total number of goals
that team \( A\) or team \( B\) scored, but not really concerned by how the game played out. Random
variables give us a way to think about these consequences in those situations when they take real
values.
</p>
<h3 class='sectionHead'><span class='titlemark'>3.1   </span> <a id='x1-230003.1'></a>Definition</h3>
<div class='center'>
<!-- l. 1066 --><p class='noindent'>
</p>
<div class='fbox'>A <span class='cmssbx-10x-x-109'>random variable</span> \( X\) is a  <span class='cmssbx-10x-x-109'>function</span> \(X\colon \Omega \to \mathbb {R}\). \(X\) associates each outcome \(\omega \) in the sample
space \(\Omega \) with a unique real number \( X(\omega )\).                                       </div>
</div>
<div class='newtheorem'>
<!-- l. 1070 --><p class='noindent'><span class='head'>
<a id='x1-23001r1'></a>
<span class='cmssbx-10x-x-109'>Example 3.1.</span>  </span>Suppose \( \Omega = \{ (i,j) : i, j \in \{ 1, \ldots , 6 \}\}\) is the sample space resulting from rolling two dice. We can define natural random
variables by </p>
     <ul class='itemize1'>
     <li class='itemize'>\(X((i,j)) = i+j\), the sum of the values on the dice,
     </li>
     <li class='itemize'>\(Y((i,j)) = \max \{ i,j \}\), the bigger of the two values on the dice.</li></ul>
</div>
<!-- l. 1077 --><p class='noindent'>
</p><!-- l. 1079 --><p class='noindent'>Every time the experiment is conducted exactly one value of the random variable is observed; this is called
a <span class='cmssbx-10x-x-109'>realisation</span> of the random variable.
</p>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 1083 --><p class='noindent'>
</p>
<div class='fbox'>The range of values taken by the random variable \( X\) defined on \( \Omega \), that is \(\{X(\omega ) : \omega \in \Omega \}\), is known
as the <span class='cmssbx-10x-x-109'>induced sample space</span> for \( X\) and is sometimes written as \( \mathcal {S}\).             </div>
</div>
<!-- l. 1085 --><p class='noindent'>In this chapter we shall focus on <span class='cmssbx-10x-x-109'>discrete random variables</span> – that is functions where \( \mathcal {S}\) is finite or
countable e.g. \(\mathcal {S} = \mathbb Z\). We will move on to <span class='cmssbx-10x-x-109'>continuous random variables</span> – functions where \( \mathcal {S}\) is uncountable e.g. \(\mathcal {S} = \mathbb {R}\)
– in Chapter <a href='#x1-490005'>5<!-- tex4ht:ref: chap:CtsRVs  --></a>.
</p><!-- l. 1091 --><p class='noindent'>Random variables are important as: </p>
     <ul class='itemize1'>
     <li class='itemize'>they are the result of most real experiments
     </li>
     <li class='itemize'>additional  structure  imposed  by  the  number  system,  such  as  ordering,  enables  further
     development of the ideas in the previous chapters.</li></ul>
<!-- l. 1097 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-240003.1'></a>Introductory examples of random variables</h4>
<!-- l. 1098 --><p class='noindent'>Discrete random variables arise in a variety of ways: From experiments </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 1101 --><p class='noindent'>with a natural integer valued outcome </p>
          <ul class='itemize2'>
          <li class='itemize'>the number of buses to stop in the hour,
          </li>
          <li class='itemize'>the number of goals in a football match.</li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 1106 --><p class='noindent'>with a continuous outcome which is recorded on an integer scale </p>
          <ul class='itemize2'>
          <li class='itemize'>heights, ages, salaries</li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 1111 --><p class='noindent'>with non-integer outcomes to which numerical values are assigned </p>
          <ul class='itemize2'>
          <li class='itemize'>a coin is tossed with outcome H or T, converted to \( 1\) and \( 0\) respectively.
          </li>
          <li class='itemize'>disease stage coded on a numerical scale (e.g. \(1,2,\ldots ,5\)).</li></ul>
     </li></ul>
<div class='newtheorem'>
<!-- l. 1120 --><p class='noindent'><span class='head'>
<a id='x1-24001r2'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.2.</span>  </span>A coin is tossed three times. The sample space is \[ \Omega =\{\text {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}\}.\] Define a random variable giving
the number of heads thrown. What is the induced sample space?
</p>
</div>
<!-- l. 1124 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1126 --><p class='noindent'><span class='head'>
<a id='x1-24002r3'></a>
<span class='cmssbx-10x-x-109'>Example 3.3.</span>  </span>Suppose we decide to record the number of children born in the local maternity
ward tomorrow. Any outcome is a non-negative integer, so a suitable sample space is \( \Omega =\{0,1,2,\ldots \}\). The random
variable is \( X(\omega ) = \omega \), giving the number of children born.
</p>
</div>
<!-- l. 1132 --><p class='noindent'>
</p><!-- l. 1135 --><p class='noindent'>Suppose that \( A \subseteq \mathcal {S}\) is a subset of the induced sample space. Then we can define the event \[ \{ X \in A \} = \{ \omega \in \Omega : X(\omega ) \in A \}\subseteq \Omega . \] In the special case \(A=\{a\}\)
we can write \(\{X=a\}\) for the event \(\{X\in A\}\).
</p><!-- l. 1141 --><p class='noindent'>In the three-coins example above, we have that \[{ \{X=2\} = \{\text {HHT, HTH, THH}\}}.\]
</p><!-- l. 1145 --><p class='noindent'>The right hand side of these equations is an event in \( \Omega \) and hence has a probability assigned to it. This
induces a probability on the induced sample space, given by \[ \mathbb {P}( X \in A ) = \mathbb {P}(\{ \omega \in \Omega : X(\omega ) \in A \}). \]
                                                                                      
                                                                                      
</p>
<div class='newtheorem'>
<!-- l. 1151 --><p class='noindent'><span class='head'>
<a id='x1-24003r4'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.4.</span>  </span>Suppose our sample space consists of the outcomes of throwing a fair die, and suppose we
gamble on the outcome: </p>
     <ul class='itemize1'>
     <li class='itemize'>lose \( \pounds 1\) if the outcome is \( 1\), \( 2\) or \( 3\);
     </li>
     <li class='itemize'>win nothing if the outcome is \( 4\);
     </li>
     <li class='itemize'>win \( \pounds 2\) if the outcome is \( 5\) or \( 6\).</li></ul>
<!-- l. 1159 --><p class='noindent'>Define \( X\) to be the random variable giving our profit. Find the induced sample space for \( X\), and evaluate the
probabilities on the induced sample space.
</p>
</div>
<!-- l. 1161 --><p class='noindent'>
</p><!-- l. 1163 --><p class='noindent'>To summarise: </p>
     <ul class='itemize1'>
     <li class='itemize'>The outcomes in the <span class='cmssbx-10x-x-109'>sample space</span>, \( \Omega \), of the probability experiment may or may not be
     numerically valued.
     </li>
     <li class='itemize'>A <span class='cmssbx-10x-x-109'>random variable</span> \( X\) is a function that associates a unique real number with each outcome in
     the sample space, \( \Omega \).
     </li>
     <li class='itemize'>A random variable is <span class='cmssbx-10x-x-109'>not </span>a number. It is neither random, nor a variable. It is a <span class='cmssbx-10x-x-109'>function</span>.
     </li>
     <li class='itemize'>The set of values taken by random variable \( X\) defined on \( \Omega \), is known as the <span class='cmssbx-10x-x-109'>induced sample
                                                                                      
                                                                                      
     space</span> for \( X\) and is sometimes written as \( \mathcal {S}\).
     </li>
     <li class='itemize'>The event \( \{X=x\}= \{\omega : X(\omega )=x\}\) and this correspondence induces a probability distribution on \( \mathcal {S}\).</li></ul>
<!-- l. 1176 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>3.2   </span> <a id='x1-250003.2'></a>Probability mass functions</h3>
<!-- l. 1179 --><p class='noindent'>Here \( X\) is a discrete random variable that takes values in the non-negative integers \(\mathbb {N}_0\), or a subset
thereof.
</p><!-- l. 1183 --><p class='noindent'>We cannot predict the value of \( X\) exactly, but we can state the values \( X\) could take and attach probabilities to
these values.
</p>
<div class='center'>
<!-- l. 1192 --><p class='noindent'>
</p>
<div class='fbox'>The  <span class='cmssbx-10x-x-109'>probability  mass  function</span> (p.m.f.) \(p_X\)  of  a  discrete  random  variable  \(X\)  is
defined by \[ p_X(x)=\mathbb {P}(X=x) \] for all \(x\in \mathcal {S}\).                                                    </div>
</div>
<div class='newtheorem'>
<!-- l. 1194 --><p class='noindent'><span class='head'>
<a id='x1-25001r5'></a>
<span class='cmssbx-10x-x-109'>Lemma 3.5.</span>  </span>If \( p_X\) is a probability mass function then it satisfies the conditions \begin {gather*}  p_X(x) \geq 0 \quad \forall x\quad \text {and}\\ \sum _{ x\in \mathcal {S}}p_X(x)=1.  \end {gather*}
</p>
</div>
<!-- l. 1199 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 1200 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 1205 --><p class='noindent'>In theory, any function \( p: \mathbb {N}_0 {\rightarrow } \mathbb {R}\), which satisfies \( p(x) \geq 0\) for \( x=0,1, \dots \) and \( \sum _{x=0}^{\infty } p(x) = 1\) is a p.m.f. of some random variable. The next few
exercises give some natural examples. In the next chapter we extend these to ones which correspond to
random variables of interest.
</p>
<div class='newtheorem'>
<!-- l. 1207 --><p class='noindent'><span class='head'>
<a id='x1-25002r6'></a>
<span class='cmssbx-10x-x-109'>Example 3.6.</span>  </span>A random variable \( X\) which has the same outcome \( k\) every time the experiment is
undertaken is a constant. The p.m.f. for this random variable is \[ p_X(x) = \begin {cases}1\quad&amp; \text {if }x=k;\\ 0\quad&amp; \text {if }x \neq k.\end {cases} \] This clearly satisfies the two
conditions.
</p>
</div>
<!-- l. 1215 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1218 --><p class='noindent'><span class='head'>
<a id='x1-25003r7'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.7.</span>  </span>Find the p.m.f. of the number of heads in three tosses of a fair coin. The sample
space is \( \Omega =\{\text {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}\}\)
</p>
</div>
<!-- l. 1222 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1225 --><p class='noindent'><span class='head'>
<a id='x1-25004r8'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.8.</span>  </span>Suppose \( \Omega = \{a, b, c, d\}\) and each outcome occurs with equal probability. The random variable \( X\) is
defined by \( X(a) = 2, X(b) = 4, X(c) = 3, X(d) = 2\). Write down the induced sample space and p.m.f. of \( X\).
</p>
</div>
<!-- l. 1228 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<div class='newtheorem'>
<!-- l. 1231 --><p class='noindent'><span class='head'>
<a id='x1-25005r9'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.9.</span>  </span>If a p.m.f. is specified by \( p_X(x) = c\) for \( x = 0, 1, \dots , m\) and \( p_X(x)=0\) otherwise, where \( c\) is constant, then determine the
value of \( c\).
</p>
</div>
<!-- l. 1233 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1236 --><p class='noindent'><span class='head'>
<a id='x1-25006r10'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.10.</span>  </span>If a p.m.f. is specified by \( p_X(x) = cx\) for \( x = 1, 2, 3, 4\) and \( p_X(x)=0\) otherwise, where \( c\) is constant, then determine
the value of \( c\).
</p>
</div>
<!-- l. 1238 --><p class='noindent'>
</p><!-- l. 1242 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>3.3   </span> <a id='x1-260003.3'></a>The probability of an event</h3>
<!-- l. 1243 --><p class='noindent'>If we are interested in evaluating the probability of some event occurring for a random variable, this can
easily be obtained from the p.m.f.
</p>
<div class='newtheorem'>
<!-- l. 1245 --><p class='noindent'><span class='head'>
<a id='x1-26001r11'></a>
<span class='cmssbx-10x-x-109'>Lemma 3.11.</span>  </span>Let \( E\subseteq \mathcal {S}\) be an event in the induced sample space. The probability of \(E\) is given by \[\mathbb {P}(X\in E) = \sum _{x\in E}p_X(x).\]
</p>
</div>
<!-- l. 1247 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<div class='proof'>
<!-- l. 1249 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<div class='newtheorem'>
<!-- l. 1251 --><p class='noindent'><span class='head'>
<a id='x1-26002r12'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.12.</span>  </span>The length of stay in hospital after surgery is modelled as a random variable \( X\). The
following table gives the p.m.f. for \( X\). </p>
<div class='center'>
<!-- l. 1255 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-4'><colgroup id='TBL-4-1g'><col id='TBL-4-1' /><col id='TBL-4-2' /></colgroup><colgroup id='TBL-4-3g'><col id='TBL-4-3' /><col id='TBL-4-4' /><col id='TBL-4-5' /><col id='TBL-4-6' /><col id='TBL-4-7' /><col id='TBL-4-8' /><col id='TBL-4-9' /><col id='TBL-4-10' /></colgroup><tr id='TBL-4-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-1-1' style='white-space:nowrap; text-align:center;'><span class='cmss-10'>Days stayed</span></td><td class='td11' id='TBL-4-1-2' style='white-space:nowrap; text-align:center;'>\(x\)</td><td class='td11' id='TBL-4-1-3' style='white-space:nowrap; text-align:center;'>\(4\)</td><td class='td11' id='TBL-4-1-4' style='white-space:nowrap; text-align:center;'>\(5\)</td><td class='td11' id='TBL-4-1-5' style='white-space:nowrap; text-align:center;'>\(6\)</td><td class='td11' id='TBL-4-1-6' style='white-space:nowrap; text-align:center;'>\(7\)</td><td class='td11' id='TBL-4-1-7' style='white-space:nowrap; text-align:center;'>\(8\)</td><td class='td11' id='TBL-4-1-8' style='white-space:nowrap; text-align:center;'>\(9\)</td><td class='td11' id='TBL-4-1-9' style='white-space:nowrap; text-align:center;'>\(10\)</td><td class='td11' id='TBL-4-1-10' style='white-space:nowrap; text-align:center;'><span class='cmss-10'>total</span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-2-1' style='white-space:nowrap; text-align:center;'> <span class='cmss-10'>Probability </span></td><td class='td11' id='TBL-4-2-2' style='white-space:nowrap; text-align:center;'>\( p_X(x)\)</td><td class='td11' id='TBL-4-2-3' style='white-space:nowrap; text-align:center;'>\(0.038\)</td><td class='td11' id='TBL-4-2-4' style='white-space:nowrap; text-align:center;'>\(0.114\)</td><td class='td11' id='TBL-4-2-5' style='white-space:nowrap; text-align:center;'>\(0.430\)</td><td class='td11' id='TBL-4-2-6' style='white-space:nowrap; text-align:center;'>\(0.300\)</td><td class='td11' id='TBL-4-2-7' style='white-space:nowrap; text-align:center;'>\(0.080\)</td><td class='td11' id='TBL-4-2-8' style='white-space:nowrap; text-align:center;'>\(0.030\)</td><td class='td11' id='TBL-4-2-9' style='white-space:nowrap; text-align:center;'>\(0.008\)</td><td class='td11' id='TBL-4-2-10' style='white-space:nowrap; text-align:center;'>  \(1\)  </td>
</tr></table></div></div>
<!-- l. 1262 --><p class='noindent'>Find the probability of being in hospital for:
   </p><dl class='enumerate'><dt class='enumerate'>
a. </dt><dd class='enumerate'>
   <!-- l. 1264 --><p class='noindent'>at most \( 6\) days;
   </p></dd><dt class='enumerate'>
b. </dt><dd class='enumerate'>
   <!-- l. 1265 --><p class='noindent'>between \( 5\) and \( 7\) days inclusive;
   </p></dd><dt class='enumerate'>
c. </dt><dd class='enumerate'>
   <!-- l. 1266 --><p class='noindent'>at least \( 7\) days.</p></dd></dl>
</div>
<!-- l. 1268 --><p class='noindent'>
</p>
<div class='newtheorem'>
                                                                                      
                                                                                      
<!-- l. 1271 --><p class='noindent'><span class='head'>
<a id='x1-26006r13'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.13.</span>  </span>Find the probability of an odd number of heads in \( 3\) tosses of a fair coin.
</p>
</div>
<!-- l. 1274 --><p class='noindent'>
</p><!-- l. 1278 --><p class='noindent'>A specific family of events in which we are often interested, particularly for continuous random variables, is \( \{x : x \leq m\}\)
for different values of \(m\); we can equivalently write this event as \(X\leq m\). These events are useful because for any
event \(E\subseteq \mathcal {S}\) we can calculate \(\mathbb {P}(E)\) from the probabilities of events of the type \( \{x : x \leq m\}\). For example, let \(E = \{4,5,6\}\). Then \[ \{x: x\leq 6\} = \{x: x\leq 3\} \cup E,\] a disjoint
union. Therefore, by Axiom 3, \[ \mathbb {P}(E)={\mathbb {P}(X\leq 6) - \mathbb {P}(X\leq 3).}\]
</p>
<div class='center'>
<!-- l. 1289 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>cumulative distribution function</span> or c.d.f. of a random variable \( X\) is a
function \( F_X: \mathbb {X} \rightarrow \mathbb {X}\) given by \[F_X(m) = \mathbb {P} (X \leq m).\] For a discrete random variable \( X\) with induced sample space \(\mathcal {S}\subseteq \mathbb N_0\),
the cumulative distribution function is given by \[F_R(m) = \mathbb {P} (X \leq m) = \sum _{x=0}^{\lfloor m\rfloor } p_X(x).\]                           </div>
</div>
<div class='newtheorem'>
<!-- l. 1291 --><p class='noindent'><span class='head'>
<a id='x1-26007r14'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.14.</span>  </span>What is the cumulative distribution function for a random variable \( X\) whose p.m.f. is
specified by \( p_X(x) = x/10\) for \( x = 1, 2, 3, 4\)?
</p>
</div>
<!-- l. 1293 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1294 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Solution.</span>  </span>Let us first compute some values. We have:
</p><!-- l. 1297 --><p class='noindent'>Of course, we’re not done, as we need to specify \(F_X(x)\) for any real number \(x\) (since the c.d.f. is a function
\(\mathbb {R}\to \mathbb {R}\)). For this, we notice that for any \(x&lt; 1\), we have \(F_X(x)=0\) since \(X\) as given in the problem cannot produce any
                                                                                      
                                                                                      
value that is less than \(1\). Similarly, for any \(rx\in [1,2)\), we have \(F_X(x)=\mathbb {P}(X\leq x)=\mathbb {P}(X=1)={1/10\quad }\) since \(X\) as given in the problem is not allowed to
take on non-integer values. Continuing in this vein, we obtain the full expression for \(F_X\), namely:
</p><!-- l. 1300 --><p class='noindent'>Now we’re done.
</p><!-- l. 1302 --><p class='noindent'>Though we weren’t asked to do this, it is instructive to also plot \(F_X\). It has the following graph:
</p>
<div class='center'>
<!-- l. 1304 --><p class='noindent'>
</p><!-- l. 1305 --><p class='noindent'><img alt='PIC' height='256' src='cdf.png' width='256' /></p></div>
<!-- l. 1311 --><p class='noindent'>In this case, the c.d.f. starts at the value \(0\), proceeds in jumps whose heights are the values of the p.m.f.,
and ends at the value \(1\). Is this an accident?
</p><!-- l. 1313 --><p class='noindent'>More generally, a cumulative distribution function \(F_X\) will have the following properties: </p>
     <ul class='itemize1'>
     <li class='itemize'>\(F_X\) is non-decreasing, meaning that if \(x_1&lt;x_2\), we must have \(F_X(x_1)\leq F_X(x_2)\).
     </li>
     <li class='itemize'>\(\displaystyle \lim _{r\to -\infty } F_X(x)=0\).
     </li>
     <li class='itemize'>\(\displaystyle \lim _{r\to \infty } F_X(x)=1\).
     </li>
     <li class='itemize'>For any \(\displaystyle x_0\in \mathbb {R}\), \(\mathbb {P}(X=x_0)=F_X(x_0)-\lim _{x\to x_0^-} F_X(x)\). (Here, \(\lim _{x\to x_0^-} F_X(x)\) denotes the limit of \(F_X(x)\) as \(x\) approaches \(x_0\) from the left.)</li></ul>
<!-- l. 1320 --><p class='noindent'>These properties will be satisfied by any cumulative distribution function, including in the case of
continuous random variables, and even more generally. You may wish to use these properties to check that
your c.d.f. (on a coursework, on an exam) was indeed computed correctly. (For instance, if your
c.d.f. starts decreasing, you must have made a mistake.)
</p>
</div>
<!-- l. 1321 --><p class='noindent'>
</p><!-- l. 1323 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<h3 class='sectionHead'><span class='titlemark'>3.4   </span> <a id='x1-270003.4'></a>Expectation</h3>
<!-- l. 1324 --><p class='noindent'>Expectation is a simple measure to calculate the average value taken by a random variable.
</p><!-- l. 1327 --><p class='noindent'>Suppose the outcome of an experiment is the random variable \( X\). If the experiment is repeated, we observe
outcomes \( x_1,x_2,\ldots \). The mean observed value of \( X\) is \[\frac {x_1+x_2+\cdots +x_n}{n}.\] Let \( n_x\) be the number of times that \( x\) is observed in the \( n\)
experiments. Then \[ x_1+x_2+\cdots +x_n = \sum _{ x\in \mathcal {S}} x n_x.\] We motivated probability with the notion (as yet unproved) that \[ \frac {n_x}{n} \rightarrow \mathbb {P}(X=x) = p_X(x)\] as \( n\rightarrow \infty \). If this holds then
\begin {align*}  \frac {x_1+x_2+\cdots +x_n}{n} &amp;= \frac {\sum _{ x\in \mathcal {S}} x n_x}{n}\\ &amp;= \sum _{ x\in \mathcal {S}} x \frac {n_x}{n}\\ &amp;\rightarrow \sum _{ x\in \mathcal {S}} x p_X(x). \end {align*}
</p><!-- l. 1338 --><p class='noindent'>This motivates the following definition: </p>
<div class='center'>
<!-- l. 1342 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>expected value</span> or <span class='cmssbx-10x-x-109'>expectation</span> of a discrete random variable \( R\) is written \(\mathbb {E}[X]\)
and defined by \[\mathbb {E}[X]=\sum _{ x\in \mathcal {S}}x\,p_X(x).\]                                                        </div>
</div>
<!-- l. 1347 --><p class='noindent'>The expectation is also known as the <span class='cmssbx-10x-x-109'>mean </span>or the <span class='cmssbx-10x-x-109'>first moment </span>of a random variable.
</p><!-- l. 1349 --><p class='noindent'>An equivalent expression of the expectation that can be useful if the p.m.f. of a random variable is not
known is the following: \[ \mathbb {E}[X]=\sum _{\omega \in \Omega } X(\omega )\mathbb {P}(\{\omega \}). \] The equivalence between these two expressions can be shown using the way the
p.m.f. is defined, and is left as an exercise.
</p>
<div class='newtheorem'>
<!-- l. 1354 --><p class='noindent'><span class='head'>
<a id='x1-27001r15'></a>
<span class='cmssbx-10x-x-109'>Example 3.15.</span>  </span>Recall the gambling exercise above, where the random variable \( X\), profit, is defined
by \[ X(\omega )=\begin {cases}-1 &amp;\text { if } \omega =1,2,3 \\ 0 &amp;\mbox { if } \omega = 4\\ 2 &amp;\mbox { if } \omega = 5,6 \end {cases} \] from the throw of a fair die. The induced sample space for \( X\) is \( \mathcal {S}=\{-1,0,2\}\). The p.m.f. of \( X\) is \( p_X(-1)=3/6\), \( p_X(0)=1/6\), \( p_X(2)=2/6\). Let us find
the expected profit, using both definitions above.
</p><!-- l. 1369 --><p class='noindent'>Using the first definition:
</p><!-- l. 1372 --><p class='noindent'>Using the second definition:
</p><!-- l. 1375 --><p class='noindent'>Both give the same result of \( \pounds 1/6\).
</p>
</div>
<!-- l. 1376 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1378 --><p class='noindent'><span class='head'>
<a id='x1-27002r16'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Exercise 3.16.</span>  </span>Find the expected number of heads in three tosses of a fair coin.
</p>
</div>
<!-- l. 1381 --><p class='noindent'>
Note \(\mathbb {P}(\{X=\mathbb {E}[X]\})=0\), which may seem surprising. The expected value of a random variable is <span class='cmssbx-10x-x-109'>not</span> the value that you
expect to obtain.
</p><!-- l. 1385 --><p class='noindent'>A similar calculation gives the expected number of tails is \( 1.5\). This accords with intuition: we expect <span class='cmssbx-10x-x-109'>on
average</span> the same number of heads and tails for a fair coin.
</p>
<div class='newtheorem'>
<!-- l. 1389 --><p class='noindent'><span class='head'>
<a id='x1-27003r17'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.17.</span>  </span> Find the expected value of the score on a fair die.
</p>
</div>
<!-- l. 1391 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1394 --><p class='noindent'><span class='head'>
<a id='x1-27004r18'></a>
<span class='cmssbx-10x-x-109'>Example 3.18.</span>  </span>For any event \( A \subseteq \Omega \), the <span class='cmssbx-10x-x-109'>indicator function</span> of \( A\) is the function \( I_A:\Omega \rightarrow \{ 0,1\}\) such that \( I_A(\omega ) = 1\) if \( \omega \in A\), and \( I_A(\omega ) = 0\)
otherwise. Since \( I_A\) is a real-valued function on \( \Omega \), it is a random variable. What is its expected value?
</p>
</div>
<!-- l. 1396 --><p class='noindent'>
</p><!-- l. 1398 --><p class='noindent'>If \( g : \mathbb {R} \rightarrow \mathbb {R}\) is any real valued function, then \( g(X) = g \circ X\) is a function from \( \Omega \to \mathbb {R}\), so is also a random variable. We can therefore
work out its expected value:
</p><!-- l. 1401 --><p class='noindent'>We have proved the following:
</p>
<div class='newtheorem'>
<!-- l. 1402 --><p class='noindent'><span class='head'>
<a id='x1-27005r19'></a>
<span class='cmssbx-10x-x-109'>Lemma 3.19.</span>  </span>The expected value of a function \( g\) of a discrete random variable \( X\) is \[ \mathbb {E}[g(X)]=\sum _{ x\in \mathcal {S}}g(x)\,p_X(x). \]
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 1407 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1410 --><p class='noindent'><span class='head'>
<a id='x1-27006r20'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.20.</span>  </span>If \( p_X(x) = \frac {1}{3}\) for \( x = 0, 1, 2\), find \( \mathbb {E}[X^2]\).
</p>
</div>
<!-- l. 1412 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1416 --><p class='noindent'><span class='head'>
<a id='x1-27007r21'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.21.</span>  </span>Find \( \mathbb {E}[X^3 + 2X]\) if \( p_X(1) = 3/4\) and \( p_X(2) = 1/4\).
</p>
</div>
<!-- l. 1418 --><p class='noindent'>
</p><!-- l. 1420 --><p class='noindent'>Expectation obeys two important rules of linearity. For arbitrary functions \( g\) and \( h\), and constant \( c\):
\begin {gather*}  \mathbb {E}[g (X) + h (X) ] = \mathbb {E}[g (X)] + \mathbb {E}[ h(X)] \\ \mathbb {E}[cg(X)] = c \mathbb {E}[g(X)].  \end {gather*}
A special case is that \( \mathbb {E}[c] = c\).
</p><!-- l. 1429 --><p class='noindent'>These results can be verified using the definition of the expectation of a function. We show how to obtain
the first identity; the others are obtained similarly.
</p>
<div class='newtheorem'>
<!-- l. 1433 --><p class='noindent'><span class='head'>
<a id='x1-27008r22'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.22.</span>  </span>Find \( \mathbb {E}[X]\) if it is known that \( \mathbb {E}[X(X-1)] =4\) and \( \mathbb {E}[X^2]=3\).
</p>
</div>
<!-- l. 1435 --><p class='noindent'>
                                                                                      
                                                                                      
</p><!-- l. 1438 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>3.5   </span> <a id='x1-280003.5'></a>Variance</h3>
<!-- l. 1439 --><p class='noindent'>Expectation is a weighted average of the possible values of a random variable, and consequently is a
measure of the location of the p.m.f.
</p><!-- l. 1442 --><p class='noindent'>The spread, or dispersion, of a random variable is usually measured by either the variance or the standard
deviation, two closely related concepts that we define below.
</p>
<div class='center'>
<!-- l. 1453 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>variance</span> of the random variable \( X\), written \( \mathrm {Var}(X)\), is defined as \[ \mathrm {Var}(X) = \mathbb {E}((X - \mathbb {E}[X])^2), \] The <span class='cmssbx-10x-x-109'>standard
deviation</span> of \( X\), written \( \mathrm {s.d.}(X)\), is defined as the square root of the variance: \[\mathrm {s.d.}(X)=\sqrt {\mathrm {Var}(X)}.\]        </div>
</div>
<!-- l. 1455 --><p class='noindent'>The variance is the expectation of the function of the random variable \( g(X)=(X-m)^2\), where \( m=\mathbb {E}(X)\) is a real number.
</p><!-- l. 1458 --><p class='noindent'>The standard deviation quantifies how far away from the mean we can expect a random
variable to typically be. To get a feel for this, in practice (and in theory that appears in later
courses),<span class='footnote-mark'><a href='LNforHTML2.html#fn1x4'><sup class='textsuperscript'>1</sup></a></span><a id='x1-28001f1'></a> 
many random variables we deal with are within \(\pm 2\) standard deviations of the mean approximately \( 95\%\) of the
time. However, not every random variable has this property, as we shall see at the end of this
chapter.
</p>
<div class='newtheorem'>
<!-- l. 1464 --><p class='noindent'><span class='head'>
<a id='x1-28002r23'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.23.</span>  </span>Suppose that four random variables \( X_1\), \( X_2\), \( X_3\) and \( X_4\) on \( \mathcal {S}=\{0,1,2\}\) have p.m.f. \[ \begin {array}{r|rrr} &amp;0&amp;1&amp;2\\ \hline p_1(x)&amp; 0&amp;1&amp;0\\ p_2(x)&amp;1/4&amp;1/2&amp;1/4\\ p_3(x)&amp;1/3&amp;1/3&amp;1/3\\ p_4(x)&amp;1/2&amp;0&amp;1/2\\ \hline \end {array}\] respectively. These are
plotted below. </p>
<div class='center'>
<!-- l. 1475 --><p class='noindent'>
                                                                                      
                                                                                      
</p><!-- l. 1475 --><p class='noindent'><span class='rotatebox' style='transform: rotate(-90deg);'> <img alt='PIC' height='170' src='varpmf.png' width='170' /></span></p></div>
<!-- l. 1476 --><p class='noindent'>Note for each p.m.f. the sum of the probabilities is \( 1\). The expectations are the same so that \[\mathbb {E}[X_1]=\mathbb {E}[X_2]=\mathbb {E}[X_3]=\mathbb {E}[X_4]=1.\] Find the
variances.
</p>
</div>
<!-- l. 1479 --><p class='noindent'>
</p><!-- l. 1481 --><p class='noindent'>This formulation of the variance is inconvenient for calculation, so we next derive an equivalent alternative
form which is often easier to use in practice. Writing \( \mathbb {E}[X]\) as \( m\), a constant, we have
</p>
<div class='newtheorem'>
<!-- l. 1487 --><p class='noindent'><span class='head'>
<a id='x1-28003r24'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.24.</span>  </span>Find the variance of a random number \(X\) uniformly distributed on the integers \(1,2,\dots ,6\).
</p>
</div>
<!-- l. 1490 --><p class='noindent'>
</p><!-- l. 1492 --><p class='noindent'>As with linearity for expectation there is an important result for the variance of linear transformations of a
random variable \(X\). Suppose \( a\) and \( b\) are constants. We will determine \(\mathrm {Var}(aX+b)\) in terms of \(\mathrm {Var}(X)\).
</p><!-- l. 1497 --><p class='noindent'>This result shows how variance is affected by linear transformations: \begin {align*}  \mathrm {Var}(X+b)&amp;= \mathrm {Var}(X)\\ \mathrm {Var}(aX) &amp;= a^2\mathrm {Var}(X).  \end {align*}
</p>
<div class='newtheorem'>
<!-- l. 1504 --><p class='noindent'><span class='head'>
<a id='x1-28004r25'></a>
<span class='cmssbx-10x-x-109'>Exercise 3.25.</span>  </span>For a random variable \( X\), \( \mathbb {E}[X] = 3\) and \( \mathrm {Var}(X) = 2\). Find
    </p><dl class='enumerate'><dt class='enumerate'>
 i. </dt><dd class='enumerate'>
    <!-- l. 1508 --><p class='noindent'>\( \mathbb {E}[2X]\) \(=2\mathbb {E}[X]=6\).
    </p></dd><dt class='enumerate'>
 ii. </dt><dd class='enumerate'>
    <!-- l. 1509 --><p class='noindent'>\( \mathbb {E}[-2X + 6]\) \(=-2\mathbb {E}[X] + 6=0\).
    </p></dd><dt class='enumerate'>
 iii. </dt><dd class='enumerate'>
                                                                                      
                                                                                      
    <!-- l. 1510 --><p class='noindent'>\( \mathrm {Var}(2X)\) \(=2^2\mathrm {Var}(X)=8\).
    </p></dd><dt class='enumerate'>
 iv. </dt><dd class='enumerate'>
    <!-- l. 1511 --><p class='noindent'>\( \mathrm {Var}(-2X + 6)\) \(=(-2)^2\mathrm {Var}(X)=8\).</p></dd></dl>
</div>
<!-- l. 1513 --><p class='noindent'>
</p><!-- l. 1515 --><p class='noindent'>In summary </p>
<div class='center'>
<!-- l. 1522 --><p class='noindent'>
</p>
<div class='fbox'>\begin {align*}  \mathrm {Var}(X) &amp;= \mathbb {E}[(X - \mathbb {E}(X))^2]\\ &amp; = \mathbb {E}[X^2] -(\mathbb {E}[X])^2.  \end {align*}
It satisfies \[\mathrm {Var}(aX+b) = a^2\mathrm {Var}(X).\]                                                            </div>
</div>
<h3 class='sectionHead'><span class='titlemark'>3.6   </span> <a id='x1-290003.6'></a>Chebyshev’s inequality (not examinable)</h3>
<!-- l. 1526 --><p class='noindent'>A first step to understanding why the variance matters is given by <span class='cmssbx-10x-x-109'>Chebyshev’s inequality</span>. Let \( X\) be <span class='cmssbx-10x-x-109'>any</span>
random variable. Suppose \( \mathbb {E}(X)=m\) and \( \mathrm {Var}(X)=\sigma ^2\). Let \( c&gt;0\) be any constant: we will find a bound on the probability \[\mathbb {P}(|X-m|\geq c\sigma )\] that \( X\) is at
least \( c\) standard deviations away from its expected value.
</p><!-- l. 1532 --><p class='noindent'>Let \( A\) be the event that \( |X-m|\geq c\sigma \), and let \( I_A\) be the indicator of \( A\). Recall that \[\mathbb {E}[I_A] = 1\times \mathbb {P}(I_A=1) + 0\times \mathbb {P}(I_A=0) = \mathbb {P}(A).\] Also define the function \( g(x) = (x-m)^2/(c\sigma )^2\), and notice that \( g(x) \geq 0 \)
for all \( x\), and \( g(x) \geq 1 \) whenever \( |X-m|\geq c\sigma \), i.e. whenever \(A\) occurs.
</p><!-- l. 1545 --><p class='noindent'>So if \( A\) does not occur, then \( I_A=0\leq g(X)\).
</p><!-- l. 1547 --><p class='noindent'>And if \( A\) does occur, then \( I_A=1\leq g(X)\).
</p><!-- l. 1549 --><p class='noindent'>So \( I_A\leq g(X)\) and it follows that
</p><!-- l. 1551 --><p class='noindent'>This is Chebyshev’s inequality. It is the strongest inequality that is true for <span class='cmssbx-10x-x-109'>every </span>random variable.
</p><!-- l. 1553 --><p class='noindent'>Suppose \(c&gt;1\) and \(X\) is a random variable with p.m.f. \(p_X(0)=\frac {1}{2c^2}\), \(p_X(1)=1-\frac {1}{c^2}\) and \(p_X(2)=\frac {1}{2c^2}\). Then we can calculate that \(\mathbb {E}[X]=1\) and \(\mathrm {s.d.}(X)=1/c\). But \[\mathbb {P}(|X-1|\geq c\mathrm {s.d.}(X))=\mathbb {P}(|X-1|\geq 1)=p_X(0)+p_X(2)=\frac {1}{c^2},\] which is
exactly the bound from Chebyshev’s inequality.
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 4</span><br /><a id='x1-300004'></a> Models for discrete random variables</h2>
<!-- l. 1559 --><p class='noindent'>From a mathematical viewpoint any sequence of numbers, \(p(x)\), satisfying </p>
<div class='center'>
<!-- l. 1561 --><p class='noindent'>
</p><!-- l. 1562 --><p class='noindent'>\(p(x) \geq 0\) for all \( x\) and \(\sum _{x=0}^{\infty }p(r)=1\),</p></div>
<!-- l. 1564 --><p class='noindent'>is a valid p.m.f. corresponding to some random variable. However, we are interested mainly in those
random variables which arise from experiments of practical relevance, and thus form plausible models for
future statistical modelling.
</p><!-- l. 1569 --><p class='noindent'>In this chapter we examine some examples of random variables that result from experiments with
well-defined physical mechanisms.
</p>
<h3 class='sectionHead'><span class='titlemark'>4.1   </span> <a id='x1-310004.1'></a>Useful mathematical identities proved elsewhere</h3>
<!-- l. 1573 --><p class='noindent'>The following identities are helpful in the subsequent developments.
</p>
<!-- l. 1575 --><p class='noindent'><span class='paragraphHead'><a id='x1-320004.1'></a><span class='cmssbx-10x-x-109'>Arithmetic progression:</span></span>
\[ \sum _{i=0}^n i = n(n+1)/2. \]
</p>
<!-- l. 1580 --><p class='noindent'><span class='paragraphHead'><a id='x1-330004.1'></a><span class='cmssbx-10x-x-109'>Sum of squares:</span></span>
\[ \sum _{i=0}^n i^2 = n(n+1)(2n+1)/6. \]
</p>
<!-- l. 1585 --><p class='noindent'><span class='paragraphHead'><a id='x1-340004.1'></a><span class='cmssbx-10x-x-109'>Exponential series:</span></span>
\begin {equation*}  \exp (x) = 1+x+\frac {x^2}{2!}+\frac {x^3}{3!}+\cdots = \sum _{i=0}^{\infty } \frac {x^i}{i!}  \end {equation*}
Note also that: \begin {gather*}  \exp (x+y) = \exp (x) \exp (y)\\ \exp (-x) = 1/{\exp (x)}  \end {gather*}
</p><!-- l. 1595 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-350004.1'></a>Geometric type sums</h4>
                                                                                      
                                                                                      
<!-- l. 1596 --><p class='noindent'><span class='paragraphHead'><a id='x1-360004.1'></a><span class='cmssbx-10x-x-109'>Partial geometric sum:</span></span>
\[ S_m = 1+x+ x^2+ \ldots +x^m = \frac {1-x^{m+1}}{1-x} \]
</p>
<!-- l. 1600 --><p class='noindent'><span class='paragraphHead'><a id='x1-370004.1'></a><span class='cmssbx-10x-x-109'>Geometric sum:</span></span>
\begin {equation}  S_{\infty } = 1+x+ x^2+ \cdots = \frac {1}{1-x}\label {geo-sum}  \end {equation}
for \(|x |&lt;1\).
</p>
<!-- l. 1606 --><p class='noindent'><span class='paragraphHead'><a id='x1-380004.1'></a><span class='cmssbx-10x-x-109'>Weighted geometric sum:</span></span>
Using the result for geometric sums further results can be derived. Assuming \(|x|&lt;1\), we can differentiate
(<span class='cmssbx-10x-x-109'>??</span>) and then multiply by \(x\) to obtain \[ \sum _{i=0}^\infty ix^i=x+2x^2+3x^3+\cdots =\frac {x}{(1-x)^2}. \] Applying a similar process to the equation above gives
\[ \sum _{i=0}^\infty i^2x^i=x+4x^2+9x^3+\cdots =\frac {x+x^2}{(1-x)^3}. \]
</p><!-- l. 1617 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-390004.1'></a>Binomial expansion</h4>
<!-- l. 1618 --><p class='noindent'>For any positive integer \( n\) \begin {align*}  (p+q)^n &amp; = \binom {n}{0}p^nq^0+\binom {n}{1}p^{n-1}q^1+\cdots + \binom {n}{i}p^{n-i}q^{i}+ \ldots + \binom {n}{n}p^0q^n\\ &amp; = \sum _{i=0}^{n}\binom {n}{i}p^{n-i}q^{i}.  \end {align*}
</p><!-- l. 1627 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.2   </span> <a id='x1-400004.2'></a>Discrete uniform random variables</h3>
<!-- l. 1628 --><p class='noindent'>Consider an experiment where the sample space is \( \{a,a+1,\ldots ,b\}\) for some integers \(a,b\) with \(0\leq a&lt;b\), and the random variable \(X\)
corresponds to an outcome being picked at random from the sample space. Each outcome is <span class='cmssbx-10x-x-109'>equally
likely</span>.
</p><!-- l. 1633 --><p class='noindent'>We write this as \(X\sim \operatorname {Uniform}(a,b)\).
</p><!-- l. 1635 --><p class='noindent'>Examples include: </p>
     <ul class='itemize1'>
     <li class='itemize'>the score on a fair die, where \(a=1\) and \(b=6\);
     </li>
     <li class='itemize'>the date of someone’s birthday, given that they were born in January (assuming all birthdays
     are equally likely), where \(a=1\) and \(b=31\).</li></ul>
<div class='newtheorem'>
<!-- l. 1640 --><p class='noindent'><span class='head'>
<a id='x1-40001r1'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.1.</span>  </span>Write down the p.m.f. of a discrete uniform random variable.
</p>
</div>
<!-- l. 1642 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1644 --><p class='noindent'><span class='head'>
<a id='x1-40002r2'></a>
<span class='cmssbx-10x-x-109'>Example 4.2.</span>  </span>Calculate the expectation and variance of a discrete uniform random variable.
</p>
</div>
<!-- l. 1644 --><p class='noindent'>
</p><!-- l. 1648 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.3   </span> <a id='x1-410004.3'></a>Bernoulli random variables</h3>
<!-- l. 1649 --><p class='noindent'>Jacob Bernoulli (1654–1705) was a member of a prolific scientific family: at least twelve members
contributed to some branch of mathematics or physics and at least five to probability. Jacob and his brother
John were great rivals and would only communicate in print arguing over the correctness of each
other’s mathematical proofs. Jacob gives his name to the simplest family of non-uniform random
variables.
</p><!-- l. 1652 --><p class='noindent'>Consider an experiment where the sample space is \( \{0,1\}\) and the probability of a \( 1\) is \( \theta \) \( (0\leq \theta \leq 1)\). A random variable \( X\) with
such a p.m.f. is termed a <span class='cmssbx-10x-x-109'>Bernoulli random variable</span> with <span class='cmssbx-10x-x-109'>parameter</span> \(\theta \). Examples include:
</p>
     <ul class='itemize1'>
     <li class='itemize'>number of heads on a single toss a of biased coin;
     </li>
     <li class='itemize'>the number of positive results on a single COVID test;
     </li>
     <li class='itemize'>\(1\) if a candidate passes an exam and \(0\) otherwise;
     </li>
     <li class='itemize'>\(1\) if the next baby is a girl, \(0\) otherwise (here \(\theta \approx 0.49\)).</li></ul>
<!-- l. 1661 --><p class='noindent'>Here outcomes of \(1\) and \(0\) are sometimes called “success” and “failure” respectively – although, as in the
COVID example above, “failure” can be the better outcome! Note that all of these examples give an
indicator function for some event, and indicator functions of events are always Bernoulli random
variables.
</p><!-- l. 1663 --><p class='noindent'>For Bernoulli random variables \[\mathbb {P}(X=1)=\theta ,\quad \mathbb {P}(X=0)=1-\theta ,\] and \( \mathbb {P}(X=x)=0\) otherwise. </p>
<div class='center'>
<!-- l. 1672 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>p.m.f. of a Bernoulli random variable</span> \( X\) is \[p_X(i)=\begin {cases}(1-\theta )&amp;\text { if }i=0\\ \theta &amp;\text { if }i=1\\ 0&amp;\text { otherwise.}\end {cases} \] We write \( X\sim \mathrm {Bernoulli}(\theta )\).                </div>
</div>
<div class='newtheorem'>
<!-- l. 1675 --><p class='noindent'><span class='head'>
<a id='x1-41001r3'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.3.</span>  </span>Find the expectation and variance of a Bernoulli random variable.
</p>
</div>
<!-- l. 1677 --><p class='noindent'>
</p><!-- l. 1679 --><p class='noindent'>Notice if \(\theta =0\) or \(\theta =1\) the variance is \(0\), whereas the maximum possible variance is \(0.25\) when \(\theta =0.5\). Is this logical?
</p><!-- l. 1683 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.4   </span> <a id='x1-420004.4'></a>Binomial random variables</h3>
<!-- l. 1684 --><p class='noindent'>Consider an experiment in which \(n\) independent Bernoulli trials are carried out, each with probability of
success being \(\theta \). Let \(X\) be the random variable reporting the number of successes in these \(n\) trials. The induced
sample space is \(\{0,1, \ldots ,n\}\quad \). The random variable \( X\) is a <span class='cmssbx-10x-x-109'>binomial random variable</span> with <span class='cmssbx-10x-x-109'>parameters</span> \(n\) and \(\theta \). We write
\(X\sim \mathrm {Bin}(n,\theta )\).
                                                                                      
                                                                                      
</p><!-- l. 1687 --><p class='noindent'>Examples include: </p>
     <ul class='itemize1'>
     <li class='itemize'>the number of heads in \(n\) tosses of a biased coin;
     </li>
     <li class='itemize'>the number of positive COVID tests when \(n\) people are tested;
     </li>
     <li class='itemize'>the number of girls among the next \(n\) babies born.</li></ul>
<!-- l. 1695 --><p class='noindent'>The derivation is a little more complex here so first consider the \( n=3\) case with S and F denoting success and
failure respectively. The sample space for the experiment is \[ \Omega = \{\text {SSS, SSF, SFS, FSS, SFF, FSF, FFS, FFF}\}. \] The random variable of interest, \( X\), is the
number of successes.
</p>
<div class='newtheorem'>
<!-- l. 1702 --><p class='noindent'><span class='head'>
<a id='x1-42001r4'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.4.</span>  </span>Find \(\mathbb {P}(X=x)\) for \( x=0,1,2,3\).
</p>
</div>
<!-- l. 1704 --><p class='noindent'>
</p><!-- l. 1707 --><p class='noindent'>We can summarise these results as \[ p_X(x)=\binom {3}{x}\theta ^x (1-\theta )^{3-x} \] for \(x=0,1,2,3\).
</p><!-- l. 1713 --><p class='noindent'>The more general form for the p.m.f. is as follows:
</p>
<div class='newtheorem'>
<!-- l. 1714 --><p class='noindent'><span class='head'>
<a id='x1-42002r5'></a>
<span class='cmssbx-10x-x-109'>Lemma 4.5 </span>(The p.m.f. of a binomial random variable)<span class='cmssbx-10x-x-109'>.</span>  </span>The p.m.f. of a binomial random variable
\(X\sim \mathrm {Bin}(n,\theta )\) is \[p_X(r) = \binom {n}{r} \theta ^r (1-\theta )^{n-r}\] for \(r=0,1,2,\dots ,n\), with \(p_X(r)=0\) otherwise, where \( 0\leq \theta \leq 1\).
</p>
</div>
<!-- l. 1719 --><p class='noindent'>
</p>
<div class='proof'>
                                                                                      
                                                                                      
<!-- l. 1722 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>                                                                                                                 □
</p>
</div>
<div class='newtheorem'>
<!-- l. 1724 --><p class='noindent'><span class='head'>
<a id='x1-42003r6'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.6.</span>  </span>Show that \(\sum _{r=0}^m p_X(r) = 1\)
</p>
</div>
<!-- l. 1724 --><p class='noindent'>
</p><!-- l. 1727 --><p class='noindent'>The software package R can evaluate p.m.f.s from many standard probability models, including the
binomial.
</p>
<div class='newtheorem'>
<!-- l. 1729 --><p class='noindent'><span class='head'>
<a id='x1-42004r7'></a>
<span class='cmssbx-10x-x-109'>Example 4.7.</span>  </span>The random variable \( X\sim \mathrm {Bin}(3,0.5)\). Use R to evaluate and plot the p.m.f. of \( X\). Repeat with
\( \theta =0.4\).
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-1'>
dbinom(0:3,size=3,prob=0.5)
dbinom(0:3,size=3,prob=0.4)
# Note how the probabilities change.
p = dbinom(0:3,size=3,prob=0.5)
barplot(p, names.arg=c(0:3))</pre>
<!-- l. 1739 --><p class='nopar'> </p>
<div class='center'>
<!-- l. 1740 --><p class='noindent'>
</p><!-- l. 1741 --><p class='noindent'><span class='rotatebox' style='transform: rotate(-90deg);'> <img alt='PIC' height='170' src='bino3.png' width='170' /></span></p></div>
</div>
<!-- l. 1743 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1746 --><p class='noindent'><span class='head'>
<a id='x1-42005r8'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.8.</span>  </span>Find the probability of rolling a fair die and finding
    </p><dl class='enumerate'><dt class='enumerate'>
 i. </dt><dd class='enumerate'>
    <!-- l. 1749 --><p class='noindent'>two sixes in four rolls,
    </p></dd><dt class='enumerate'>
 ii. </dt><dd class='enumerate'>
    <!-- l. 1750 --><p class='noindent'>two sixes in five rolls,
    </p></dd><dt class='enumerate'>
 iii. </dt><dd class='enumerate'>
    <!-- l. 1751 --><p class='noindent'>at least two sixes in four rolls.</p></dd></dl>
</div>
<!-- l. 1755 --><p class='noindent'>
</p><!-- l. 1758 --><p class='noindent'>We could calculate these in R using the following commands:
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-2'>
dbinom(2,size=4,prob=1/6)
dbinom(2,size=5,prob=1/6)
1-dbinom(0,size=4,prob=1/6)-dbinom(1,size=4,prob=1/6)</pre>
<!-- l. 1763 --><p class='nopar'>
</p>
<div class='newtheorem'>
<!-- l. 1766 --><p class='noindent'><span class='head'>
<a id='x1-42009r9'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.9.</span>  </span>There are two families each with three children. Suppose each child is independently
a girl with probability \(1/2\). Find the probability that the families have the same number of girls.
</p><!-- l. 1771 --><p class='noindent'>R hint: <span class='obeylines-h'><span class='verb'><span class='cmtt-10x-x-109'> sum( dbinom(0:3, size=3, prob=1/2)^2 )</span></span></span>
</p>
</div>
<!-- l. 1772 --><p class='noindent'>
</p><!-- l. 1777 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-430004.4'></a>Expectation and variance</h4>
<!-- l. 1778 --><p class='noindent'>Using the definitions and algebraic manipulation gives </p>
<div class='center'>
<!-- l. 1783 --><p class='noindent'>
</p>
<div class='fbox'>For a binomial random variable \( X\sim \mathrm {Bin}(n,\theta )\) \begin {align*}  \mathbb {E}[X] &amp;= n\theta \\ \mathrm {Var}(X) &amp;= n\theta (1-\theta ).  \end {align*}                                        </div>
</div>
<!-- l. 1786 --><p class='noindent'>The general proof is given in a worksheet solution. We instead consider a special case:
</p>
<div class='newtheorem'>
<!-- l. 1787 --><p class='noindent'><span class='head'>
<a id='x1-43001r10'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Example 4.10.</span>  </span>If \( X\sim \mathrm {Bin}(3,\theta )\) show that \( \mathbb {E}[X]=3\theta \).
</p>
</div>
<!-- l. 1789 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1793 --><p class='noindent'><span class='head'>
<a id='x1-43002r11'></a>
<span class='cmssbx-10x-x-109'>Practice question 4.11.</span>  </span>The popular website Dialtome shows its users adverts when they log in,
and gains revenue when users click on adverts, so its owners need to predict how many times this
happens. Suppose that 900 million users log in any given day, and each user independently clicks an
advert with probability \(0.01\). What is the expected number of clicks in a day, and the variance of this
number?
</p>
</div>
<!-- l. 1795 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1799 --><p class='noindent'><span class='head'>
<a id='x1-43003r12'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.12.</span>  </span> Suppose an experiment is carried out \( n\) times, let \( A\) be an event associated with the
experiment, and let \( \theta \) be the probability of the event \( A\).
</p><!-- l. 1802 --><p class='noindent'>Let \( X\) count the number of times that event \( A\) occurs in the \( n\) experiments. \( X\) is therefore a \( \mathrm {Bin}(n,\theta )\) random
variable.
</p><!-- l. 1805 --><p class='noindent'>Calculate the expectation and variance of \( X/n\), the proportion of times that \( A\) occurs. Use Chebyshev’s
inequality to say something about how close \( X/n\) is to \( \theta \) for large \( n\).
</p>
</div>
<!-- l. 1807 --><p class='noindent'>
</p><!-- l. 1810 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<h3 class='sectionHead'><span class='titlemark'>4.5   </span> <a id='x1-440004.5'></a>Geometric random variables</h3>
<!-- l. 1811 --><p class='noindent'>Consider an experiment based on independent Bernoulli trials, each with the probability of a success being
\( \theta \).
</p><!-- l. 1814 --><p class='noindent'>Now define the variable of interest, \( X\), to be the number of trials up to but <span class='cmssbx-10x-x-109'>not including </span>the first
success.
</p><!-- l. 1817 --><p class='noindent'>Here the induced sample space is \( \mathcal {S}=\{0,1,2,\ldots \}\), and is infinite, corresponding to outcomes in the original sample space \[ \Omega =\{{\text {S, FS, FFS, FFFS, }\ldots \quad } \}. \]
If, for example, the sequence \(\text {FFFFS}\) occurs then we have \(X(\text {FFFFS})=4\).
</p><!-- l. 1824 --><p class='noindent'>Such a random variable is called a <span class='cmssbx-10x-x-109'>geometric random variable</span> with parameter
\(\theta \).<span class='footnote-mark'><a href='LNforHTML3.html#fn1x5'><sup class='textsuperscript'>1</sup></a></span><a id='x1-44001f1'></a> 
Examples of geometric random variables include: </p>
     <ul class='itemize1'>
     <li class='itemize'>the number of heads of a coin toss before the first tail;
     </li>
     <li class='itemize'>the number of boys born before the first girl;
     </li>
     <li class='itemize'>the number of black cars passed before a red car;
     </li>
     <li class='itemize'>the number of years to pass before your team wins the FA cup.</li></ul>
<!-- l. 1832 --><p class='noindent'>We write \( X\sim \mathrm {Geometric}(\theta )\).
</p>
<div class='newtheorem'>
<!-- l. 1836 --><p class='noindent'><span class='head'>
<a id='x1-44002r13'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.13.</span>  </span>Use the independence of the Bernoulli random variables to derive the p.m.f. of the
geometric random variable.
</p><!-- l. 1840 --><p class='noindent'><span class='cmssbx-10x-x-109'>Hint:</span> \(X=4\) corresponds to the sample point \(\text {FFFFS}\).
</p>
</div>
<!-- l. 1841 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<div class='newtheorem'>
<!-- l. 1843 --><p class='noindent'><span class='head'>
<a id='x1-44003r14'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.14.</span>  </span>The random variable \( X\sim \mathrm {Geometric}(0.3)\). Use R to evaluate and plot the p.m.f. of \( X\) for \( r=0,1,2,\dots ,5\), with the
commands
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-3'>
dgeom(0:5,prob=0.3)
dgeom(0:5,prob=0.4)
# Note how the probabilities change
barplot( dgeom(0:5,prob=0.4),names.arg=c(0:5) )</pre>
<!-- l. 1852 --><p class='nopar'> Repeat with \( \theta =0.4\) and plot.
</p>
</div>
<!-- l. 1854 --><p class='noindent'>
</p>
<div class='center'>
<!-- l. 1855 --><p class='noindent'>
</p><!-- l. 1856 --><p class='noindent'><img alt='PIC' height='170' src='geom-plot.png' width='170' /></p></div>
<div class='newtheorem'>
<!-- l. 1859 --><p class='noindent'><span class='head'>
<a id='x1-44004r15'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.15.</span>  </span>Verify that \( \sum _{r=0}^\infty \;p_X(r)=1\) for the geometric p.m.f. This requires familiarity with the sum of a
geometric series given at the start of this chapter.
</p>
</div>
<!-- l. 1862 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1865 --><p class='noindent'><span class='head'>
<a id='x1-44005r16'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.16.</span>  </span>For a general \( X\sim \mathrm {Geometric}(\theta )\), find \( \mathbb {P}(X\geq r)\).
</p>
</div>
<!-- l. 1867 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<div class='newtheorem'>
<!-- l. 1872 --><p class='noindent'><span class='head'>
<a id='x1-44006r17'></a>
<span class='cmssbx-10x-x-109'>Example 4.17.</span>  </span>Find \( \mathbb {E}[X]\) and \(\mathrm {Var}(X)\) for a geometric random variable.
</p>
</div>
<!-- l. 1874 --><p class='noindent'>
We need to use the basic identities from Section <a href='#x1-310004.1'>4.1<!-- tex4ht:ref: sec:identities  --></a> on page <a href='#x1-310004.1'>57<!-- tex4ht:ref: sec:identities  --></a>.
</p><!-- l. 1878 --><p class='noindent'>Note that if the Bernoulli probability \(\theta \) is very small then the expectation and variance are both very large –
in the limit as \(\theta \to 0\) these go to infinity.
</p><!-- l. 1881 --><p class='noindent'>To summarise: </p>
<div class='center'>
<!-- l. 1889 --><p class='noindent'>
</p>
<div class='fbox'>For a geometric random variable \( X\sim \mathrm {Geometric}(\theta )\) \begin {gather*}  p_X(r)= (1-\theta )^{r}\theta \mbox { for }r=0,1,2,,\dots \\ p_X(r)= 0\quad \mbox {otherwise}\\ \mathbb {E}[X]=\frac {1-\theta }{\theta }\\ \mathrm {Var}(X)=\frac {1-\theta }{\theta ^2}  \end {gather*}                                       </div>
</div>
<div class='newtheorem'>
<!-- l. 1893 --><p class='noindent'><span class='head'>
<a id='x1-44007r18'></a>
<span class='cmssbx-10x-x-109'>Practice question 4.18.</span>  </span>Alice and Bob play a series of chess matches. Alice’s probability of winning
a match is \(0.4\), Bob’s probability of winning a match is \(0.3\), and the probability of a draw is \(0.3\).
   </p><dl class='enumerate'><dt class='enumerate'>
a. </dt><dd class='enumerate'>
   <!-- l. 1897 --><p class='noindent'>What is the probability that Alice wins three matches out of five?
   </p></dd><dt class='enumerate'>
b. </dt><dd class='enumerate'>
   <!-- l. 1901 --><p class='noindent'>Find the p.m.f. of the number of matches won by Alice out of \(n\) matches played.
   </p></dd><dt class='enumerate'>
c. </dt><dd class='enumerate'>
   <!-- l. 1904 --><p class='noindent'>Find the p.m.f. of the number of draws up until the first match won by either Alice or Bob.
                                                                                      
                                                                                      
   </p></dd><dt class='enumerate'>
d. </dt><dd class='enumerate'>
   <!-- l. 1907 --><p class='noindent'>If we know that Alice did not win a particular match, what is the probability that it was a draw?
   </p></dd><dt class='enumerate'>
e. </dt><dd class='enumerate'>
   <!-- l. 1910 --><p class='noindent'>If Alice and Bob play until the first time Alice wins, find the p.m.f. of the number of matches
   that are played.
   </p></dd><dt class='enumerate'>
 f. </dt><dd class='enumerate'>
   <!-- l. 1913 --><p class='noindent'>Find the p.m.f. of the number of draws up until the first match won by Alice.</p></dd></dl>
</div>
<!-- l. 1915 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.6   </span> <a id='x1-450004.6'></a>Poisson random variables</h3>
<!-- l. 1921 --><p class='noindent'>Unlike the other random variables discussed here, we define the Poisson random variable directly through
its p.m.f.:
</p>
<div class='center'>
<!-- l. 1929 --><p class='noindent'>
</p>
<div class='fbox'>Let \(\lambda \) be a positive real number. The p.m.f. of a <span class='cmssbx-10x-x-109'>Poisson random variable</span> \( X\) with
parameter \(\lambda \) is \[ p_X(r)=\frac {\lambda ^r \exp (-\lambda )}{r!}, \] for \(r=0,1,2,\dots \), with \(p_X(r)=0\) otherwise. We write \( X\sim \mathrm {Pois}(\lambda )\).                            </div>
</div>
<!-- l. 1931 --><p class='noindent'>The Poisson random variable arises physically in two ways:
     </p><dl class='enumerate'><dt class='enumerate'>
   1. </dt><dd class='enumerate'>
     <!-- l. 1933 --><p class='noindent'>The number of events in a fixed time interval of a continuous time process where events occur
     at random at a given rate over time. (Covered in later courses.)
     </p></dd><dt class='enumerate'>
   2. </dt><dd class='enumerate'>
     <!-- l. 1936 --><p class='noindent'>As an approximation for the number of successes when there are many trials but the probability
     of success is very rare.</p></dd></dl>
<!-- l. 1942 --><p class='noindent'>Examples of quantities that may be modelled by Poisson random variables are: </p>
                                                                                      
                                                                                      
     <ul class='itemize1'>
     <li class='itemize'>the number of raindrops to land on your head in a given time interval;
     </li>
     <li class='itemize'>the number of floods of a river in a year;
     </li>
     <li class='itemize'>the number of deaths due to typhoid over a year (assuming typhoid cases are independent);
     </li>
     <li class='itemize'>the number of hits on a website in a given period of time.</li></ul>
<div class='newtheorem'>
<!-- l. 1950 --><p class='noindent'><span class='head'>
<a id='x1-45003r19'></a>
<span class='cmssbx-10x-x-109'>Practice question 4.19.</span>  </span>Calculate the probabilities of \( 0\), \( 1\) and \( 2\) deaths from typhoid in a year if the
number \( X\) has a Poisson random variable with \( \lambda =4.6\).
</p><!-- l. 1954 --><p class='noindent'>R hint: <span class='obeylines-h'><span class='verb'><span class='cmtt-10x-x-109'> dpois(0:2,lambda=4.6) </span></span></span>
</p>
</div>
<!-- l. 1955 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1957 --><p class='noindent'><span class='head'>
<a id='x1-45004r20'></a>
<span class='cmssbx-10x-x-109'>Exercise 4.20.</span>  </span>Verify that \( p_X(r)=\frac {\lambda ^r \exp (-\lambda )}{r!}\) is a proper probability mass function. You will need to use the definition
of the exponential function through its series expansion.
</p>
</div>
<!-- l. 1962 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1967 --><p class='noindent'><span class='head'>
<a id='x1-45005r21'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Exercise 4.21.</span>  </span>If \( X\sim \mathrm {Pois}(\lambda )\) show that \( \mathbb {E}[X]=\lambda \). The technique is similar to that in the above example.
</p>
</div>
<!-- l. 1969 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 1974 --><p class='noindent'><span class='head'>
<a id='x1-45006r22'></a>
<span class='cmssbx-10x-x-109'>Example 4.22.</span>  </span>Find the variance of a Poisson random variable by first computing \( \mathbb {E}[X(X-1)]\).
</p>
</div>
<!-- l. 1977 --><p class='noindent'>
</p><!-- l. 1979 --><p class='noindent'>Thus a key property of the Poisson p.m.f. is that the expectation and variance are both equal to
\( \lambda \).
</p>
<div class='newtheorem'>
<!-- l. 1982 --><p class='noindent'><span class='head'>
<a id='x1-45007r23'></a>
<span class='cmssbx-10x-x-109'>Example 4.23.</span>  </span>Use R to give a barplot of the Poisson p.m.f. on \( 0,1,\dots ,7\), when \( \lambda =0.5\) and when \( \lambda =3\).
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-4'>
par(mfrow=c(1,2))
barplot( dpois(0:7,lambda=0.5),names.arg=c(0:7),ylim=c(0,1) )
barplot( dpois(0:7,lambda=3),names.arg=c(0:7),ylim=c(0,1)  )</pre>
<!-- l. 1990 --><p class='nopar'>
</p>
</div>
<!-- l. 1991 --><p class='noindent'>
</p>
<div class='center'>
<!-- l. 1993 --><p class='noindent'>
</p><!-- l. 1994 --><p class='noindent'><img alt='PIC' height='227' src='pois-plot.png' width='227' /></p></div>
<!-- l. 1997 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-460004.6'></a>Poisson approximation to the binomial</h4>
<!-- l. 1998 --><p class='noindent'>One use of Poisson random variables is for rare events. Consider a binomial random variable \( X\) from \( n\) trials
with the probability of success being \( \theta \). Suppose \(n\) is very large, but \(\theta \) is very small.
</p><!-- l. 2001 --><p class='noindent'>Each event has a small probability of occurring, but when there are a large number of trials, the probability
that one or more events occur is not negligible. Define \( \lambda =n\theta \). We will make \(n\) large while keeping \(\lambda \) fixed (and so
\(\theta =\lambda /n\)).
</p><!-- l. 2004 --><p class='noindent'>We will use the following fact: \[\lim _{n\to \infty }\left (1+\frac {x}{n}\right )^n= \exp (x)\] for all \(x\in \mathbb {R}\).
</p><!-- l. 2009 --><p class='noindent'>Therefore a binomial p.m.f. with large \( n\) and correspondingly small \( \theta \) can be approximated by a Poisson
p.m.f. with parameter \( \lambda =n\theta \). For a good approximation we should have \( n\geq 100\) and \( \theta \leq .01\).
</p>
<div class='newtheorem'>
<!-- l. 2013 --><p class='noindent'><span class='head'>
<a id='x1-46001r24'></a>
<span class='cmssbx-10x-x-109'>Example 4.24.</span>  </span>The number of cases of a rare disease is on average \( 3.67\) per month. In the month after
a festival there were \( 14\) cases of the disease reported.
</p><!-- l. 2018 --><p class='noindent'>How unusual is this? Compute the \(p\)-value = \(\mathbb {P}(\text {observed value or worse})\), under the assumption of natural Poisson variability.
</p><!-- l. 2022 --><p class='noindent'>It is \( \mathbb {P}(X\geq 14)\) and measures the worry in observing \( 14\) cases.
                                                                                      
                                                                                      
</p><!-- l. 2024 --><p class='noindent'>R hint: <span class='obeylines-h'><span class='verb'><span class='cmtt-10x-x-109'> 1-sum( dpois(0:13,lambda=3.67) ) </span></span></span>
</p>
</div>
<!-- l. 2025 --><p class='noindent'>
</p><!-- l. 2030 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.7   </span> <a id='x1-470004.7'></a>Negative binomial random variables (not examinable)</h3>
<!-- l. 2031 --><p class='noindent'>Consider an experiment for which the random variable of interest, \( X\), corresponds to the number of failures
to occur before the \( k\)th success in a series of independent Bernoulli trials, each with probability of success
being \( \theta \).
</p><!-- l. 2036 --><p class='noindent'>Here the sample space when \( k=2\) is \( \{\text {SS, FSS, SFS, FFSS, FSFS, SFFS, }\ldots \}\), and the induced sample space is \( \{0,1,2, \ldots \}\).
</p><!-- l. 2040 --><p class='noindent'>What is the probability that \(X=r\)? This requires \(r\) failures and \(k\) successes in \(r+k\) trials, with the last
trial being a success (or we would have stopped earlier). In other words, we need \(k-1\) successes
out of the first \(r+k-1\) trials, followed by a success. This has probability\[\binom {k+r-1}{k-1}(1-\theta )^{r}\theta ^{k -1} \times \theta = \binom {k+r-1}{k-1}(1-\theta )^{r}\theta ^{k} .\] It is possible to show that
\begin {gather*}  \mathbb {E}[X]=\frac {k(1-\theta )}{\theta }\\ \mathrm {Var}(X)=\frac {k(1-\theta )}{\theta ^2}.  \end {gather*}
Note that, setting \( k=1\), these results match what we already know about the geometric p.m.f.
</p><!-- l. 2049 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>4.8   </span> <a id='x1-480004.8'></a>Summary</h3>
<!-- l. 2050 --><p class='noindent'>There are many discrete probability models based on Bernoulli trials. In these cases </p>
     <ul class='itemize1'>
     <li class='itemize'>the sample space \(\Omega \) is a set of sequences of S and F (successes and failures);
     </li>
     <li class='itemize'>the induced sample space \( \mathcal {S}\) is usually \(\{0,1,2,\ldots \}\) or \(\{0,1,\ldots ,n\}\) for some parameter \(n\);
     </li>
     <li class='itemize'>trials are independent, with the same success probability on each trial \(\mathbb {P}(\text {S})=\theta \).</li></ul>
<!-- l. 2056 --><p class='noindent'>A random variable \( X\) is a function from \( \Omega \) to \( \mathcal {S}\), with p.m.f. \( p_X(r)=\mathbb {P}(\{X=r\})\).
</p><!-- l. 2059 --><p class='noindent'>Natural constructions for \(X\) give the p.m.f. for Bernoulli, binomial, geometric random variables etc. The
following table summarises the distributions we have looked at.
</p>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 2062 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-5'><colgroup id='TBL-5-1g'><col id='TBL-5-1' /></colgroup><colgroup id='TBL-5-2g'><col id='TBL-5-2' /><col id='TBL-5-3' /><col id='TBL-5-4' /><col id='TBL-5-5' /></colgroup><tr id='TBL-5-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-1-1' style='white-space:nowrap; text-align:right;'>              </td><td class='td11' id='TBL-5-1-2' style='white-space:nowrap; text-align:left;'>\( \mathcal {S}\)</td><td class='td11' id='TBL-5-1-3' style='white-space:nowrap; text-align:left;'>construction                  </td><td class='td11' id='TBL-5-1-4' style='white-space:nowrap; text-align:left;'>\( p_X(0)\)</td></tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-5-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-2-1' style='white-space:nowrap; text-align:right;'> Bernoulli</td><td class='td11' id='TBL-5-2-2' style='white-space:nowrap; text-align:left;'>\( \{0,1\}\)</td><td class='td11' id='TBL-5-2-3' style='white-space:nowrap; text-align:left;'>single trial </td><td class='td11' id='TBL-5-2-4' style='white-space:nowrap; text-align:left;'>\( 1-\theta \)</td>
</tr><tr id='TBL-5-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-3-1' style='white-space:nowrap; text-align:right;'>       Binomial</td><td class='td11' id='TBL-5-3-2' style='white-space:nowrap; text-align:left;'>\( \{0,1,\ldots ,n\}\)</td><td class='td11' id='TBL-5-3-3' style='white-space:nowrap; text-align:left;'>successes in \( n\) trials          </td><td class='td11' id='TBL-5-3-4' style='white-space:nowrap; text-align:left;'>\( (1-\theta )^n\)</td>
</tr><tr id='TBL-5-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-4-1' style='white-space:nowrap; text-align:right;'>      Geometric</td><td class='td11' id='TBL-5-4-2' style='white-space:nowrap; text-align:left;'>\( \{0,1,\ldots \}\)</td><td class='td11' id='TBL-5-4-3' style='white-space:nowrap; text-align:left;'>failures before first success</td><td class='td11' id='TBL-5-4-4' style='white-space:nowrap; text-align:left;'>\( \theta \)</td>
</tr><tr id='TBL-5-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-5-1' style='white-space:nowrap; text-align:right;'>        Poisson</td><td class='td11' id='TBL-5-5-2' style='white-space:nowrap; text-align:left;'>\( \{0,1,\ldots \}\)</td><td class='td11' id='TBL-5-5-3' style='white-space:nowrap; text-align:left;'>Limit of \(\mathrm {Bin}(n,\lambda /n)\) as \(n\to \infty \)                  </td><td class='td11' id='TBL-5-5-4' style='white-space:nowrap; text-align:left;'>\( \exp (-\lambda )\)</td>
</tr><tr id='TBL-5-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-6-1' style='white-space:nowrap; text-align:right;'><span class='cmssi-10x-x-109'>Negative binomial</span></td><td class='td11' id='TBL-5-6-2' style='white-space:nowrap; text-align:left;'>\( \{0,1,\ldots \}\)</td><td class='td11' id='TBL-5-6-3' style='white-space:nowrap; text-align:left;'><span class='cmssi-10x-x-109'>failures before</span> \(k\)<span class='cmssi-10x-x-109'>th success  </span></td><td class='td11' id='TBL-5-6-4' style='white-space:nowrap; text-align:left;'>\( \theta ^{k}\)</td>
</tr></table></div></div>
                                                                                      
                                                                                      
<h2 class='chapterHead'><span class='titlemark'>Chapter 5</span><br /><a id='x1-490005'></a>Continuous random variables</h2>
<h3 class='sectionHead'><span class='titlemark'>5.1   </span> <a id='x1-500005.1'></a>Introduction to continuous variables</h3>
<!-- l. 2075 --><p class='noindent'>Discrete random variables described the outcomes of experiments which were in a countable set (usually
non-negative integer values). This covered models for the number of successes in a fixed number of trials,
the number of floods of a river in a year, the number of children in a family until a girl is
born.
</p><!-- l. 2082 --><p class='noindent'>Focusing only on discrete random variables is too restrictive for many situations. Examples include the
nicotine levels in the blood plasma of smokers, the time intervals between floods of a river, and the waiting
time for admissions to an intensive care unit. In each case the outcome of the experiment is a measurement
on a continuous scale. This suggests we need to consider <span class='cmssbx-10x-x-109'>continuous random variables</span>, that is variables
whose set of possible values is uncountable (usually the real numbers, or real numbers in some fixed
range).
</p><!-- l. 2091 --><p class='noindent'>To describe continuous random variables we need slightly different mathematical tools than we
used for discrete random variables. For example, the discrete probability mass function \(p_R(r)=\mathbb {P}(R=r)\) does
not work for a continuous random variable \(X\), since \(\mathbb {P}(X=x)=0\) for any specific value of \(x\). We therefore
focus on probabilities of events instead of probabilities of single outcomes. In particular we
focus on events of the form \[ \{X\le x\} \] for fixed \(x\), and consider these as \(x\) varies. For discrete random
variables we defined (on page <a href='#x1-26006r3.3'>49<!-- tex4ht:ref: DiscreteCDF  --></a>) the cumulative distribution function \[ F_R(r) = \mathbb {P}(R\leq r) \] for all real numbers
\(r\).
</p><!-- l. 2106 --><p class='noindent'>Recall that as \(r\) varies the function \(\mathbb {P}(R\le x)\) jumps at the values that \(R\) could take, and the size of the jump is equal
to the corresponding value of the p.m.f.
</p><!-- l. 2109 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>5.2   </span> <a id='x1-510005.2'></a>The cumulative distribution function</h3>
<!-- l. 2113 --><p class='noindent'>The <span class='cmssbx-10x-x-109'>cumulative distribution function</span> (c.d.f.) of a (continuous or discrete) random variable, \(X\), is defined,
for all real values of \(x\), by \[ F_{X}(x)=\mathbb {P}(X \leq x), \]the probability that the random variable \(X\) takes a value less than or equal to \(x\).
When \(F_X\) is continuous, we have a <span class='cmssbx-10x-x-109'>continuous random variable</span>. Below is an example of \(F_X\) for a continuous
random variable. </p>
<div class='center'>
<!-- l. 2120 --><p class='noindent'>
</p><!-- l. 2121 --><p class='noindent'><img alt='PIC' height='28' src='gamma25_cdf.png' width='28' /></p></div>
<!-- l. 2124 --><p class='noindent'><span class='paragraphHead'><a id='x1-520005.2'></a><span class='cmssbx-10x-x-109'>Properties of</span> \(F_X(x)\)<span class='cmssbx-10x-x-109'>:</span></span>
                                                                                      
                                                                                      
     </p><dl class='enumerate'><dt class='enumerate'>
   1. </dt><dd class='enumerate'>
     <!-- l. 2126 --><p class='noindent'>\(0\leq F_X(x)\leq 1\), with \(\lim _{x\rightarrow -\infty }F_X(x)={ 0}\) and \(\lim _{x\rightarrow \infty }F_X(x)={ 1}\),
     </p></dd><dt class='enumerate'>
   2. </dt><dd class='enumerate'>
     <!-- l. 2129 --><p class='noindent'>\(F_X(x)\) is non-decreasing function of \(x\).  Why?</p></dd></dl>
<!-- l. 2132 --><p class='noindent'>The distribution function is particularly useful for continuous random variables as we often want to know
the probability of events that can be related by the laws of probability into probability statements about the
event \(\{X\le x\}\) for some \(x\).
</p><!-- l. 2138 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-530005.2'></a>Probabilities of intervals</h4>
<!-- l. 2139 --><p class='noindent'>Often the probability of the random variable \(X\) falling in the interval \((a,b]\) is of interest for some real numbers \(a, b\)
with \(a&lt;b\). This corresponds to the event \(\{a&lt; X\le b\}\). By using the law of total probability \(\mathbb {P}(X\le b)=\mathbb {P}(X\le a)+\mathbb {P}(a&lt;X\le b)\) so the probability of the
interval event is
</p><!-- l. 2148 --><p class='noindent'>As \(\mathbb {P}(X=x)=0\) for all \(x\), for any continuous random variable \(X\) \[ \mathbb {P}(X\le x)=\mathbb {P}(X&lt; x). \]
</p>
<div class='newtheorem'>
<!-- l. 2155 --><p class='noindent'><span class='head'>
<a id='x1-53001r1'></a>
<span class='cmssbx-10x-x-109'>Exercise 5.1.</span>  </span>Let \(X\) be a random variable with cumulative distribution function \[ F_X(x)= \begin {cases} 0&amp;\mbox { if }x\le 0,\\ x&amp;\mbox { if }0&lt;x\le 1\\ 1&amp;\mbox { if }x&gt;1. \end {cases} \]
</p><!-- l. 2167 --><p class='noindent'>Obtain the following probabilities:
     </p><dl class='enumerate'><dt class='enumerate'>
   a. </dt><dd class='enumerate'>
     <!-- l. 2169 --><p class='noindent'>\(\mathbb {P}(X\leq 0.5)= {F_X(0.5) = 0.5}\),<br class='newline' />
     </p></dd><dt class='enumerate'>
   b. </dt><dd class='enumerate'>
     <!-- l. 2170 --><p class='noindent'>\(\mathbb {P}(X&gt; 0.5)={1-\mathbb {P}(X\leq 0.5)=1-F_X(0.5)=0.5}\),<br class='newline' />
     </p></dd><dt class='enumerate'>
   c. </dt><dd class='enumerate'>
     <!-- l. 2171 --><p class='noindent'>\(\mathbb {P}(X = 0.5)={0}\),<br class='newline' />
                                                                                      
                                                                                      
     </p></dd><dt class='enumerate'>
   d. </dt><dd class='enumerate'>
     <!-- l. 2172 --><p class='noindent'>\(\mathbb {P}(X &lt; .9)={0.9}\),<br class='newline' />
     </p></dd><dt class='enumerate'>
   e. </dt><dd class='enumerate'>
     <!-- l. 2173 --><p class='noindent'>\(\mathbb {P}(0.5&lt;X\le 0.9)={\mathbb {P}( X\leq .9)-\mathbb {P}( X\leq 0.5)=0.9-0.5=0.4}\).</p></dd></dl>
</div>
<!-- l. 2175 --><p class='noindent'>
</p><!-- l. 2182 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>5.3   </span> <a id='x1-540005.3'></a>The probability density function</h3>
<!-- l. 2184 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-550005.3'></a>Recap of definite integrals</h4>
<!-- l. 2185 --><p class='noindent'>We will frequently have to evaluate \[ F(x) := \int _{-\infty }^x f(s) \,\mathrm {d} s. \] For any particular value of \(x\) (e.g. \(x=2\)) this is the <span class='cmssbx-10x-x-109'>definite integral</span> that
you know. It is <span class='cmssbx-10x-x-109'>NOT </span>the same as the <span class='cmssbx-10x-x-109'>indefinite integral</span>, i.e. \[ F(x) \ne \int f(x) \,\mathrm {d} x. \] For example, suppose \[ f(x) = \begin {cases} 0&amp;\mbox { when }x&lt;1\\ 1/x^2&amp;\mbox { otherwise.} \end {cases} \] Then for \(x&lt;1\) we have \(F(x)=0\),
and when \(x\ge 1\), \[ \int _{-\infty }^x f(s)\,\mathrm {d} s=\int _1^x f(s)\,\mathrm {d} s = \left [-\frac {1}{s}\right ]^x_1 = 1-1/x. \] Whereas, for \(x\ge 1\) \[ \int f(x)\,\mathrm {d} x = -1/x +c, \] which gives a whole family of functions (including the one we want) depending
on \(c\).
</p><!-- l. 2212 --><p class='noindent'><span class='cmssbx-10x-x-109'>One of the most frequent single mistakes by students encountering continuous random variables
for the first time is evaluating an indefinite integral with</span> \(c=0\) <span class='cmssbx-10x-x-109'>when they should have been evaluating
a definite integral.</span>
</p><!-- l. 2215 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-560005.3'></a>The probability density function</h4>
<!-- l. 2217 --><p class='noindent'>Recall (page <a href='#x1-26006r3.3'>49<!-- tex4ht:ref: DiscreteCDF  --></a>) that the c.d.f., \(F_R(r)\), of a discrete random variable, \(R\), is the sum of its p.m.f. up to the value \(r\),
i.e. \(\sum _{i=-\infty }^r p_R(i)\).
</p><!-- l. 2220 --><p class='noindent'>Analogously the c.d.f. \(F_X(x)\) of a continuous random variable should correspond to an <span class='cmssbx-10x-x-109'>integral</span>. We therefore
define the <span class='cmssbx-10x-x-109'>probability density function</span>, or p.d.f., \(f_X(x)\), of a continuous random variable \(X\), to be \[ f_X(x)=\frac {\mathrm {d}}{\mathrm {d}x}F_X(x), \] so that it
satisfies  \begin {equation*}  F_X(x)=\int _{-\infty }^{x}f_X(s)\,\mathrm {d}s.  \end {equation*}
For example \[ P(X\le 10)=F_X(10)={ \int _{-\infty }^{10}f_X(s)\,\mathrm {d}s.} \] Note that \(s\) is a <span class='cmssbx-10x-x-109'>dummy variable</span>; one could use any letter for the integrand except \(x\), \(d\) or \(f\). The
following is the p.d.f. corresponding to the c.d.f. we saw earlier.
</p>
                                                                                      
                                                                                      
<div class='center'>
<!-- l. 2236 --><p class='noindent'>
</p><!-- l. 2237 --><p class='noindent'><img alt='PIC' height='28' src='gamma25_pdf.png' width='28' /></p></div>
<!-- l. 2240 --><p class='noindent'>Notice that the p.d.f. is zero in regions where there are no outcomes, in this example for \(x\leq 0\). Note also that it
exceeds \(1\) in some places, so it cannot be interpreted as a probability despite having some properties
(which we shall explore in what follows) that are very similar to those of the probability mass
function.
</p><!-- l. 2247 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-570005.3'></a>Properties of \(f_X(x)\)</h4>
<!-- l. 2249 --><p class='noindent'>
     </p><dl class='enumerate'><dt class='enumerate'>
   1. </dt><dd class='enumerate'>
     <!-- l. 2249 --><p class='noindent'>Positivity: \(f_{X}(x)\ge 0\) for all \(x\),  Why?
     </p></dd><dt class='enumerate'>
   2. </dt><dd class='enumerate'>
     <!-- l. 2250 --><p class='noindent'>Unit-integrability: \(\int _{-\infty }^{\infty } f_X(x)\,\mathrm {d}x=1\).  Why?</p></dd></dl>
<!-- l. 2254 --><p class='noindent'>What is the probability that an observation on a continuous random variable \(X\) lies in the interval
\((a,b]\)?
</p><!-- l. 2261 --><p class='noindent'>We see that \(\mathbb {P}(a&lt; X\leq b)\) may be calculated as the area under the curve of \(f_X(x)\) between \(x=a\) and \(x=b\).
</p><!-- l. 2264 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-580005.3'></a>An illuminating idea</h4>
<!-- l. 2265 --><p class='noindent'>For some <span class='cmssbx-10x-x-109'>very small </span>interval width \(\delta \),
</p>
<div class='newtheorem'>
<!-- l. 2269 --><p class='noindent'><span class='head'>
<a id='x1-58001r2'></a>
<span class='cmssbx-10x-x-109'>Example 5.2.</span>  </span>A random variable \(X\) has cumulative distribution function \[ F_X(x)= \begin {cases} 1-\exp (-3x) &amp; \mbox { for }x\ge 0,\\ 0 &amp; \mbox { for }x&lt;0. \end {cases} \] Find the p.d.f. of \(X\).
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 2278 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2281 --><p class='noindent'><span class='head'>
<a id='x1-58002r3'></a>
<span class='cmssbx-10x-x-109'>Exercise 5.3.</span>  </span>A triangular p.d.f.: a random variable \(X\) has p.d.f. \[ f_X(x)= \begin {cases} 2(1-x) &amp; \mbox {if }0\leq x\leq 1,\\ 0 &amp; \mbox {otherwise}. \end {cases} \] Obtain the c.d.f. \(F_X(x)\).
</p>
</div>
<!-- l. 2291 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2293 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Solution.</span>  </span>We <span class='cmssbx-10x-x-109'>split the range </span>of \(x\), \((-\infty ,\infty )\) into sensible intervals. </p>
     <ul class='itemize1'>
     <li class='itemize'>For \(x\in (-\infty ,0]\), \[F_X(x)=\int _{-\infty }^x f_X(s)\,\mathrm {d}s={\int _{-\infty }^x 0 \,\mathrm {d} s=0.}\]
     </li>
     <li class='itemize'>For \(x\in (0,1]\),
     </li>
     <li class='itemize'>For \(x\in {(1,\infty )}\), \[ F_X(x)=\int _{-\infty }^x f_X(s)\,\mathrm {d}s={ F_X(1)+\int _1^x 0\,\mathrm {d}s=1.} \]</li></ul>
<!-- l. 2309 --><p class='noindent'>Now find \(\mathbb {P}(X&lt; 0.5)\), \(\mathbb {P}(0.5&lt;X&lt;0.8)\) and \(\mathbb {P}(0.5&lt;X&lt;1.75)\).
</p><!-- l. 2312 --><p class='noindent'>Obtain \(\mathbb {P}(0.5&lt;X&lt;0.8)\) directly from the p.d.f.
</p>
</div>
<!-- l. 2313 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2316 --><p class='noindent'><span class='head'>
                                                                                      
                                                                                      
<a id='x1-58003r4'></a>
<span class='cmssbx-10x-x-109'>Example 5.4.</span>  </span>The lifetime, in years, that a computer functions before breaking down is a continuous
random variable \(X\) with p.d.f. given by \[ f_X(x)=\begin {cases} \lambda \exp (-\lambda x) &amp;\mbox { for } x\geq 0,\\ 0&amp;\mbox { for } x&lt;0. \end {cases} \] The parameter \(\lambda \) depends on the type of computer. To set a
time for a guarantee the company wants to know the time \(t\) for which with probability \(0.9\) the lifetime
of the computer will exceed \(t\).
</p>
</div>
<!-- l. 2330 --><p class='noindent'>
</p><!-- l. 2333 --><p class='noindent'>The quantity that we have just found is called a quantile of the distribution; see Section <a href='#x1-600005.5'>5.5<!-- tex4ht:ref: sec:quantiles  --></a>.
</p><!-- l. 2336 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>5.4   </span> <a id='x1-590005.4'></a>Expectation and variance</h3>
<!-- l. 2337 --><p class='noindent'>All the information about the distribution of a continuous random variable \(X\) is contained in either the c.d.f. \(F_X(x)\)
or the p.d.f. \(f_X(x)\). However it is often helpful to summarise the main characteristics of the distribution in terms
of a few values, analogously to the discrete case.
</p><!-- l. 2342 --><p class='noindent'>The expected value of a continuous random variable \(X\) can be thought of as the average of the different
values that \(X\) may take, weighted according to their chance of occurrence.
</p>
<div class='center'>
<!-- l. 2349 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>expected value</span> of a continuous random variable \(X\) is  \[ \mathbb {E}[X]=\int _{-\infty }^{\infty }xf_X(x)\,\mathrm d x. \]                  </div>
</div>
<!-- l. 2350 --><p class='noindent'>This is very similar to the definition of expectation of a discrete random variable.
</p><!-- l. 2353 --><p class='noindent'>Similarly one can show that the expected value of a real-valued function \(g(X)\) of a continuous random variable \(X\)
is \[ \mathbb {E}[g(X)]=\int _{-\infty }^{\infty }g(x)f_X(x)\,\mathrm {d} x. \] For example, \[ \mathbb {E}[X^2]=\int _{-\infty }^{\infty }x^2f_X(x)\,\mathrm {d} x. \]
</p>
<div class='newtheorem'>
<!-- l. 2365 --><p class='noindent'><span class='head'>
<a id='x1-59001r5'></a>
<span class='cmssbx-10x-x-109'>Exercise 5.5.</span>  </span>For a random variable \(X\) with p.d.f. given by \[f_X(x)= \begin {cases} 2x&amp;\mbox {for }0\leq x \leq 1\\ 0&amp;\mbox {otherwise}, \end {cases} \] find \(\mathbb {E}[X]\), \(\mathbb {E}[2X]\) and \(\mathbb {E}[X^2]\).
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 2374 --><p class='noindent'>
</p><!-- l. 2377 --><p class='noindent'>For continuous variables expectation has the same linearity properties that we found in Chapter <a href='#x1-220003'>3<!-- tex4ht:ref: chap:DiscreteRVs  --></a> for
discrete random variables.
</p><!-- l. 2380 --><p class='noindent'>For arbitrary functions \(g\) and \(h\) and constant \(c\), \begin {align*}  \mathbb {E}[g(X)+h(X)] &amp; = \mathbb {E}[g(X)] + \mathbb {E}[h(X)]; \\ \mathbb {E}[cg(X)] &amp; = c\mathbb {E}[g(X)];\\ \mathbb {E}[g(X)+c] &amp; = \mathbb {E}[g(X)] +c.  \end {align*}
</p><!-- l. 2387 --><p class='noindent'>Derivation (of first property):
</p><!-- l. 2389 --><p class='noindent'>The other properties may be shown similarly.
</p><!-- l. 2392 --><p class='noindent'>We can now define the variance in exactly the same way as for discrete random variables. Again, the
standard deviation is the square root of this. </p>
<div class='center'>
<!-- l. 2398 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>variance</span>, measuring of the spread or dispersion of a random variable about
the expectation, for a continuous distribution is  \[ \mathrm {Var}(X)=\mathbb {E}\left [(X-\mathbb {E}[X])^2\right ]\]                          </div>
</div>
<!-- l. 2399 --><p class='noindent'>As with discrete random variables, the easiest way to evaluate the variance is usually
</p>
<div class='newtheorem'>
<!-- l. 2402 --><p class='noindent'><span class='head'>
<a id='x1-59002r6'></a>
<span class='cmssbx-10x-x-109'>Exercise 5.6.</span>  </span>For a random variable \(X\) with p.d.f. given by \[f_X(x)= \begin {cases} 2x&amp;\mbox {for }0\leq x \leq 1,\\ 0&amp;\mbox {otherwise}, \end {cases} \] find \(\mathrm {Var}(X)\). You might well want to use some
results from Exercise <a href='#x1-59001r5'>5.5<!-- tex4ht:ref: exam:low_moments  --></a>.
</p>
</div>
<!-- l. 2411 --><p class='noindent'>
</p><!-- l. 2413 --><p class='noindent'><span class='cmssbx-10x-x-109'>Warning: </span>Sometimes the expectation or variance do not exist. This occurs when the chance of obtaining
very large values is too big.
</p><!-- l. 2416 --><p class='noindent'>
</p>
                                                                                      
                                                                                      
<h3 class='sectionHead'><span class='titlemark'>5.5   </span> <a id='x1-600005.5'></a>Quantiles</h3>
<!-- l. 2417 --><p class='noindent'>The c.d.f. tells us the probability that a continuous random variable does not exceed a specified value.
Often we are interested in the reverse question: we specify a probability and want to know the value which
is not exceeded with that probability.
</p><!-- l. 2419 --><p class='noindent'>Such values are termed <span class='cmssbx-10x-x-109'>quantiles</span> with \(x_p\) the \(100p\%\) quantile defined by \[ F_X(x_p)=p. \]
</p><!-- l. 2425 --><p class='noindent'>They can be read off from the graph of the c.d.f.: the graph below shows the \(40\%\) quantile. (This is sometimes
referred to as the “40th centile”.) </p>
<div class='center'>
<!-- l. 2426 --><p class='noindent'>
</p><!-- l. 2427 --><p class='noindent'><img alt='PIC' height='28' src='quantile_diag.png' width='28' /></p></div>
<!-- l. 2430 --><p class='noindent'>Certain quantiles are of special interest:
     </p><dl class='description'><dt class='description'>
     <!-- l. 2432 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Median:</span> </p></dt><dd class='description'>
     <!-- l. 2432 --><p class='noindent'>the median is the middle of the distribution in the sense that the value of the random variable
     is equally likely to fall on either side of this value. The median is the \(50\%\) quantile, \(x_{0.5}\), so that \(F(x_{0.5})=0.5\). As
     a measure of location, the median has the advantage over the expectation of existing for all
     distributions. It also is less sensitive to small changes in the distribution of very large values.
     For this reason, the “average” income given in news stories almost always uses the median.
     </p></dd><dt class='description'>
     <!-- l. 2437 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Quartiles:</span> </p></dt><dd class='description'>
     <!-- l. 2437 --><p class='noindent'>the quartiles split the distribution into four equally likely regions, \(x_{0.25}\) the lower quartile, \(x_{0.5}\) the
     median and \(x_{0.75}\) the upper quartile. \[ \mathbb {P}(X&lt;x_{0.25})=\mathbb {P}(x_{0.25}&lt;X&lt;x_{0.5})= \mathbb {P}(x_{0.5}&lt;X&lt;x_{0.75})= \mathbb {P}(X&gt;x_{0.75})=0.25. \] This is illustrated below for the c.d.f. and p.d.f. of the same
     random variable.
     </p></dd><dt class='description'>
     <!-- l. 2445 --><p class='noindent'>
<span class='cmssbx-10x-x-109'>Inter-quartile range:</span> </p></dt><dd class='description'>
     <!-- l. 2445 --><p class='noindent'>the difference in values of quartiles provides a measure of the variability of a random variable
     (measured in the units of the variable) that does not require the evaluation of the standard
     deviation (which can be infinite). The inter-quartile range is \[ x_{0.75}-x_{0.25}. \]</p></dd></dl>
<div class='center'>
<!-- l. 2453 --><p class='noindent'>
</p><!-- l. 2455 --><p class='noindent'><img alt='PIC' height='25' src='meanquartile_diag.png' width='25' /> <img alt='PIC' height='25' src='meanquartile_diag2.png' width='25' /></p></div>
                                                                                      
                                                                                      
<div class='newtheorem'>
<!-- l. 2460 --><p class='noindent'><span class='head'>
<a id='x1-60001r7'></a>
<span class='cmssbx-10x-x-109'>Example 5.7.</span>  </span>A possible model for the claim sizes received by an insurance company is a random variable
with c.d.f. \begin {align*}  F_{X}(x)=1-\exp (-\lambda x),  \end {align*}
</p><!-- l. 2466 --><p class='noindent'>for some \(\lambda &gt;0\). The company is legally obliged to pay the smallest \(99\%\) of claims without requiring
re-insurance support. What claim size must the company be able to pay without re-insurance
support?
</p>
</div>
<!-- l. 2469 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2471 --><p class='noindent'><span class='head'>
<a id='x1-60002r8'></a>
<span class='cmssbx-10x-x-109'>Exercise 5.8.</span>  </span>It is considered suitable to model the annual maximum sea level by an extreme value
distribution \[ F_X(x)=\exp [-\exp \{-(x-\alpha )/\beta \}], \] for \(\beta &gt;0\). The sea flood defence needs to be built to withstand a flood of the size which
occurs in any year with probability \(0.01\) (i.e. once on average every 100 years). Evaluate the required
height of the flood defence.
</p>
</div>
<!-- l. 2482 --><p class='noindent'>
</p><!-- l. 2485 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>5.6   </span> <a id='x1-610005.6'></a>Transformations of random variables</h3>
<!-- l. 2487 --><p class='noindent'>Suppose we have a continuous random variable \(X\), but actually want to know about a random variable \(Y=f(X)\) for
some function \(f\) that is increasing (on the induced sample space).
</p><!-- l. 2489 --><p class='noindent'>For a discrete random variable, it is straightforward to write down the p.m.f. of \(Y\) based on the p.m.f. of \(X\).
For continuous random variables, we can’t go directly from the p.d.f. of \(X\) to the p.d.f. of \(Y\), but need to
make use of the c.d.f.
</p>
<div class='newtheorem'>
                                                                                      
                                                                                      
<!-- l. 2491 --><p class='noindent'><span class='head'>
<a id='x1-61001r9'></a>
<span class='cmssbx-10x-x-109'>Example 5.9.</span>  </span>Suppose \(X\) is a random variable with p.d.f. \(f_X(x)=1\) for \(0\leq x\leq 1\) (and \(0\) otherwise). What is the p.d.f. of
\(Y=\sqrt X\)?
</p>
</div>
<!-- l. 2493 --><p class='noindent'>
</p><!-- l. 2496 --><p class='noindent'>We can follow a similar process for more general increasing functions \(f\). If \(f\) is decreasing, we have to consider
the complementary event in the middle step.
</p>
<div class='newtheorem'>
<!-- l. 2497 --><p class='noindent'><span class='head'>
<a id='x1-61002r10'></a>
<span class='cmssbx-10x-x-109'>Example 5.10.</span>  </span>What is the p.d.f. of \(Z=1/X\)?
</p>
</div>
<!-- l. 2499 --><p class='noindent'>
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 6</span><br /><a id='x1-620006'></a>Models for continuous random variables</h2>
<!-- l. 2504 --><p class='noindent'>As with discrete random variables, there are a number of “standard” continuous random variables. In this
chapter we give the details of some of the most important for subsequent study of probability and
statistics.
</p>
<h3 class='sectionHead'><span class='titlemark'>6.1   </span> <a id='x1-630006.1'></a>The uniform distribution</h3>
<!-- l. 2508 --><p class='noindent'>A continuous random variable with a fixed range, for which all outcomes in that range have equal chance of
occurring is said to be uniformly distributed. Specifically, a random variable \(X\) has a <span class='cmssbx-10x-x-109'>uniform distribution</span>
over the interval \([a,b]\) if the p.d.f. is given by \[ f_X(x)= \begin {cases} \frac {1}{b-a}&amp;\mbox { for } a&lt;x&lt;b, \\ 0&amp;\mbox { otherwise.} \end {cases} \] We write \(X\sim \mathrm {U}(a,b)\). This p.d.f. for two different sets of parameter values is
illustrated below.
</p>
<div class='center'>
<!-- l. 2522 --><p class='noindent'>
</p><!-- l. 2524 --><p class='noindent'><img alt='PIC' height='28' src='unifA.png' width='28' /> <img alt='PIC' height='28' src='unifB.png' width='28' /></p></div>
<!-- l. 2529 --><p class='noindent'>We find that for all \(x\) and \(x+c\) such that \(a\leq x&lt;x+c\leq b\) \[ \mathbb {P}(x&lt;X\leq x+c)={c/(b-a),} \] so the probability of \(X\) falling in any interval of length \(c\) in the range \((a,b)\)
is the same for all \(x\), i.e. independent of the position \(x\) and proportional to the interval length
\(c\).
</p><!-- l. 2538 --><p class='noindent'>Possible examples of uniform random variables are: completely random numbers between \(0\) and \(1\), the times
of births in a 24 hour period, and the times of goals in a football match.
</p>
<div class='newtheorem'>
<!-- l. 2542 --><p class='noindent'><span class='head'>
<a id='x1-63001r1'></a>
<span class='cmssbx-10x-x-109'>Exercise 6.1.</span>  </span>Find the c.d.f., expected value and variance of the \(\mathrm {U}(a,b)\) distribution.
</p>
</div>
<!-- l. 2544 --><p class='noindent'>
</p><!-- l. 2548 --><p class='noindent'>These results seem logical as if all values in the interval \((a,b)\) are equally likely then the expected value should
be in the middle. Similarly the wider the interval, the more variable the outcomes, hence the larger the
variance should be.
</p>
<div class='newtheorem'>
                                                                                      
                                                                                      
<!-- l. 2552 --><p class='noindent'><span class='head'>
<a id='x1-63002r2'></a>
<span class='cmssbx-10x-x-109'>Exercise 6.2.</span>  </span>Find the upper quartile of the random variable \(X\sim \mathrm {U}(3,5)\).
</p>
</div>
<!-- l. 2554 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2557 --><p class='noindent'><span class='head'>
<a id='x1-63003r3'></a>
<span class='cmssbx-10x-x-109'>Exercise 6.3.</span>  </span>If \(X\sim \mathrm {U}(0,10)\), use R to calculate the probability that (a) \(X&lt;3\), (b) \(X&gt;6\), (c) \(3&lt;X&lt;8\), (d) \(8&lt;X&lt;10\), (e) \(8&lt;X&lt;13\).
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-5'>
punif(3,min=0,max=10)
1-punif(6,0,10)
punif(8,0,10)-punif(3,0,10)
punif(10,0,10)-punif(8,0,10)
punif(13,0,10)-punif(8,0,10)</pre>
<!-- l. 2566 --><p class='nopar'>
</p>
</div>
<!-- l. 2567 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2570 --><p class='noindent'><span class='head'>
<a id='x1-63004r4'></a>
<span class='cmssbx-10x-x-109'>Example 6.4.</span>  </span>Suppose that you know the score in a football game was 1–0. You watch a recording.
Assuming a uniform distribution for the time of goals (and no extra time, so the match lasts \(90\)
minutes), what is the expected time until the goal? What is the probability it is in the first half?
</p>
</div>
<!-- l. 2574 --><p class='noindent'>
</p><!-- l. 2578 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>6.2   </span> <a id='x1-640006.2'></a>The exponential distribution</h3>
<!-- l. 2579 --><p class='noindent'>A random variable \(X\) has an <span class='cmssbx-10x-x-109'>exponential distribution</span> with rate \(\beta \) if its p.d.f. is given by \[ f_X(x)= \begin {cases} C \exp (-\beta x)&amp; x \geq 0,\\ 0&amp;\mbox {otherwise,} \end {cases} \] where \(\beta &gt;0\) and \(C\) is a
<span class='cmssbx-10x-x-109'>normalising constant</span>. We write \(X\sim \mathrm {Exp}(\beta )\).
</p>
<div class='newtheorem'>
<!-- l. 2591 --><p class='noindent'><span class='head'>
<a id='x1-64001r5'></a>
<span class='cmssbx-10x-x-109'>Exercise 6.5.</span>  </span>For which value of \(C\) is this a valid p.d.f. (i.e. is non-negative and integrates to \(1\))?
                                                                                      
                                                                                      
</p>
</div>
<!-- l. 2593 --><p class='noindent'>
</p><!-- l. 2596 --><p class='noindent'>The p.d.f.s for two different values of \(\beta \) are shown below. </p>
<div class='center'>
<!-- l. 2598 --><p class='noindent'>
</p><!-- l. 2599 --><p class='noindent'><img alt='PIC' height='25' src='expA.png' width='25' /> <img alt='PIC' height='25' src='expB.png' width='25' /></p></div>
<div class='newtheorem'>
<!-- l. 2605 --><p class='noindent'><span class='head'>
<a id='x1-64002r6'></a>
<span class='cmssbx-10x-x-109'>Example 6.6.</span>  </span>Find the c.d.f. of the \(\mathrm {Exp}(\beta )\) distribution.
</p>
</div>
<!-- l. 2608 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2611 --><p class='noindent'><span class='head'>
<a id='x1-64003r7'></a>
<span class='cmssbx-10x-x-109'>Example 6.7.</span>  </span> The exponential distribution is often used to model waiting times. Suppose that the
length of a phone call to a company, in minutes, is distributed as \(\mathrm {Exp}(1/10)\). If you phone and the number is
busy then you are put on hold until the operator is free. If someone phones the company immediately
before you, find the probability that you will have to wait (a) less than 10 minutes, (b) between 10
and 20 minutes, and (c) between 10 and 20 minutes once you have already waited for 10 minutes.
</p>
</div>
<!-- l. 2618 --><p class='noindent'>
</p><!-- l. 2624 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-650006.2'></a>Lack of memory</h4>
<!-- l. 2627 --><p class='noindent'>A key property of the exponential distribution is its lack of memory property.
                                                                                      
                                                                                      
</p>
<div class='center'>
<!-- l. 2635 --><p class='noindent'>
</p>
<div class='fbox'>A random variable satisfies the <span class='cmssbx-10x-x-109'>lack of memory property</span> if \[ \mathbb {P}(X&gt;s+t\mid X&gt;t)=\mathbb {P}(X&gt;s)\quad \text {for all}\quad s,t&gt;0. \] That is, the
conditional  probability  that  a  variable  exceeds  \(s+t\),  given  that  it  exceeds  \(t\),  is
independent of \(t\).                                                       </div>
</div>
<!-- l. 2637 --><p class='noindent'>If we interpret \(X\) as a waiting time to an event, this means that the probability that you have to wait a
further time \(s\) is independent of how long you have waited already.
</p><!-- l. 2641 --><p class='noindent'>To show this result holds for \(X\sim \mathrm {Exp}(\beta )\) recall that \(\mathbb {P}(X&gt;x) =\exp (-\beta x)\) for all \(x&gt;0\). Hence, for \(s&gt;0\), \(t&gt;0\)
</p><!-- l. 2645 --><p class='noindent'>Note that we already observed this phenomenon in Example <a href='#x1-64003r7'>6.7<!-- tex4ht:ref: WaitingTimeExample  --></a>
</p><!-- l. 2647 --><p class='noindent'>Actually, the exponential distribution is the <span class='cmssbx-10x-x-109'>only </span>continuous distribution with the lack of memory property.
We will not prove that here.
</p><!-- l. 2650 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-660006.2'></a>The Gamma function</h4>
<!-- l. 2652 --><p class='noindent'>The integral required to obtain the expected value and variance of an exponential random variable will
occur several times in this chapter. We first define a slightly simplified form, which occurs in many areas of
mathematics, and discover several of its properties.
</p>
<div class='center'>
<!-- l. 2659 --><p class='noindent'>
</p>
<div class='fbox'>The <span class='cmssbx-10x-x-109'>Gamma function</span>, \(\mathrm {Gamma}ma(\alpha )\), is defined as \[ \mathrm {Gamma}ma(\alpha )=\int _{0}^{\infty } t^{\alpha -1} \exp (-t)\,\mathrm {d}t \]                                   </div>
</div>
<!-- l. 2661 --><p class='noindent'>Firstly we note that \[ \mathrm {Gamma}ma(1)={\int _{0}^{\infty } \exp (-t)\,\mathrm {d}t =\Bigl [-\exp (-t)\Bigr ]_0^\infty =1.} \]
</p>
<div class='newtheorem'>
<!-- l. 2667 --><p class='noindent'><span class='head'>
<a id='x1-66001r8'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Lemma 6.8.</span>  </span>For \(\alpha &gt;0\), \begin {align*}  \mathrm {Gamma}ma(\alpha +1)=\alpha \,\mathrm {Gamma}ma(\alpha ).  \end {align*}
</p>
</div>
<!-- l. 2671 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 2673 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>We use integration by parts:                                                                              □
</p>
</div>
<!-- l. 2676 --><p class='noindent'>Since \(\mathrm {Gamma}ma(1)=1\), we have that \(\mathrm {Gamma}ma(2)=1\), \(\mathrm {Gamma}ma(3)=2\mathrm {Gamma}ma(2)=2\), \(\mathrm {Gamma}ma(4)=3\mathrm {Gamma}ma(3)=6,\dots \). By induction we can easily see that for \(n\) a positive integer \(\mathrm {Gamma}ma(n)=(n-1)!\).
</p><!-- l. 2679 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-670006.2'></a>Expectation and variance</h4>
<!-- l. 2681 --><p class='noindent'>The \(r\)<span class='cmssbx-10x-x-109'>th moment</span> of a general random variable \(X\) is defined to be \(\mathbb {E}[X^r]\). In the case of exponential random
variables, we have that \begin {align*}  \mathbb {E}[X^r]&amp;=\int _{-\infty }^{\infty } x^r f_X(x)\mathrm {d} x\\ &amp;=\int _{0}^{\infty } x^r \beta \exp (-\beta x)\mathrm {d} x\\ &amp;= \beta \int _{0}^{\infty } x^r \exp (-\beta x)\mathrm {d} x.  \end {align*}
</p><!-- l. 2688 --><p class='noindent'>We will need to evaluate integrals of this form many times, so the following will be useful.
</p>
<div class='newtheorem'>
<!-- l. 2689 --><p class='noindent'><span class='head'>
<a id='x1-67001r9'></a>
<span class='cmssbx-10x-x-109'>Lemma 6.9.</span>  </span> \[ \int _{0}^{\infty } x^{\alpha -1} \exp (-\beta x)\,\mathrm {d}x =\frac {\mathrm {Gamma}ma(\alpha )}{\beta ^\alpha }. \]
</p>
</div>
<!-- l. 2695 --><p class='noindent'>
</p>
<div class='proof'>
                                                                                      
                                                                                      
<!-- l. 2697 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>Substituting \(t=\beta x\) gives
                                                                                     □
</p>
</div>
<!-- l. 2701 --><p class='noindent'>Using Lemma <a href='#x1-67001r9'>6.9<!-- tex4ht:ref: prop.gen.gamma  --></a> with \(\alpha =r+1\) we see that
</p><!-- l. 2704 --><p class='noindent'>For integer values of \(r\), therefore, \(\mathbb {E}[X^r]=r!/\beta ^r\).
</p><!-- l. 2706 --><p class='noindent'>In particular the expectation and variance of an exponential random variable are
</p><!-- l. 2709 --><p class='noindent'>Hence the expectation and standard deviation are the same. Note that the expectation decreases with \(\beta \); \(\beta \) is
the rate at which events occur, so the higher the rate of events the shorter the expected waiting time to
the next event.
</p><!-- l. 2713 --><p class='noindent'><span class='cmssbx-10x-x-109'>Beware: </span>Some computer packages use a different parameterisation of exponential random variables, in
which an \(\mathrm {Exp}(m)\) random variable has expected value \(m\).
</p><!-- l. 2717 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>6.3   </span> <a id='x1-680006.3'></a>The gamma distribution</h3>
<!-- l. 2722 --><p class='noindent'>A random variable \(X\) has a <span class='cmssbx-10x-x-109'>gamma distribution</span> with <span class='cmssbx-10x-x-109'>shape</span> parameter \(\alpha \) and <span class='cmssbx-10x-x-109'>rate</span> parameter \(\beta \) if its p.d.f. is
given by \[ f_X(x)= \begin {cases} \frac {\beta ^{\alpha }}{\mathrm {Gamma}ma(\alpha )} x^{\alpha -1}~\exp (-\beta x) &amp; x \geq 0,\\ 0&amp;\mbox {otherwise}, \end {cases} \] where \(\alpha &gt;0\) and \(\beta &gt;0\). We write \(X\sim \mathrm {Gamma}(\alpha ,\beta )\).
</p><!-- l. 2736 --><p class='noindent'>Lemma <a href='#x1-67001r9'>6.9<!-- tex4ht:ref: prop.gen.gamma  --></a> shows directly that
</p><!-- l. 2739 --><p class='noindent'>Plotted below are the p.d.f.s of four different gamma distributions.
</p>
<div class='center'>
<!-- l. 2741 --><p class='noindent'>
</p><!-- l. 2743 --><p class='noindent'><img alt='PIC' height='25' src='gammaA-1.png' width='25' /> <img alt='PIC' height='25' src='gammaB.png' width='25' /></p></div>
<div class='center'>
<!-- l. 2747 --><p class='noindent'>
                                                                                      
                                                                                      
</p><!-- l. 2748 --><p class='noindent'><img alt='PIC' height='25' src='gammaC.png' width='25' /> <img alt='PIC' height='25' src='gammaD.png' width='25' /></p></div>
<!-- l. 2757 --><p class='noindent'>The family of gamma distributions provides a flexible class of p.d.f.s which may describe the distribution of
a non-negative variable even when there is no strong model-based justification.
</p><!-- l. 2762 --><p class='noindent'>Note that when \(\alpha =1\) the gamma distribution reduces to the exponential distribution. However, unlike the
exponential distribution we cannot evaluate the c.d.f. in closed form for a general (non-integer) value of
\(\alpha \).
</p><!-- l. 2767 --><p class='noindent'>The statistical package R has built-in functions for evaluating the p.d.f., c.d.f. and inverse c.d.f. (for
obtaining quantiles) for many common distributions including the gamma distribution.
                                                                                      
                                                                                      
</p>
<pre class='verbatim' id='verbatim-6'>
&gt; dgamma(4,shape=6,rate=1) # p.d.f. of Gamma(6,1) evaluated at x=4, i.e. f(4)
[1] 0.1562935
&gt; dgamma(4,6,1)    # p.d.f. of Gamma(6,1) evaluated at x=4, i.e. f(4)
[1] 0.1562935
&gt; pgamma(2,0.5,1)  # c.d.f. of Gamma(0.5,1) evaluated at x=2, i.e. P(X&lt;2)
[1] 0.9544997
&gt; qgamma(0.5,3,1)  # the median of the Gamma(3,1) distribution
[1] 2.67406</pre>
<!-- l. 2781 --><p class='nopar'>
</p><!-- l. 2783 --><p class='noindent'>The \(r\)th moment of a gamma random variable is \begin {align*}  \mathbb {E}[X^r] &amp;= \int _{-\infty }^{\infty } x^r f_X(x)\mathrm {d} x = \int _{0}^{\infty } x^r \beta ^{\alpha } x^{\alpha -1} \exp (-\beta x)/\mathrm {Gamma}ma(\alpha )\mathrm {d} x\\ &amp; = \frac {\beta ^{\alpha }}{\mathrm {Gamma}ma(\alpha )} \int _{0}^{\infty }x^{r+\alpha -1}\exp (-\beta x)\mathrm {d} x\\ &amp;= \frac {\beta ^\alpha }{\mathrm {Gamma}ma(\alpha )}\times \frac {\mathrm {Gamma}ma(r+\alpha )}{\beta ^{r+\alpha }}\\ &amp;= \frac {\mathrm {Gamma}ma(r+\alpha )}{\beta ^{r}\mathrm {Gamma}ma(\alpha )}.  \end {align*}
</p><!-- l. 2796 --><p class='noindent'>where the penultimate line follows from Lemma <a href='#x1-67001r9'>6.9<!-- tex4ht:ref: prop.gen.gamma  --></a>.
</p><!-- l. 2798 --><p class='noindent'>An alternative to remembering or rederiving Lemma <a href='#x1-67001r9'>6.9<!-- tex4ht:ref: prop.gen.gamma  --></a> is to use the <span class='cmssbx-10x-x-109'>unit integrability</span> property of the
density (this trick can be useful for densities other than the gamma). \begin {align*}  \mathbb {E}[X^r]&amp;=\frac {\beta ^{\alpha }}{\mathrm {Gamma}ma(\alpha )} \int _{0}^{\infty }x^{r+\alpha -1}\exp (-\beta x)\mathrm {d} x\\ &amp;= \frac {\beta ^{\alpha }}{\mathrm {Gamma}ma(\alpha )} \times \frac {\mathrm {Gamma}ma(\alpha +r)}{\beta ^{\alpha +r}} \times \int _{0}^{\infty }\frac {\beta ^{\alpha +r}}{\mathrm {Gamma}ma(\alpha +r)}x^{r+\alpha -1}\exp (-\beta x)\mathrm {d} x\\ &amp;= \frac {\mathrm {Gamma}ma(\alpha +r)}{\beta ^{r}\mathrm {Gamma}ma(\alpha )}\times 1.  \end {align*}
</p><!-- l. 2814 --><p class='noindent'>Both approaches result in the same formula for \(\mathbb {E}[X^r]\). We can now derive the expectation and variance.
</p><!-- l. 2818 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>6.4   </span> <a id='x1-690006.4'></a>The normal distribution</h3>
<!-- l. 2820 --><p class='noindent'>A random variable \(X\) has a <span class='cmssbx-10x-x-109'>normal distribution</span>, also known as a <span class='cmssbx-10x-x-109'>Gaussian distribution</span>, with parameters \(\mu \)
and \(\sigma ^2\) if its p.d.f. is given by \[ f_X(x)=\frac {1}{\sqrt {2\pi \sigma ^2}} \exp \left \{- \frac {(x-\mu )^2}{2\sigma ^2}\right \} \] for \(x\in \mathbb {R}\). We write \(X\sim N(\mu , \sigma ^2)\).
</p><!-- l. 2829 --><p class='noindent'>This is the most commonly used continuous random variable, cropping up all over the place, for good
theoretical reasons. We will not study it in detail here; this will be done in MATH104.
                                                                                      
                                                                                      
</p>
<h2 class='chapterHead'><span class='titlemark'>Chapter 7</span><br /><a id='x1-700007'></a>More than one random variable</h2>
<!-- l. 2833 --><p class='noindent'>Often we have more than one random quantity in an experiment. For example: </p>
     <ul class='itemize1'>
     <li class='itemize'>the height and weight of a randomly sampled tree;
     </li>
     <li class='itemize'>the amount of precipitation and the height of a river;
     </li>
     <li class='itemize'>the outcomes of repeated trials in an experiment.</li></ul>
<!-- l. 2840 --><p class='noindent'>So far we have only introduced the machinery to deal with a single random variable for each experiment. In
this final chapter we will introduce some basic principles that enable us to deal with more than one random
quantity at a time.
</p>
<h3 class='sectionHead'><span class='titlemark'>7.1   </span> <a id='x1-710007.1'></a>Joint probability mass functions</h3>
<!-- l. 2843 --><p class='noindent'>Recall that a random variable is simply a function from the sample space \(\Omega \) to the real numbers \(\mathbb R\), mapping
each elementary outcome \(\omega \) to a number. Formally, there is no reason not to define several such functions, \(X_1, X_2, \ldots \)
such that for each \(\omega \) we get a set of numbers \(X_1(\omega ), X_2(\omega ), \ldots \).
</p>
<div class='newtheorem'>
<!-- l. 2845 --><p class='noindent'><span class='head'>
<a id='x1-71001r1'></a>
<span class='cmssbx-10x-x-109'>Example 7.1.</span>  </span>Let \(\Omega \) be the set of all trees in a forest. An experiment consists of selecting a tree \(\omega \). Let
\(X_1\) report the height of a selected tree, and \(X_2\) report the weight of a selected tree. Then the reported
numbers on carrying out an experiment are the measured height \(X_1(\omega )\) and weight \(X_2(\omega )\).
</p>
</div>
<!-- l. 2847 --><p class='noindent'>
</p><!-- l. 2849 --><p class='noindent'>We present some basic theory for discrete random variables. Let \(X\) and \(Y\) be discrete random
variables defined on the same sample space \(\Omega \). Their <span class='cmssbx-10x-x-109'>joint probability mass function</span> is \[ p_{X,Y}(x,y) = \mathbb {P}(\{X=x\}\cap \{Y=y\}). \] As
in earlier chapters, we concentrate on discrete random variables taking non-negative integer
values.
                                                                                      
                                                                                      
</p><!-- l. 2855 --><p class='noindent'>
</p>
<h4 class='likesubsectionHead'><a id='x1-720007.1'></a>Properties of \(p_{X,Y}(x,y)\):</h4>
<!-- l. 2857 --><p class='noindent'>
     </p><dl class='enumerate'><dt class='enumerate'>
   1. </dt><dd class='enumerate'>
     <!-- l. 2857 --><p class='noindent'>For all \(x\) and \(y\), we have \(0\leq p_{X,Y}(x,y)\leq 1\),
     </p></dd><dt class='enumerate'>
   2. </dt><dd class='enumerate'>
     <!-- l. 2858 --><p class='noindent'>\(\sum _{\text {all }x,y} p_{X,Y}(x,y) =1\),
     </p></dd><dt class='enumerate'>
   3. </dt><dd class='enumerate'>
     <!-- l. 2859 --><p class='noindent'>\(\mathbb {P}((X,Y)\in A) = \sum _{(x,y)\in A}p_{X,Y}(x,y).\)</p></dd></dl>
<div class='newtheorem'>
<!-- l. 2862 --><p class='noindent'><span class='head'>
<a id='x1-72004r2'></a>
<span class='cmssbx-10x-x-109'>Exercise 7.2.</span>  </span>The joint p.m.f. of \(X\) and \(Y\) is \[ p_{X,Y}(x,y)=(x+y)/18\] for \(x,y=0,1,2\).
     </p><dl class='enumerate'><dt class='enumerate'>
   a. </dt><dd class='enumerate'>
     <!-- l. 2866 --><p class='noindent'>Write out the joint probability table.
     </p></dd><dt class='enumerate'>
   b. </dt><dd class='enumerate'>
     <!-- l. 2867 --><p class='noindent'>Show this is a valid joint p.m.f.
     </p></dd><dt class='enumerate'>
   c. </dt><dd class='enumerate'>
     <!-- l. 2868 --><p class='noindent'>Evaluate (i) \(\mathbb {P}(X=2)\), (ii) \(\mathbb {P}(X=Y)\), (iii) \(\mathbb {P}(X+Y\geq 2)\).</p></dd></dl>
</div>
<!-- l. 2870 --><p class='noindent'>
</p><!-- l. 2873 --><p class='noindent'>Note that each of \(X\) and \(Y\) still have their own probability mass functions \(p_X\) and \(p_Y\). In the context of jointly
distributed random variables, these are called the <span class='cmssbx-10x-x-109'>marginal probability mass functions</span>.
</p>
<div class='newtheorem'>
<!-- l. 2877 --><p class='noindent'><span class='head'>
<a id='x1-72008r3'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Exercise 7.3.</span>  </span>Bivariate random variables \( X\) and \( Y\) have joint p.m.f. </p>
<div class='center'>
<!-- l. 2879 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-6'><colgroup id='TBL-6-1g'><col id='TBL-6-1' /><col id='TBL-6-2' /></colgroup><colgroup id='TBL-6-3g'><col id='TBL-6-3' /><col id='TBL-6-4' /><col id='TBL-6-5' /><col id='TBL-6-6' /></colgroup><colgroup id='TBL-6-7g'><col id='TBL-6-7' /></colgroup><tr id='TBL-6-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-1-1' style='white-space:nowrap; text-align:left;'></td><td class='td11' id='TBL-6-1-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-1-3' style='white-space:nowrap; text-align:right;'>     \(y\)</td><td class='td11' id='TBL-6-1-4' style='white-space:nowrap; text-align:right;'>     </td><td class='td11' id='TBL-6-1-5' style='white-space:nowrap; text-align:right;'>     </td><td class='td11' id='TBL-6-1-6' style='white-space:nowrap; text-align:right;'>    </td><td class='td11' id='TBL-6-1-7' style='white-space:nowrap; text-align:right;'>     \(p_X(x)\)</td></tr><tr id='TBL-6-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-2-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-2-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-2-3' style='white-space:nowrap; text-align:right;'> 0</td><td class='td11' id='TBL-6-2-4' style='white-space:nowrap; text-align:right;'> 1</td><td class='td11' id='TBL-6-2-5' style='white-space:nowrap; text-align:right;'> 2</td><td class='td11' id='TBL-6-2-6' style='white-space:nowrap; text-align:right;'> 3</td><td class='td11' id='TBL-6-2-7' style='white-space:nowrap; text-align:right;'></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-3-1' style='white-space:nowrap; text-align:left;'>\( x\) </td><td class='td11' id='TBL-6-3-2' style='white-space:nowrap; text-align:left;'>1</td><td class='td11' id='TBL-6-3-3' style='white-space:nowrap; text-align:right;'> 5/60</td><td class='td11' id='TBL-6-3-4' style='white-space:nowrap; text-align:right;'> 8/60</td><td class='td11' id='TBL-6-3-5' style='white-space:nowrap; text-align:right;'> 2/60</td><td class='td11' id='TBL-6-3-6' style='white-space:nowrap; text-align:right;'>1/60</td><td class='td11' id='TBL-6-3-7' style='white-space:nowrap; text-align:right;'>16/60</td>
</tr><tr id='TBL-6-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-4-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-4-2' style='white-space:nowrap; text-align:left;'>2</td><td class='td11' id='TBL-6-4-3' style='white-space:nowrap; text-align:right;'>12/60</td><td class='td11' id='TBL-6-4-4' style='white-space:nowrap; text-align:right;'> 7/60</td><td class='td11' id='TBL-6-4-5' style='white-space:nowrap; text-align:right;'> 3/60</td><td class='td11' id='TBL-6-4-6' style='white-space:nowrap; text-align:right;'>2/60</td><td class='td11' id='TBL-6-4-7' style='white-space:nowrap; text-align:right;'>24/60</td>
</tr><tr id='TBL-6-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-5-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-5-2' style='white-space:nowrap; text-align:left;'>3</td><td class='td11' id='TBL-6-5-3' style='white-space:nowrap; text-align:right;'> 4/60</td><td class='td11' id='TBL-6-5-4' style='white-space:nowrap; text-align:right;'> 8/60</td><td class='td11' id='TBL-6-5-5' style='white-space:nowrap; text-align:right;'> 6/60</td><td class='td11' id='TBL-6-5-6' style='white-space:nowrap; text-align:right;'>2/60</td><td class='td11' id='TBL-6-5-7' style='white-space:nowrap; text-align:right;'>20/60</td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-6-1' style='white-space:nowrap; text-align:left;'>\(p_Y(y)\) </td><td class='td11' id='TBL-6-6-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-6-6-3' style='white-space:nowrap; text-align:right;'>21/60</td><td class='td11' id='TBL-6-6-4' style='white-space:nowrap; text-align:right;'>23/60</td><td class='td11' id='TBL-6-6-5' style='white-space:nowrap; text-align:right;'>11/60</td><td class='td11' id='TBL-6-6-6' style='white-space:nowrap; text-align:right;'>5/60</td><td class='td11' id='TBL-6-6-7' style='white-space:nowrap; text-align:right;'>    1</td>
</tr></table></div></div>
<!-- l. 2891 --><p class='noindent'>Fill in the marginal p.m.f.s in the final row and column.
</p>
</div>
<!-- l. 2892 --><p class='noindent'>
</p><!-- l. 2895 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>7.2   </span> <a id='x1-730007.2'></a>Independence</h3>
<!-- l. 2897 --><p class='noindent'>Independence is the simplest form for <span class='cmssbx-10x-x-109'>joint</span> behaviour of two (or more) random variables. Informally, two
random variables \( X\) and \( Y\) are independent if knowing the value of one of them gives <span class='cmssbx-10x-x-109'>no information</span> about
the value of the other.
</p><!-- l. 2903 --><p class='noindent'>The outcomes of, say, rolls of two separate dice are independent in exactly this sense: knowing that the red
die showed a particular value does not give us any information about the score of the blue die, or vice
versa.
</p>
<div class='center'>
<!-- l. 2915 --><p class='noindent'>
</p>
<div class='fbox'>Two random variables \( X\) and \( Y\) are <span class='cmssbx-10x-x-109'>independent</span> if the events \(\{ X \in A\}\) and \(\{ Y\in B\}\) are independent
for all sets \( A\) and \( B\), i.e. \[ \mathbb {P}(\{X\in A\}\cap \{Y\in B\}) = \mathbb {P}(X\in A)\mathbb {P}(Y\in B) \] for all sets \(A, B\).                                       </div>
</div>
<div class='newtheorem'>
<!-- l. 2920 --><p class='noindent'><span class='head'>
<a id='x1-73001r4'></a>
                                                                                      
                                                                                      
<span class='cmssbx-10x-x-109'>Theorem 7.4.</span>  </span>Two  discrete  random  variables  \( X\)  and  \( Y\)  with  joint  probability  mass  function  \(p_{X,Y}\)  are
independent if and only if \[ p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y) \] for all \(x\) and \(y\).
</p>
</div>
<!-- l. 2927 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 2929 --><p class='noindent'><span class='head'>
<span class='cmssi-10x-x-109'>Proof.</span> </span>Let \(X\) and \(Y\) be independent, and let \( A=\{x\}\) and \( B=\{y\}\). Then
</p><!-- l. 2932 --><p class='noindent'>Conversely, suppose \(p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)\) for all \(x,y\). Then, for arbitrary sets \( A\) and \( B\),
                                                                                     □
</p>
</div>
<div class='center'>
<!-- l. 2942 --><p class='noindent'>
</p>
<div class='fbox'>If \( X\) and \( Y\) are discrete random variables, and \(p_Y(y)&gt;0\), the <span class='cmssbx-10x-x-109'>conditional probability mass
functions</span> are defined as \begin {align*}  p_{X\mid Y}(x\mid y) &amp;= \frac {p_{X,Y}(x,y)}{p_{Y}(y)}, \\ p_{Y\mid X}(y\mid x) &amp;= \frac {p_{X,Y}(x,y)}{p_{X}(x)}.  \end {align*}                                               </div>
</div>
<!-- l. 2943 --><p class='noindent'>Thus \(p_{X\mid Y}(x\mid y) = \mathbb {P}(\{X=x\}\cap \{Y=y\})/\mathbb {P}(Y=y) = \mathbb {P}(X=x\mid Y=y)\).
</p>
<div class='newtheorem'>
<!-- l. 2945 --><p class='noindent'><span class='head'>
<a id='x1-73002r5'></a>
<span class='cmssbx-10x-x-109'>Exercise 7.5.</span>  </span>Show that if the discrete variables \((X,Y)\) are independent then for all \(x,y\) we have \[ p_{X\mid Y}(x\mid y)=p_{X}(x). \]
</p>
</div>
<!-- l. 2949 --><p class='noindent'>
These results conform with intuition as, when \(X\) and \(Y\) are independent, knowing the value of \(X\) should tell us
nothing about \(Y\).
                                                                                      
                                                                                      
</p><!-- l. 2953 --><p class='noindent'>The converse is also true: if the conditional distribution of \( X\) given \({Y=y}\) does not depend on \( y\), or equivalently, the
conditional distribution of \( Y\) given \(X=x\) does not depend on \( x\), then \( X\) and \( Y\) are independent.
</p>
<div class='newtheorem'>
<!-- l. 2958 --><p class='noindent'><span class='head'>
<a id='x1-73003r6'></a>
<span class='cmssbx-10x-x-109'>Exercise 7.6.</span>  </span>A fair coin is tossed. If it shows \( H\) a fair die is thrown, if \( T\) a biased die. The biased die
makes even numbers twice as probable as odd numbers. Let \(X\) be the random variable with value \(1\) if
the coin shows a head and \(0\) otherwise, and let \(Y\) be the score on the die. Find the joint p.m.f. of \( X\) and
\( Y\).
</p>
</div>
<!-- l. 2962 --><p class='noindent'>
</p>
<div class='newtheorem'>
<!-- l. 2965 --><p class='noindent'><span class='head'>
<span class='cmssbx-10x-x-109'>Solution.</span>  </span>First, let’s work out the probability distribution for the biased die.
</p><!-- l. 2968 --><p class='noindent'>We know that the marginal p.m.f. \({{p_{ X}(x)=}} {1/2}\) for \({x=0,1}\).
</p><!-- l. 2970 --><p class='noindent'>From the information given, we can work out the conditional p.m.f. <br class='newline' />\(x=1\): \(p_{Y\mid X}(y\mid 1)={}\)\(1/6\) for \( y=1,2,\dots ,6\).<br class='newline' />\(x=0\): \(p_{Y\mid X}(y\mid 0)= {c}\) for \( y={1,3,5}\) and \(p_{Y\mid X}(y\mid 0)= {2c}\) for \(y={2,4,6}\).
</p><!-- l. 2975 --><p class='noindent'>Using \({p_{X,Y}(x,y)}= {p_{Y\mid X}(y\mid x)\; p_{X}(x)}\) gives
</p>
<div class='center'>
<!-- l. 2977 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-7'><colgroup id='TBL-7-1g'><col id='TBL-7-1' /></colgroup><colgroup id='TBL-7-2g'><col id='TBL-7-2' /><col id='TBL-7-3' /><col id='TBL-7-4' /><col id='TBL-7-5' /><col id='TBL-7-6' /><col id='TBL-7-7' /></colgroup><tr id='TBL-7-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-1-1' style='white-space:nowrap; text-align:left;'></td><td class='td11' id='TBL-7-1-2' style='white-space:nowrap; text-align:right;'>   1</td><td class='td11' id='TBL-7-1-3' style='white-space:nowrap; text-align:right;'>   2</td><td class='td11' id='TBL-7-1-4' style='white-space:nowrap; text-align:right;'>   3</td><td class='td11' id='TBL-7-1-5' style='white-space:nowrap; text-align:right;'>   4</td><td class='td11' id='TBL-7-1-6' style='white-space:nowrap; text-align:right;'>   5</td><td class='td11' id='TBL-7-1-7' style='white-space:nowrap; text-align:right;'>   6</td></tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-2-' style='vertical-align:baseline;'><td class='td
11' id='TBL-7-2-1' style='white-space:nowrap; text-align:left;'>0</td><td class='td11' id='TBL-7-2-2' style='white-space:nowrap; text-align:right;'>1/18</td><td class='td11' id='TBL-7-2-3' style='white-space:nowrap; text-align:right;'>2/18</td><td class='td11' id='TBL-7-2-4' style='white-space:nowrap; text-align:right;'>1/18</td><td class='td11' id='TBL-7-2-5' style='white-space:nowrap; text-align:right;'>2/18</td><td class='td11' id='TBL-7-2-6' style='white-space:nowrap; text-align:right;'>1/18</td><td class='td11' id='TBL-7-2-7' style='white-space:nowrap; text-align:right;'>2/18</td>
</tr><tr id='TBL-7-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-3-1' style='white-space:nowrap; text-align:left;'>1</td><td class='td11' id='TBL-7-3-2' style='white-space:nowrap; text-align:right;'>1/12</td><td class='td11' id='TBL-7-3-3' style='white-space:nowrap; text-align:right;'>1/12</td><td class='td11' id='TBL-7-3-4' style='white-space:nowrap; text-align:right;'>1/12</td><td class='td11' id='TBL-7-3-5' style='white-space:nowrap; text-align:right;'>1/12</td><td class='td11' id='TBL-7-3-6' style='white-space:nowrap; text-align:right;'>1/12</td><td class='td11' id='TBL-7-3-7' style='white-space:nowrap; text-align:right;'>1/12</td>
</tr></table></div></div>
</div>
<!-- l. 2984 --><p class='noindent'>
</p>
<div class='newtheorem'>
                                                                                      
                                                                                      
<!-- l. 2986 --><p class='noindent'><span class='head'>
<a id='x1-73004r7'></a>
<span class='cmssbx-10x-x-109'>Example 7.7.</span>  </span>For the joint p.m.f. in Example <a href='#x1-72008r3'>7.3<!-- tex4ht:ref: example:joint_pmf  --></a> obtain the conditional p.m.f. of \( X\) given \( Y=2\).
</p>
</div>
<!-- l. 2989 --><p class='noindent'>
</p>
<div class='center'>
<!-- l. 2990 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-8'><colgroup id='TBL-8-1g'><col id='TBL-8-1' /><col id='TBL-8-2' /></colgroup><colgroup id='TBL-8-3g'><col id='TBL-8-3' /><col id='TBL-8-4' /><col id='TBL-8-5' /><col id='TBL-8-6' /></colgroup><colgroup id='TBL-8-7g'><col id='TBL-8-7' /></colgroup><tr id='TBL-8-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-1-1' style='white-space:nowrap; text-align:left;'></td><td class='td11' id='TBL-8-1-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-1-3' style='white-space:nowrap; text-align:right;'> \( Y\)</td></tr><tr id='TBL-8-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-2-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-2-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-2-3' style='white-space:nowrap; text-align:right;'>0</td><td class='td11' id='TBL-8-2-4' style='white-space:nowrap; text-align:right;'>1</td><td class='td11' id='TBL-8-2-5' style='white-space:nowrap; text-align:center;'>  2   </td><td class='td11' id='TBL-8-2-6' style='white-space:nowrap; text-align:right;'>3</td><td class='td11' id='TBL-8-2-7' style='white-space:nowrap; text-align:right;'>     </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-8-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-3-1' style='white-space:nowrap; text-align:left;'>\( X\) </td><td class='td11' id='TBL-8-3-2' style='white-space:nowrap; text-align:left;'>1</td><td class='td11' id='TBL-8-3-3' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-3-4' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-3-5' style='white-space:nowrap; text-align:center;'> 2/60 </td><td class='td11' id='TBL-8-3-6' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-3-7' style='white-space:nowrap; text-align:right;'> 16/60</td>
</tr><tr id='TBL-8-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-4-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-4-2' style='white-space:nowrap; text-align:left;'>2</td><td class='td11' id='TBL-8-4-3' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-4-4' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-4-5' style='white-space:nowrap; text-align:center;'> 3/60 </td><td class='td11' id='TBL-8-4-6' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-4-7' style='white-space:nowrap; text-align:right;'> 24/60</td>
</tr><tr id='TBL-8-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-5-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-5-2' style='white-space:nowrap; text-align:left;'>3</td><td class='td11' id='TBL-8-5-3' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-5-4' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-5-5' style='white-space:nowrap; text-align:center;'> 6/60 </td><td class='td11' id='TBL-8-5-6' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-5-7' style='white-space:nowrap; text-align:right;'> 20/60</td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-8-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-6-1' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-6-2' style='white-space:nowrap; text-align:left;'> </td><td class='td11' id='TBL-8-6-3' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-6-4' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-6-5' style='white-space:nowrap; text-align:center;'>11/60</td><td class='td11' id='TBL-8-6-6' style='white-space:nowrap; text-align:right;'> </td><td class='td11' id='TBL-8-6-7' style='white-space:nowrap; text-align:right;'>     </td>
</tr></table></div></div>
<!-- l. 3003 --><p class='noindent'>So \[ p_{X\mid Y}(x\mid 2) = {p_{X,Y}(x,2)/p_{Y}(2)}. \]
</p><!-- l. 3011 --><p class='noindent'>We have seen that when \(X\) and \(Y\) are both discrete, they are independent if and only if their joint p.m.f. can
be factorised as a product of the marginal p.m.f.s. \[ p_{X,Y}(x,y) = p_X(x)p_Y(y). \] Our definition of independence holds also for continuous
random variables, but there is no joint p.m.f. for continuous random variables. It is beyond the scope of
this module, but there can exist a joint probability density function \(f_{X,Y}(x,y)\). As with univariate random variables,
results that hold in the discrete case with probability mass functions often hold in the continuous case with
joint mass functions.
</p>
<div class='newtheorem'>
<!-- l. 3019 --><p class='noindent'><span class='head'>
<a id='x1-73005r8'></a>
<span class='cmssbx-10x-x-109'>Theorem 7.8.</span>  </span>Two continuous random variables \(X\) and \(Y\) are independent if and only if \[ f_{X,Y}(x,y) = f_X(x)f_Y(y) \] for all \(x\) and
\(y\).
</p>
</div>
<!-- l. 3026 --><p class='noindent'>
</p>
<div class='proof'>
<!-- l. 3027 --><p class='noindent'><span class='head'>
                                                                                      
                                                                                      
<span class='cmssi-10x-x-109'>Proof.</span> </span>Not given here.                                                                                              □
</p>
</div>
<!-- l. 3029 --><p class='noindent'>The result is needed for constructing <span class='cmssbx-10x-x-109'>likelihood-based estimates</span> in statistics: often it is assumed that
repeated experiments result in \(n\) independent observations of a random variable, and the joint density
function of the observations is the product of the marginal densities.
</p><!-- l. 3031 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>7.3   </span> <a id='x1-740007.3'></a>The weak law of large numbers</h3>
<!-- l. 3035 --><p class='noindent'>Recall from Exercise <a href='#x1-43003r12'>4.12<!-- tex4ht:ref: WeakLawEvents  --></a> that if an experiment is repeated \(n\) times then, as \(n\) increases, the proportion of
times an event \(A\) occurs converges to \(\mathbb {P}(A)\). We will now prove a similar result concerning the average of several
realisations of a random variable converging to the expected value. We start with a lemma which is proved
in MATH230.
</p>
<div class='newtheorem'>
<!-- l. 3037 --><p class='noindent'><span class='head'>
<a id='x1-74001r9'></a>
<span class='cmssbx-10x-x-109'>Lemma 7.9.</span>  </span> Let \(X_1, X_2, \ldots , X_n\) be jointly distributed random variables with finite expectation and variance. Then
</p>
     <ul class='itemize1'>
     <li class='itemize'>\(\mathbb {E}(X_1+X_2+\cdots +X_n) = \mathbb {E}(X_1) + \mathbb {E}(X_2)+\cdots +\mathbb {E}(X_n)\), and
     </li>
     <li class='itemize'>if \(X_1, X_2,\ldots ,X_n\) are independent then \[\mathrm {Var}(X_1+X_2+\cdots +X_n)=\mathrm {Var}(X_1)+\mathrm {Var}(X_2)+\cdots +\mathrm {Var}(X_n).\]</li></ul>
</div>
<!-- l. 3043 --><p class='noindent'>
</p><!-- l. 3046 --><p class='noindent'>Now suppose that \(X_1,X_2,\ldots ,X_n\) are independent copies of a random variable \(X\). For example, suppose we repeated an
experiment \(n\) times, and \(X_i\) is the measured outcome on the \(i\)th experiment. This means that for each \(i\) we have \begin {align*}  \mathbb {E}(X_i) &amp;= {\mathbb {E}(X)}\\ \mathrm {Var}(X_i) &amp;= { \mathrm {Var}(X)}  \end {align*}
</p><!-- l. 3052 --><p class='noindent'>If we want to report a value, scientists will usually measure it \(n\) times and report the average measured
value. Let \(X_i\) be the measured value on the \(i\)th experiment. The average measured value is \[\bar {X} = \frac {1}{n}(X_1+X_2+\cdots +X_n).\] Why do we do
this?
                                                                                      
                                                                                      
</p><!-- l. 3056 --><p class='noindent'>Let’s consider the properties of \(\bar {X}\). For simplicity, write \(\mu \) for \(\mathbb {E}(X)\) and \(\sigma ^2\) for \(\mathrm {Var}(X)\).
</p><!-- l. 3059 --><p class='noindent'>So \(\bar {X}\) has expectation the quantity we wish to report, the true expected value of \(X\). Of course, simply reporting
the first measurement \(X_1\) would also have this expected value.
</p><!-- l. 3061 --><p class='noindent'>Consider now the variance of \(\bar {X}\):
</p><!-- l. 3064 --><p class='noindent'>The variance of our reported quantity, \(\bar {X}\), decreases as the number of measurements \(n\) increases.
</p><!-- l. 3066 --><p class='noindent'>We can use Chebychev’s inequality (Section <a href='#x1-290003.6'>3.6<!-- tex4ht:ref: SecChebychev  --></a>) to be more precise about this. Recall that for any random
variable \(R\) with expected value \(m\) and standard deviation \(s\) \[ \mathbb {P}\left (|R-m|&gt;cs\right )\leq \frac 1{c^2},\] for any \(c&gt;0\).
</p><!-- l. 3070 --><p class='noindent'>Hence for the random variable \(\bar {X}\) with expected value \(\mu \), variance \(\sigma ^2/n\) and hence standard deviation \(\sigma /\sqrt {n}\), we have
\[ \mathbb {P}(|\bar {X}-\mu |&gt;\frac {c\sigma }{\sqrt {n}}) \leq \frac 1{c^2}\]
</p><!-- l. 3073 --><p class='noindent'>By taking \(k=c/\sqrt {n}\), we can rearrange this expression to \[ \mathbb {P}(|\bar {X}-\mu |&gt;k\sigma ) \leq \frac {1}{k^2n}.\]
</p><!-- l. 3076 --><p class='noindent'>We see that as \(n\) gets large, the probability that the sample average \(\bar {X}\) is more than distance \(k\sigma \) away from the
expected value of the original random quantity \(X\) converges to \(0\).
</p><!-- l. 3078 --><p class='noindent'>Since \(k\) is arbitrary, in some sense we can say that \(\bar {X}\) <span class='cmssbx-10x-x-109'>converges to</span> \(\mu \). This is called the <span class='cmssbx-10x-x-109'>weak law of
large numbers</span>. You will see various other forms of convergence of random variables in later
courses.
</p><!-- l. 3080 --><p class='noindent'>One final thing to note: the standard deviation \(\sigma \) is exactly the right quantity for determining the appropriate
scale for measuring distance here: the events are of the type “a random variable is more than \(k\) standard
deviations away from the mean”.
</p>
 
</body> 
</html>
